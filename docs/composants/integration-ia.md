# Int√©gration IA

## Vue d'ensemble

JodoTarot utilise deux fournisseurs d'IA pour l'interpr√©tation des tirages :
- OpenAI (API cloud)
- Ollama (API locale)

Cette documentation se concentre sur l'int√©gration avec Ollama, qui permet d'ex√©cuter des mod√®les d'IA localement.

## Configuration Ollama

### Pr√©requis

1. Installer Ollama sur votre machine : [https://ollama.ai](https://ollama.ai)
2. D√©marrer le serveur Ollama :
```bash
ollama serve
```
3. T√©l√©charger un mod√®le compatible (par exemple llama2) :
```bash
ollama pull llama2
```

### Configuration dans JodoTarot

La configuration d'Ollama se fait dans `assets/js/config.js` :

```javascript
// Param√®tres utilisateur configurables
const SETTINGS = {
  // URLs des services
  OLLAMA_URL: "http://localhost:11434",  // URL du serveur Ollama local
  OPENAI_URL: "https://api.openai.com/v1", // URL de l'API OpenAI
  
  // Autres param√®tres...
};

// Points d'acc√®s de l'API Ollama
const API_URL_OLLAMA = `${SETTINGS.OLLAMA_URL}/api/generate`;
const API_URL_OLLAMA_TAGS = `${SETTINGS.OLLAMA_URL}/api/tags`;
```

### Timeouts et Param√®tres

```javascript
// Configuration des timeouts (en millisecondes)
const TIMEOUTS = {
  OLLAMA_CONNECT: 30000,    // 30 secondes pour la connexion initiale
  OLLAMA_MODEL_LOAD: 60000, // 60 secondes pour le chargement du mod√®le
  OLLAMA_RESPONSE: 120000,  // 120 secondes pour la g√©n√©ration de r√©ponse
  OLLAMA_CHECK: 5000,      // 5 secondes pour v√©rifier l'√©tat des mod√®les
  MAX_RETRIES: 3,          // Nombre maximum de tentatives
  RETRY_DELAY: 1000,       // D√©lai entre les tentatives (1 seconde)
  MODEL_LOAD_CHECK: 10000  // 10 secondes pour v√©rifier si un mod√®le est charg√©
};
```

## M√©canismes Avanc√©s

Le module d'API de JodoTarot int√®gre plusieurs m√©canismes sophistiqu√©s pour assurer une communication fiable avec les services d'IA:

### Gestion intelligente des timeouts

- **Timeouts diff√©renci√©s par √©tape**: Diff√©rents timeouts sont configur√©s pour chaque phase (connexion, chargement du mod√®le, g√©n√©ration de r√©ponse)
- **D√©tection automatique d'inactivit√©**: Le syst√®me surveille l'arriv√©e des chunks de donn√©es et d√©clenche une erreur si aucune donn√©e n'est re√ßue pendant une p√©riode configurable
- **Timers adaptatifs**: Les d√©lais d'attente sont ajust√©s automatiquement selon le type de mod√®le (local vs cloud)

### Syst√®me de retry automatique

L'impl√©mentation utilise un syst√®me de retry intelligent dans la m√©thode `fetchWithRetry` :

```javascript
async fetchWithRetry(url, options, maxRetries = TIMEOUTS.MAX_RETRIES, timeoutMs = TIMEOUTS.OLLAMA_CONNECT) {
  let retries = 0;
  let lastError = null;
  
  while (retries <= maxRetries) {
    try {
      // Cr√©er un contr√¥leur d'abandon pour le timeout
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), timeoutMs);
      
      // Ajouter le signal au options
      const optionsWithSignal = {
        ...options,
        signal: controller.signal
      };
      
      try {
        // Tenter la requ√™te
        const response = await fetch(url, optionsWithSignal);
        clearTimeout(timeoutId);
        
        // Si la r√©ponse est OK, la retourner
        if (response.ok) {
          return response;
        }
        
        // Sinon, lire le texte d'erreur et le lancer
        const errorText = await response.text();
        throw new Error(`Erreur API (${response.status}): ${errorText.slice(0, 100)}`);
      } catch (fetchError) {
        clearTimeout(timeoutId);
        throw fetchError;
      }
    } catch (error) {
      lastError = error;
      retries++;
      
      const isTimeout = error.name === 'AbortError';
      const retriesLeft = maxRetries - retries + 1;
      
      if (retriesLeft > 0) {
        // D√©lai exponentiel avec un peu d'al√©atoire pour √©viter les collisions
        const delay = Math.pow(2, retries) * 1000 + Math.random() * 500;
        console.warn(`Tentative ${retries}/${maxRetries} √©chou√©e${isTimeout ? ' (timeout)' : ''}: ${error.message}. Nouvelle tentative dans ${delay/1000} secondes...`);
        
        // Attendre avant de r√©essayer
        await new Promise(resolve => setTimeout(resolve, delay));
      } else {
        console.error(`Toutes les tentatives ont √©chou√© (${retries}/${maxRetries}):`, error);
        throw error;
      }
    }
  }
  
  // Ce code ne devrait jamais √™tre atteint, mais par pr√©caution
  throw lastError || new Error("Erreur inconnue pendant les tentatives de connexion");
}
```

### D√©tection et adaptation de format

La classe AIService comprend une gestion sophistiqu√©e des diff√©rents formats de r√©ponse selon le mod√®le utilis√© :

```javascript
// Configuration des mod√®les Ollama
const OLLAMA_MODEL_FORMATS = {
  "llama3.1": {
    pattern: /\bllama3\.1\b/i,
    responseKey: "choices.0.message.content",
    description: "Llama 3.1 (retourne choices[0].message.content)"
  },
  "llama3": {
    pattern: /\bllama3\b(?!\.1)/i, // llama3 mais pas llama3.1
    responseKey: "message.content",
    description: "Llama 3 (retourne message.content)"
  },
  "llama2": {
    pattern: /\bllama2\b/i,
    responseKey: "response",
    description: "Llama 2 (retourne response)"
  },
  // ... autres mod√®les ...
};

function getOllamaModelFormat(modelName) {
  // Recherche du format appropri√© selon le nom du mod√®le
}
```

### Cache optimis√© des r√©ponses

```javascript
// Syst√®me simple de cache pour les r√©ponses
const responseCache = new Map();

// Exemple d'utilisation du cache
const cacheKey = JSON.stringify({
  persona,
  message,
  tirage: tirage.map(card => card.id),
  spreadType,
  langue
});

if (responseCache.has(cacheKey)) {
  return responseCache.get(cacheKey);
}
// ... g√©n√©ration de la r√©ponse ...
responseCache.set(cacheKey, fullResponse);
```

### Streaming efficace

Le service AIService impl√©mente un streaming efficace via une m√©thode d√©di√©e :

```javascript
async getOllamaStreamingResponse(prompt, systemPrompts, modelName, onChunk, signal) {
  // Nettoyer le nom du mod√®le
  const cleanModelName = modelName.replace('ollama:', '');
  
  // Construire le prompt complet
  const fullPrompt = [
    ...systemPrompts,
    prompt
  ].join('\n\n');
  
  // Corps de la requ√™te
  const body = {
    model: cleanModelName,
    prompt: fullPrompt,
    stream: true,
    options: {
      temperature: 0.7,
      num_predict: 1000
    }
  };
  
  // Effectuer la requ√™te
  const response = await fetch(API_URL_OLLAMA, options);
  
  // Lire le flux de r√©ponse
  const reader = response.body.getReader();
  const decoder = new TextDecoder();
  let completeResponse = '';
  
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    
    // Convertir et traiter les chunks
    const chunk = decoder.decode(value, { stream: true });
    const lines = chunk.split('\n').filter(line => line.trim() !== '');
    
    for (const line of lines) {
      const data = JSON.parse(line);
      if (data.response) {
        completeResponse += data.response;
        onChunk(data.response);
      }
    }
  }
  
  return completeResponse;
}
```

### Gestion robuste des erreurs

L'application g√®re de mani√®re robuste diff√©rents types d'erreurs :

```javascript
try {
  // Appel API
} catch (error) {
  this.isGenerating = false;
  
  if (this.handleAbortError(error)) {
    return "";
  }
  
  // Gestion plus d√©taill√©e des erreurs
  let errorMessage = "Une erreur est survenue lors de l'interpr√©tation.";
  
  if (!this.apiKey && model.startsWith('openai/')) {
    errorMessage = "La cl√© API OpenAI n'est pas configur√©e.";
  } else if (error.message.includes('timeout')) {
    errorMessage = "Le temps de r√©ponse a d√©pass√© la limite.";
  } else if (error.message.includes('connect')) {
    errorMessage = "Impossible de se connecter au service d'IA.";
  }
  
  throw new Error(errorMessage);
}
```

## Communication avec Ollama

### Format de Requ√™te

Pour g√©n√©rer une interpr√©tation, nous utilisons l'endpoint `/api/generate` (Ollama) ou le format correspondant pour OpenAI :

```javascript
// Format pour Ollama (m√©thode non streaming)
{
  model: cleanModelName,
  messages: [
    { role: 'system', content: systemContent },
    { role: 'user', content: prompt }
  ],
  stream: false
}

// Format pour le streaming Ollama
{
  model: cleanModelName,
  prompt: fullPrompt,
  stream: true,
  options: {
    temperature: 0.7,
    num_predict: 1000
  }
}

// Format pour OpenAI
{
  model: model,
  messages: [
    { role: 'system', content: systemContent },
    { role: 'user', content: prompt }
  ],
  temperature: 0.7
}
```

### Gestion du Streaming

Le streaming est impl√©ment√© selon ces √©tapes :

1. Cr√©ation d'une requ√™te avec `stream: true`
2. Utilisation de l'API ReadableStream via `response.body.getReader()`
3. D√©codage des chunks avec TextDecoder
4. Parsing du JSON ligne par ligne
5. Extraction du contenu via le callback `onChunk`

## Formats de R√©ponse

La configuration des formats de r√©ponse est d√©finie dans `config.js` et utilis√©e par `AIService` :

```javascript
// Exemple d'extraction du contenu adapt√© au format du mod√®le
const modelNameWithoutPrefix = model.replace('ollama:', '');
const modelFormat = getOllamaModelFormat(modelNameWithoutPrefix);
const responseKey = modelFormat.responseKey || "message.content";

return this.extractResponseContent(data, responseKey, modelNameWithoutPrefix);
```

## Test de Connectivit√©

Le syst√®me inclut des fonctions pour tester la disponibilit√© du serveur Ollama et des mod√®les :

```javascript
async testOllamaConnectivity(modelName = null) {
  try {
    // 1. V√©rifier d'abord les mod√®les d√©j√† charg√©s
    const loadedModels = await checkLoadedModels();
    if (modelName && loadedModels.models) {
      const isLoaded = loadedModels.models.some(m => m.name === modelName || m.model === modelName);
      // ...
    }

    // 2. Test de ping du serveur
    const pingResponse = await fetch(`${SETTINGS.OLLAMA_URL}/api/tags`, {
      signal: AbortSignal.timeout(TIMEOUTS.OLLAMA_CONNECT)
    });
    
    // ...

    // 3. Test rapide du mod√®le avec pr√©chargement
    const preloadResult = await preloadModel(modelName);
    // ...
  }
}
```

## Bonnes Pratiques

1. **Toujours v√©rifier la connectivit√©** avant d'envoyer des requ√™tes
2. **Utiliser le mode "prompt"** comme fallback si Ollama n'est pas disponible
3. **Nettoyer le nom du mod√®le** en retirant le pr√©fixe "ollama:"
4. **G√©rer les timeouts** appropri√©s selon l'op√©ration
5. **Utiliser le syst√®me de retry** pour les op√©rations sensibles
6. **Extraire correctement les r√©ponses** selon le format du mod√®le utilis√©

## D√©bogage

Le niveau de d√©bogage est configurable dans `config.js` :

```javascript
// Configuration de debug
const DEBUG_LEVEL = 1; // 0: aucun, 1: basique, 2: d√©taill√©, 3: verbeux
```

La classe AIService inclut un mode debug conditionnel :

```javascript
if (this.debugMode) {
  // Construire le prompt complet comme il sera envoy√© √† l'IA
  const fullPrompt = `${systemPrompts.join('\n\n')}\n\n${prompt}`;
  
  console.log('üì® PROMPT FINAL ENVOY√â √Ä L\'IA:');
  console.log(fullPrompt);
  
  // Afficher des informations suppl√©mentaires sur le persona
  if (PERSONAS[persona]) {
    const personaInstance = new PERSONAS[persona](language);
    console.log(`üßô‚Äç‚ôÇÔ∏è Persona: ${personaInstance.getName()}`);
    console.log(`üìù Description: ${personaInstance.getDescription()}`);
    console.log(`üîÆ Sp√©cialisations: ${personaInstance.getSpecializations().join(', ')}`);
  }
}
```

## Mode "Prompt"

Le syst√®me inclut un mode sp√©cial "prompt" qui fonctionne comme un fallback et un outil de d√©bogage :

```javascript
// Mode sp√©cial "prompt" (Sans IA)
if (model === 'prompt') {
  console.log('üìù Mode Prompt activ√© : affichage du prompt sans appel √† l\'IA');
  
  // Concat√©ner simplement les prompts syst√®me et utilisateur
  const fullPrompt = `${systemPrompts.join('\n\n')}\n\n${prompt}`;
  
  // Affichage minimal sans formatage particulier
  const response = `<div class="prompt-display">${fullPrompt}</div>`;
  
  this.isGenerating = false;
  return response;
}
```

## Limitations Connues

1. Certains mod√®les peuvent n√©cessiter beaucoup de RAM
2. Les temps de chargement initial peuvent √™tre longs
3. La qualit√© des r√©ponses d√©pend du mod√®le utilis√©
4. Le streaming peut √™tre instable sur certains mod√®les
5. Diff√©rents mod√®les utilisent diff√©rents formats de r√©ponse

## √âvolutions Futures

1. Support de mod√®les multimodaux (images)
2. Am√©lioration de la gestion de la m√©moire
3. Optimisation du syst√®me de cache
4. Interface de gestion des mod√®les Ollama
5. Support de nouveaux formats de mod√®les 