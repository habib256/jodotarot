/**
 * Fichier de configuration g√©n√©rale pour l'application Jodotarot
 * Ce fichier centralise tous les param√®tres utilis√©s par l'application pour faciliter la maintenance
 * et √©viter les duplications de code et les incoh√©rences de configuration.
 */

// Configuration de debug
const DEBUG_LEVEL = 1; // 0: aucun, 1: basique, 2: d√©taill√©, 3: verbeux

// Param√®tres utilisateur configurables
const SETTINGS = {
  // Param√®tres d'API
  API_KEY: "YOUR API KEY", // Cl√© API pour OpenAI (laisser vide pour utiliser Ollama)
  ENABLE_STREAMING: true,  // Activer la r√©ception des r√©ponses en temps r√©el
  MAX_TOKENS: 1000,        // Nombre maximum de tokens pour la r√©ponse
  TEMPERATURE: 0.7,        // Temp√©rature (cr√©ativit√©) de la g√©n√©ration
  
  // URLs des services
  OLLAMA_URL: "http://localhost:11434",  // URL du serveur Ollama local
  OPENAI_URL: "https://api.openai.com/v1", // URL de l'API OpenAI
  
  // Param√®tres par d√©faut
  DEFAULT_PERSONA: "tarologue",  // Persona par d√©faut
  DEFAULT_LANGUAGE: "fr",        // Langue par d√©faut
  DEFAULT_DECK: "set01",         // Jeu de cartes par d√©faut
  DEFAULT_SPREAD: "cross",       // Type de tirage par d√©faut
  DEFAULT_MODEL: "prompt",       // Mod√®le par d√©faut chang√© √† "prompt" pour plus de s√©curit√©
  
  // Param√®tres d'interface
  HIDE_PROMPT: false,      // Masquer le prompt envoy√© √† l'IA
  AUTO_SCROLL: true,       // D√©filement automatique des interpr√©tations
  DARK_MODE: false,        // Mode sombre
  
  // Nouveau param√®tre pour garantir que "prompt" est toujours disponible
  ALWAYS_AVAILABLE_MODELS: ["prompt"]  // Liste des mod√®les toujours disponibles
};

// Configuration des timeouts (en millisecondes)
const TIMEOUTS = {
  OLLAMA_CONNECT: 30000,    // 30 secondes pour la connexion initiale
  OLLAMA_MODEL_LOAD: 60000, // 60 secondes pour le chargement du mod√®le
  OLLAMA_RESPONSE: 120000,  // 120 secondes pour la g√©n√©ration de r√©ponse
  OLLAMA_CHECK: 5000,      // 5 secondes pour v√©rifier l'√©tat des mod√®les
  MAX_RETRIES: 3,          // Nombre maximum de tentatives
  RETRY_DELAY: 1000,       // D√©lai entre les tentatives (1 seconde)
  MODEL_LOAD_CHECK: 10000  // 10 secondes pour v√©rifier si un mod√®le est charg√©
};

// Configuration pour l'API OpenAI
const API_KEY = SETTINGS.API_KEY;
const API_URL_OPENAI = `${SETTINGS.OPENAI_URL}/chat/completions`;

// Configuration pour l'API Ollama (local)
const API_URL_OLLAMA = `${SETTINGS.OLLAMA_URL}/api/chat`;
const API_URL_OLLAMA_TAGS = `${SETTINGS.OLLAMA_URL}/api/tags`;

// Configuration des mod√®les Ollama
const OLLAMA_MODEL_FORMATS = {
  // Format de r√©ponse pour chaque famille de mod√®le
  // Chaque entr√©e contient:
  // - pattern: expression r√©guli√®re pour identifier le mod√®le
  // - responseKey: chemin vers la r√©ponse dans l'objet JSON
  // - description: description du format pour le d√©bogage
  "llama3.1": {
    pattern: /\bllama3\.1\b/i,
    responseKey: "choices.0.message.content",
    description: "Llama 3.1 (retourne choices[0].message.content)"
  },
  "llama3": {
    pattern: /\bllama3\b(?!\.1)/i, // llama3 mais pas llama3.1
    responseKey: "message.content",
    description: "Llama 3 (retourne message.content)"
  },
  "llama2": {
    pattern: /\bllama2\b/i,
    responseKey: "response",
    description: "Llama 2 (retourne response)"
  },
  "llama": {
    pattern: /\bllama\b(?!2|3)/i, // llama mais pas llama2 ou llama3
    responseKey: "response",
    description: "Llama original (retourne response)"
  },
  "mistral": {
    pattern: /\bmistral\b/i,
    responseKey: "message.content",
    description: "Mistral (retourne message.content)"
  },
  "mixtral": {
    pattern: /\bmixtral\b/i,
    responseKey: "message.content",
    description: "Mixtral (retourne message.content)"
  },
  "phi": {
    pattern: /\bphi\b/i,
    responseKey: "response",
    description: "Phi (retourne response)"
  },
  "gemma": {
    pattern: /\bgemma\b/i,
    responseKey: "response",
    description: "Gemma (retourne response)"
  },
  // Format par d√©faut si aucun match n'est trouv√©
  "default": {
    pattern: null,
    responseKey: "response",
    description: "Format par d√©faut (retourne response)"
  }
};

/**
 * Fonction pour obtenir le format de r√©ponse appropri√© pour un mod√®le Ollama
 * @param {string} modelName - Nom du mod√®le (ex: "llama3.1:latest")
 * @returns {Object} - Configuration du format de r√©ponse
 */
function getOllamaModelFormat(modelName) {
  if (!modelName) return OLLAMA_MODEL_FORMATS.default;
  
  // Normaliser le nom du mod√®le
  const normalizedModelName = modelName.toLowerCase().trim();
  
  // V√©rifier chaque format dans l'ordre de priorit√©
  for (const [family, config] of Object.entries(OLLAMA_MODEL_FORMATS)) {
    // Ignorer le format par d√©faut lors de la recherche
    if (family === 'default') continue;
    
    // V√©rifier si le pattern correspond
    if (config.pattern && config.pattern.test(normalizedModelName)) {
      if (DEBUG_LEVEL > 0) {
        console.log(`üîç DEBUG - Mod√®le "${modelName}" identifi√© comme "${family}" (${config.description})`);
      }
      return config;
    }
  }
  
  // Si aucun match, retourner le format par d√©faut
  if (DEBUG_LEVEL > 0) {
    console.log(`üîç DEBUG - Aucun format sp√©cifique trouv√© pour "${modelName}", utilisation du format par d√©faut`);
  }
  return OLLAMA_MODEL_FORMATS.default;
}

// Exposer SETTINGS globalement pour faciliter l'acc√®s depuis la console de d√©bogage
if (typeof window !== 'undefined') {
  window.JODOTAROT_SETTINGS = SETTINGS;
}

export {
  API_KEY,
  API_URL_OPENAI,
  API_URL_OLLAMA,
  API_URL_OLLAMA_TAGS,
  DEBUG_LEVEL,
  SETTINGS,
  getOllamaModelFormat,
  TIMEOUTS
}; 