/**
 * Service g√©rant les interactions avec diff√©rents mod√®les d'IA
 * Centralise les appels API et la gestion des r√©ponses
 * 
 * IMPORTANT: Toutes les configurations doivent √™tre import√©es depuis /assets/js/config.js
 * Ne pas red√©finir de constantes de configuration ici - importer uniquement depuis config.js
 */
import PERSONAS, { getPersonaPrompt } from '../models/personas/index.js';
import { createSpread } from '../models/spreads/index.js';
import { API_KEY, API_URL_OPENAI, API_URL_OLLAMA, API_URL_OLLAMA_TAGS, getOllamaModelFormat, DEBUG_LEVEL, TIMEOUTS } from '../config.js';
import { getMetaPrompt, getEmphasisText, enrichirPromptContextuel } from '../prompt.js';
import { testOllamaConnectivity } from '../api.js';

class AIService {
  /**
   * @param {StateManager} stateManager - Instance du gestionnaire d'√©tat
   */
  constructor(stateManager) {
    this.stateManager = stateManager;
    
    // D'abord, essayer de charger la cl√© API depuis le localStorage
    const savedApiKey = this.loadApiKey();
    
    // Si pas de cl√© sauvegard√©e, utiliser celle du fichier de configuration
    this.apiKey = savedApiKey || (API_KEY !== "YOUR API KEY" ? API_KEY : null);
    
    this.defaultModel = 'openai/gpt-3.5-turbo';
    this.baseUrl = {
      ollama: API_URL_OLLAMA.replace('/api/chat', ''),
      openai: API_URL_OPENAI.replace('/chat/completions', '')
    };
    
    // Activation du mode de d√©bogage selon le niveau d√©fini dans la configuration
    this.debugMode = typeof DEBUG_LEVEL !== 'undefined' && DEBUG_LEVEL > 1;
    
    // AbortController pour pouvoir annuler les requ√™tes en cours
    this.currentController = null;
    
    // Flag indiquant si une g√©n√©ration est en cours
    this.isGenerating = false;
  }
  
  /**
   * Annule l'interpr√©tation en cours si elle existe
   * @returns {boolean} Indique si une interpr√©tation a √©t√© annul√©e
   */
  cancelCurrentInterpretation() {
    if (this.currentController && this.isGenerating) {
      console.log('Annulation de l\'interpr√©tation en cours...');
      
      // Cr√©er une erreur d'annulation
      const abortError = new Error('Generation aborted by user');
      abortError.name = 'AbortError';
      
      // Annuler la requ√™te en cours
      this.currentController.abort();
      this.currentController = null;
      this.isGenerating = false;
      
      return true;
    }
    return false;
  }
  
  /**
   * R√©cup√®re la liste des mod√®les Ollama disponibles
   * @returns {Promise<Array>} Liste des mod√®les disponibles
   */
  async getOllamaModels() {
    try {
      console.log('Tentative de r√©cup√©ration des mod√®les Ollama...');
      const response = await fetch(API_URL_OLLAMA_TAGS);
      
      if (!response.ok) {
        console.error(`Erreur HTTP lors de la r√©cup√©ration des mod√®les Ollama: ${response.status}`);
        return [];
      }
      
      const data = await response.json();
      
      if (!data.models || !Array.isArray(data.models)) {
        console.error('Format de r√©ponse Ollama invalide:', data);
        return [];
      }
      
      // Filtrer les mod√®les pour ne garder que ceux qui sont complets
      const validModels = data.models.filter(model => {
        // Exclure les mod√®les partiels ou en cours de t√©l√©chargement
        return !model.name.includes('partial') && !model.name.includes('downloading');
      });
      
      console.log('Mod√®les Ollama r√©cup√©r√©s:', validModels);
      return validModels;
    } catch (error) {
      console.error('Erreur lors de la r√©cup√©ration des mod√®les Ollama:', error);
      return [];
    }
  }
  
  /**
   * Teste la disponibilit√© d'un mod√®le sp√©cifique
   * @param {string} modelName - Nom du mod√®le √† tester
   * @return {Promise<Object>} R√©sultat du test avec statut, disponibilit√© et d√©tails
   */
  async testModelAvailability(modelName) {
    try {
      // Le mode "prompt" est un cas sp√©cial qui :
      // 1. Est toujours disponible car il n'utilise aucun mod√®le d'IA
      // 2. Sert de fallback s√©curis√© quand aucun mod√®le n'est disponible
      // 3. Permet de voir le prompt qui serait envoy√© √† l'IA sans faire d'appel
      // 4. Utile pour le d√©bogage et la personnalisation des prompts
      if (modelName === 'prompt') {
        return {
          available: true,
          status: 'success',
          modelName: 'prompt',
          message: 'Mode Prompt toujours disponible',
          details: { mode: 'prompt' },
          suggestions: []
        };
      }

      const result = {
        available: false,
        status: 'pending',
        modelName,
        message: '',
        details: {},
        suggestions: []
      };

      // Augmenter les timeouts pour donner plus de temps au chargement des mod√®les
      // Utiliser des timeouts plus longs, surtout pour les grands mod√®les
      const timeout = modelName.includes('llama3.1') ? 
        TIMEOUTS.OLLAMA_MODEL_LOAD * 2 : // Doubler le timeout pour les grands mod√®les
        TIMEOUTS.OLLAMA_CONNECT * 1.5;   // Augmenter de 50% pour les autres mod√®les
      
      console.log(`Teste du mod√®le ${modelName} avec un timeout de ${timeout}ms`);

      // D√©tection am√©lior√©e des mod√®les Ollama
      // Gestion des mod√®les Ollama avec diff√©rents formats
      const isOllamaModel = 
        modelName.startsWith('ollama:') || 
        modelName.includes(':') ||
        !modelName.startsWith('openai/');
      
      if (isOllamaModel) {
        // Nettoyer le nom du mod√®le - supprimer les pr√©fixes et √©ventuellement isoler le nom de base
        let ollamaModelName = modelName;
        
        // Supprimer le pr√©fixe ollama: s'il existe
        if (ollamaModelName.startsWith('ollama:')) {
          ollamaModelName = ollamaModelName.replace('ollama:', '');
        }
        
        console.log(`Test de connectivit√© pour le mod√®le Ollama: ${ollamaModelName}`);
        
        try {
          // Utiliser Promise.race pour ajouter un timeout
          const availabilityPromise = testOllamaConnectivity(ollamaModelName);
          const timeoutPromise = new Promise((_, reject) => 
            setTimeout(() => reject(new Error(`Timeout lors du test de connectivit√© apr√®s ${timeout}ms`)), timeout)
          );
          
          const availability = await Promise.race([availabilityPromise, timeoutPromise]);
          
          result.available = availability.success;
          result.status = availability.success ? 'success' : 'error';
          result.message = availability.message;
          result.details = availability.details || {};
          
          // Ajouter plus d'informations de diagnostic
          if (this.debugMode) {
            console.log(`R√©sultat du test pour ${ollamaModelName}:`, availability);
          }
          
          // Sugg√©rer des alternatives en cas d'√©chec
          if (!availability.success) {
            result.suggestions.push('V√©rifier que le serveur Ollama est bien d√©marr√©');
            result.suggestions.push('V√©rifier que le mod√®le est correctement install√© dans Ollama');
            result.suggestions.push('V√©rifier la m√©moire syst√®me disponible');
            
            // Message sp√©cifique selon le type d'erreur
            if (availability.message && availability.message.includes('not found')) {
              result.suggestions.unshift(`Mod√®le "${ollamaModelName}" non trouv√©. Installez-le avec "ollama pull ${ollamaModelName}"`);
            }
            else if (availability.message && availability.message.includes('timeout')) {
              result.suggestions.unshift('Le mod√®le prend trop de temps √† charger, essayez un mod√®le plus petit');
            }
            else if (availability.message && availability.message.includes('connect')) {
              result.suggestions.unshift('Impossible de se connecter au serveur Ollama. V√©rifiez qu\'il est bien d√©marr√©');
            }
          }
        } catch (error) {
          console.error('Erreur lors du test de connectivit√© pour', modelName, ':', error);
          result.status = 'error';
          result.message = error.message;
          result.suggestions.push('R√©essayer dans quelques instants');
          result.suggestions.push('V√©rifier la m√©moire syst√®me disponible');
          
          // Ajouter plus d'informations pour le d√©bogage
          if (this.debugMode) {
            console.log(`D√©tails d'erreur pour ${ollamaModelName}:`, error);
            result.details.error = error.toString();
            result.details.stack = error.stack;
          }
        }
      }
      
      return result;
    } catch (error) {
      console.error('Erreur lors du test de disponibilit√© du mod√®le:', error);
      return {
        available: false,
        status: 'error',
        modelName,
        message: error.message,
        details: {},
        suggestions: ['R√©essayer plus tard']
      };
    }
  }
  
  /**
   * D√©finit la cl√© API OpenAI et la sauvegarde dans le localStorage
   * @param {string} apiKey - La cl√© API OpenAI
   */
  setApiKey(apiKey) {
    if (!apiKey || typeof apiKey !== 'string') {
      console.error('La cl√© API doit √™tre une cha√Æne de caract√®res valide');
      return;
    }
    
    try {
      // Enregistrer la cl√© en m√©moire
      this.apiKey = apiKey;
      
      // Sauvegarder dans le localStorage pour la persistance entre les sessions
      if (window.localStorage) {
        // Masquer l√©g√®rement la cl√© avant le stockage (n'est pas un cryptage s√©curis√©)
        const encodedKey = btoa(apiKey);
        localStorage.setItem('jodotarot_api_key', encodedKey);
        console.log('Cl√© API OpenAI sauvegard√©e dans le localStorage');
      }
    } catch (error) {
      console.error('Erreur lors de la sauvegarde de la cl√© API:', error);
    }
  }
  
  /**
   * Charge la cl√© API depuis le localStorage
   * @returns {string|null} La cl√© API ou null si non trouv√©e
   */
  loadApiKey() {
    try {
      if (window.localStorage) {
        const encodedKey = localStorage.getItem('jodotarot_api_key');
        if (encodedKey) {
          // D√©coder la cl√©
          const apiKey = atob(encodedKey);
          this.apiKey = apiKey;
          console.log('Cl√© API OpenAI charg√©e depuis le localStorage');
          return apiKey;
        }
      }
    } catch (error) {
      console.error('Erreur lors du chargement de la cl√© API:', error);
    }
    
    return null;
  }
  
  /**
   * Obtient l'interpr√©tation d'un tirage de tarot
   * @param {Array} reading - Les cartes tir√©es
   * @param {string} question - La question pos√©e
   * @param {string} persona - Le persona s√©lectionn√©
   * @param {string} model - Le mod√®le d'IA √† utiliser
   * @param {string} language - La langue (par d√©faut: 'fr')
   * @param {string} spreadType - Le type de tirage
   * @param {Function} onChunk - Callback pour le streaming (optionnel)
   * @return {Promise<string>} L'interpr√©tation du tirage
   */
  async getInterpretation(reading, question, persona, model, language = 'fr', spreadType = 'cross', onChunk = null) {
    try {
      // Annuler toute interpr√©tation en cours
      this.cancelCurrentInterpretation();
      
      // Cr√©er un nouvel AbortController
      this.currentController = new AbortController();
      this.isGenerating = true;
      
      // V√©rifier les param√®tres essentiels
      if (!reading || !reading.length || !question.trim()) {
        throw new Error('Les cartes et la question sont requises pour l\'interpr√©tation');
      }
      
      const systemPrompts = await this.buildSystemPrompts(persona, language, spreadType);
      const prompt = this.buildPrompt(reading, question, language, spreadType);
      
      // Mode sp√©cial "prompt" (Sans IA)
      // Ce mode est une fonctionnalit√© de s√©curit√© et de d√©bogage qui :
      // 1. Est toujours disponible m√™me sans connexion √† un service d'IA
      // 2. Affiche les prompts syst√®me et utilisateur qui seraient envoy√©s √† l'IA
      // 3. Permet de v√©rifier et ajuster les prompts sans faire d'appels API
      // 4. Sert de solution de repli si aucun mod√®le d'IA n'est disponible
      // 5. Aide √† la compr√©hension du syst√®me de prompts pour les d√©veloppeurs
      if (model === 'prompt') {
        console.log('üìù Mode Prompt activ√© : affichage du prompt sans appel √† l\'IA');
        
        // Concat√©ner simplement les prompts syst√®me et utilisateur
        const fullPrompt = `${systemPrompts.join('\n\n')}\n\n${prompt}`;
        
        // Affichage minimal sans formatage particulier
        const response = `<div class="prompt-display">${fullPrompt}</div>`;
        
        this.isGenerating = false;
        return response;
      }
      
      // Continuer avec le reste de la logique pour les autres mod√®les...
      
      // Afficher uniquement le prompt final
      if (this.debugMode) {
        // Construire le prompt complet comme il sera envoy√© √† l'IA
        const fullPrompt = `${systemPrompts.join('\n\n')}\n\n${prompt}`;
        
        console.log('üì® PROMPT FINAL ENVOY√â √Ä L\'IA:');
        console.log(fullPrompt);
        
        // Afficher des informations suppl√©mentaires sur le persona si possible
        if (PERSONAS[persona]) {
          const personaInstance = new PERSONAS[persona](language);
          console.log(`üßô‚Äç‚ôÇÔ∏è Persona: ${personaInstance.getName()}`);
          console.log(`üìù Description: ${personaInstance.getDescription()}`);
          console.log(`üîÆ Sp√©cialisations: ${personaInstance.getSpecializations().join(', ')}`);
        }
      }
      
      // Obtenir la r√©ponse selon le type de mod√®le (OpenAI ou Ollama)
      let response;
      
      if (model.startsWith('openai/')) {
        response = await this.getOpenAIResponse(prompt, systemPrompts, model.replace('openai/', ''));
      } else {
        // Si un callback de streaming est fourni, utiliser le streaming pour Ollama
        if (onChunk && typeof onChunk === 'function') {
          try {
            response = await this.getOllamaStreamingResponse(prompt, systemPrompts, model, onChunk, this.currentController.signal);
          } catch (error) {
            // Si l'erreur est due √† une annulation, ne pas rejeter mais retourner
            if (error.name === 'AbortError') {
              console.log('Interpr√©tation annul√©e par l\'utilisateur');
              this.isGenerating = false;
              return "";
            }
            throw error;
          }
        } else {
          response = await this.getOllamaResponse(prompt, systemPrompts, model);
        }
      }
      
      // Marquer la g√©n√©ration comme termin√©e
      this.isGenerating = false;
      return response;
    } catch (error) {
      this.isGenerating = false;
      console.error("Erreur lors de l'obtention de l'interpr√©tation:", error);
      throw error;
    }
  }
  
  /**
   * Construit les prompts syst√®me pour le mod√®le d'IA
   * @param {string} persona - Persona choisi
   * @param {string} language - Langue
   * @param {string} spreadType - Type de tirage
   * @return {Promise<Array>} Liste des prompts syst√®me
   */
  async buildSystemPrompts(persona, language, spreadType) {
    console.log(`üîç Chargement du prompt pour le persona: ${persona}, langue: ${language}, tirage: ${spreadType}`);
    
    try {
      // R√©cup√©rer le prompt sp√©cifique au persona via getPersonaPrompt
      // (maintenant enrichi avec les sp√©cialisations gr√¢ce √† notre am√©lioration de BasePersona)
      const personaPrompt = await getPersonaPrompt(persona, language, spreadType);
      
      console.log('üìã Contenu du prompt persona:', personaPrompt);
      
      // R√©cup√©rer le m√©taprompt adapt√© √† la langue
      const metaPrompt = getMetaPrompt(language);
      
      // Prompts de base - le prompt du persona et le m√©taprompt
      const basePrompts = [
        // Le m√©taprompt pour d√©finir les r√®gles g√©n√©rales
        metaPrompt,
        // Le prompt principal du persona, d√©j√† enrichi
        personaPrompt
      ];
      
      return basePrompts;
    } catch (error) {
      console.error("Erreur lors du chargement des prompts syst√®me:", error);
      // Retourner au moins le m√©taprompt en cas d'erreur
      return [getMetaPrompt(language)];
    }
  }
  
  /**
   * Construit le prompt principal pour l'interpr√©tation
   * @param {Array} reading - Tableau des cartes tir√©es
   * @param {string} question - Question pos√©e
   * @param {string} language - Code de langue 
   * @param {string} spreadType - Type de tirage
   * @return {string} Prompt format√©
   */
  buildPrompt(reading, question, language, spreadType = 'cross') {
    // S'assurer que reading est un tableau
    if (!Array.isArray(reading)) {
      console.error('Le param√®tre reading doit √™tre un tableau de cartes');
      return `Question: ${question}\n\nErreur: Format de tirage invalide`;
    }
    
    // Cr√©er une instance temporaire du tirage pour g√©n√©rer une description riche
    const spreadInstance = createSpread(spreadType, null, language);
    
    // Copier les cartes dans l'instance de tirage
    spreadInstance.cards = [...reading];
    
    // G√©n√©rer une description d√©taill√©e du tirage avec les cartes
    const spreadDescription = spreadInstance.generateReadingDescription(true);
    
    // Construction du prompt de base
    let promptBase = `${spreadDescription}`;
    
    // Enrichir le prompt avec la question et le texte d'emphase
    return enrichirPromptContextuel(question, promptBase, language);
  }
  
  /**
   * Obtient une r√©ponse d'OpenAI
   * @param {string} prompt - Le prompt principal
   * @param {Array} systemPrompts - Les prompts syst√®me
   * @param {string} model - Le mod√®le OpenAI √† utiliser
   * @return {Promise<string>} La r√©ponse g√©n√©r√©e
   */
  async getOpenAIResponse(prompt, systemPrompts, model) {
    if (!this.apiKey) {
      throw new Error('Cl√© API OpenAI non configur√©e');
    }
    
    try {
      const systemContent = systemPrompts.join('\n');
      
      // Format des messages pour OpenAI
      const messages = [
        { role: 'system', content: systemContent },
        { role: 'user', content: prompt }
      ];
      
      // Supprimer les logs d√©taill√©s
      
      const response = await fetch(API_URL_OPENAI, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: model,
          messages: messages,
          temperature: 0.7
        })
      });
      
      if (!response.ok) {
        throw new Error(`Erreur API OpenAI: ${response.status}`);
      }
      
      const data = await response.json();
      
      // Supprimer les logs d√©taill√©s
      
      return data.choices[0]?.message?.content || '';
    } catch (error) {
      console.error('Erreur lors de l\'appel √† OpenAI:', error);
      throw error;
    }
  }
  
  /**
   * Utilitaire pour les requ√™tes fetch avec timeout et r√©essai
   * @param {string} url - URL de la requ√™te
   * @param {Object} options - Options de fetch
   * @param {number} maxRetries - Nombre maximum de tentatives
   * @param {number} timeoutMs - D√©lai d'expiration en millisecondes
   * @return {Promise<Response>} - Promesse de r√©ponse
   */
  async fetchWithRetry(url, options, maxRetries = TIMEOUTS.MAX_RETRIES, timeoutMs = TIMEOUTS.OLLAMA_CONNECT) {
    let retries = 0;
    let lastError = null;
    
    while (retries <= maxRetries) {
      try {
        // Cr√©er un contr√¥leur d'abandon pour le timeout
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), timeoutMs);
        
        // Ajouter le signal au options
        const optionsWithSignal = {
          ...options,
          signal: controller.signal
        };
        
        try {
          // Tenter la requ√™te
          const response = await fetch(url, optionsWithSignal);
          clearTimeout(timeoutId);
          
          // Si la r√©ponse est OK, la retourner
          if (response.ok) {
            return response;
          }
          
          // Sinon, lire le texte d'erreur et le lancer
          const errorText = await response.text();
          throw new Error(`Erreur API (${response.status}): ${errorText.slice(0, 100)}`);
        } catch (fetchError) {
          clearTimeout(timeoutId);
          throw fetchError;
        }
      } catch (error) {
        lastError = error;
        retries++;
        
        const isTimeout = error.name === 'AbortError';
        const retriesLeft = maxRetries - retries + 1;
        
        if (retriesLeft > 0) {
          // D√©lai exponentiel avec un peu d'al√©atoire pour √©viter les collisions
          const delay = Math.pow(2, retries) * 1000 + Math.random() * 500;
          console.warn(`Tentative ${retries}/${maxRetries} √©chou√©e${isTimeout ? ' (timeout)' : ''}: ${error.message}. Nouvelle tentative dans ${delay/1000} secondes...`);
          
          // Attendre avant de r√©essayer
          await new Promise(resolve => setTimeout(resolve, delay));
        } else {
          console.error(`Toutes les tentatives ont √©chou√© (${retries}/${maxRetries}):`, error);
          throw error;
        }
      }
    }
    
    // Ce code ne devrait jamais √™tre atteint, mais par pr√©caution
    throw lastError || new Error("Erreur inconnue pendant les tentatives de connexion");
  }
  
  /**
   * Obtient une r√©ponse d'Ollama
   * @param {string} prompt - Le prompt principal
   * @param {Array} systemPrompts - Les prompts syst√®me
   * @param {string} model - Le mod√®le Ollama √† utiliser
   * @return {Promise<string>} La r√©ponse g√©n√©r√©e
   */
  async getOllamaResponse(prompt, systemPrompts, model) {
    // Construction du payload pour l'API chat
    const systemContent = systemPrompts.join('\n');
    const payload = {
      model: model.replace('ollama:', ''), // Supprimer le pr√©fixe "ollama:" si pr√©sent
      messages: [
        { role: 'system', content: systemContent },
        { role: 'user', content: prompt }
      ],
      stream: false
    };
    
    console.log("üîç DEBUG getOllamaResponse - Payload:", JSON.stringify(payload, null, 2));
    
    try {
      // Obtenir le format de r√©ponse pour ce mod√®le
      const modelNameWithoutPrefix = model.replace('ollama:', '');
      const modelFormat = getOllamaModelFormat(modelNameWithoutPrefix);
      const responseKey = modelFormat.responseKey || "message.content";
      
      if (this.debugMode) {
        console.log(`üîç DEBUG getOllamaResponse - Format d√©tect√© pour ${modelNameWithoutPrefix}: ${modelFormat.description || responseKey}`);
      }
      
      // Utiliser fetchWithRetry pour une meilleure r√©silience
      const response = await this.fetchWithRetry(
        API_URL_OLLAMA, 
        {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(payload)
        },
        2,  // maxRetries
        20000 // timeoutMs: 20 secondes
      );
      
      console.log("üîç DEBUG getOllamaResponse - Statut r√©ponse:", response.status);
      
      // Traiter la r√©ponse
      const data = await response.json();
      if (this.debugMode) {
        console.log("üîç DEBUG getOllamaResponse - R√©ponse re√ßue:", data);
      }
      
      // Utiliser la m√©thode centralis√©e pour extraire le contenu
      const responseContent = this.extractResponseContent(data, responseKey, modelNameWithoutPrefix);
      
      return responseContent;
    } catch (error) {
      console.error('Erreur lors de l\'appel √† Ollama apr√®s plusieurs tentatives:', error);
      throw error;
    }
  }
  
  /**
   * Obtient une r√©ponse en streaming d'Ollama
   * @param {string} prompt - Le prompt utilisateur
   * @param {Array} systemPrompts - Les prompts syst√®me
   * @param {string} modelName - Nom du mod√®le Ollama
   * @param {Function} onChunk - Callback pour chaque morceau de r√©ponse
   * @param {AbortSignal} signal - Signal pour annuler la requ√™te
   * @return {Promise<string>} La r√©ponse compl√®te
   */
  async getOllamaStreamingResponse(prompt, systemPrompts, modelName, onChunk, signal) {
    // Nettoyer le nom du mod√®le (enlever le pr√©fixe ollama: si pr√©sent)
    const cleanModelName = modelName.replace('ollama:', '');
    
    // Construire le prompt complet en combinant les prompts syst√®me et le prompt utilisateur
    const fullPrompt = [
      ...systemPrompts,
      prompt
    ].join('\n\n');
    
    // Corps de la requ√™te pour Ollama
    const body = {
      model: cleanModelName,
      prompt: fullPrompt,
      stream: true,
      options: {
        temperature: 0.7,
        num_predict: 1000
      }
    };
    
    try {
      if (this.debugMode) {
        console.log(`üîÑ Envoi de la requ√™te en streaming √† Ollama (${cleanModelName})`);
        console.log('Prompt complet:', fullPrompt);
        console.log('Corps de la requ√™te:', body);
      }
      
      // V√©rifier que le callback est bien une fonction
      if (typeof onChunk !== 'function') {
        throw new Error("Le callback onChunk doit √™tre une fonction");
      }
      
      // Options de la requ√™te
      const options = {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(body),
        signal // Utiliser le signal d'annulation
      };
      
      // Effectuer la requ√™te
      const response = await fetch(API_URL_OLLAMA, options);
      
      if (!response.ok) {
        const errorData = await response.text();
        throw new Error(`Erreur Ollama [${response.status}]: ${errorData}`);
      }
      
      // Initialiser le lecteur de flux
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let completeResponse = '';
      
      // Lire le flux de r√©ponse
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        // Convertir les donn√©es binaires en texte
        const chunk = decoder.decode(value, { stream: true });
        
        // Traiter les lignes JSON individuelles
        const lines = chunk.split('\n').filter(line => line.trim() !== '');
        
        for (const line of lines) {
          try {
            // Analyser chaque ligne comme un objet JSON
            const data = JSON.parse(line);
            
            // Extraire le contenu de la r√©ponse
            if (data.response) {
              completeResponse += data.response;
              onChunk(data.response);
            }
          } catch (error) {
            console.error("Erreur lors de l'analyse du chunk JSON:", error);
            // Ne pas interrompre le traitement en cas d'erreur sur un chunk
          }
        }
      }
      
      return completeResponse;
    } catch (error) {
      // Propager l'erreur d'annulation
      if (error.name === 'AbortError') {
        throw error;
      }
      
      console.error("Erreur lors de l'obtention de la r√©ponse streaming Ollama:", error);
      throw new Error(`Erreur lors de la communication avec Ollama: ${error.message}`);
    }
  }
  
  /**
   * Extrait le contenu de la r√©ponse selon le format du mod√®le
   * @param {Object} data - Donn√©es de r√©ponse JSON
   * @param {string} responseKey - Chemin d'acc√®s √† la r√©ponse (ex: "message.content")
   * @param {string} modelName - Nom du mod√®le pour le d√©bogage
   * @return {string} - Le contenu extrait ou cha√Æne vide si non trouv√©
   */
  extractResponseContent(data, responseKey, modelName = "inconnu") {
    if (!data) return "";
    
    // Mode debug pour diagnostiquer les r√©ponses
    if (this.debugMode) {
      console.log(`üîç DEBUG extractResponseContent - Structure de donn√©es pour ${modelName}:`, data);
    }
    
    // 1. Essayer d'extraire selon le chemin d√©fini dans le format du mod√®le
    try {
      const parts = responseKey.split('.');
      let content = data;
      
      for (const part of parts) {
        if (!content || typeof content !== 'object') {
          if (this.debugMode) {
            console.warn(`‚ö†Ô∏è Chemin ${responseKey} interrompu √† "${part}" pour le mod√®le ${modelName}`);
          }
          break;
        }
        content = content[part];
      }
      
      if (content && typeof content === 'string') {
        if (this.debugMode) {
          console.log(`‚úÖ Contenu extrait avec chemin "${responseKey}" pour le mod√®le ${modelName}`);
        }
        return content;
      }
    } catch (e) {
      console.warn(`Erreur lors de l'extraction via ${responseKey}:`, e.message);
    }
    
    // 2. Tentatives de r√©cup√©ration par ordre de priorit√©
    // Format message.content (API chat standard)
    if (data.message && data.message.content) {
      if (this.debugMode) {
        console.log(`‚ÑπÔ∏è Contenu extrait depuis message.content pour ${modelName}`);
      }
      return data.message.content;
    }
    
    // Format response (certains mod√®les Ollama)
    if (data.response) {
      if (this.debugMode) {
        console.log(`‚ÑπÔ∏è Contenu extrait depuis response pour ${modelName}`);
      }
      return data.response;
    }
    
    // Format content (certains mod√®les Ollama exp√©rimentaux)
    if (data.content) {
      if (this.debugMode) {
        console.log(`‚ÑπÔ∏è Contenu extrait depuis content pour ${modelName}`);
      }
      return data.content;
    }
    
    // Format sp√©cifique llama3.1
    if (modelName.includes('llama3.1') && data.choices && data.choices.length > 0) {
      const choice = data.choices[0];
      
      if (choice.message && choice.message.content) {
        if (this.debugMode) {
          console.log(`‚ÑπÔ∏è Contenu extrait depuis choices[0].message.content pour ${modelName}`);
        }
        return choice.message.content;
      }
      
      if (choice.content) {
        if (this.debugMode) {
          console.log(`‚ÑπÔ∏è Contenu extrait depuis choices[0].content pour ${modelName}`);
        }
        return choice.content;
      }
      
      if (choice.text) {
        if (this.debugMode) {
          console.log(`‚ÑπÔ∏è Contenu extrait depuis choices[0].text pour ${modelName}`);
        }
        return choice.text;
      }
    }
    
    // 3. Tentative d√©sesp√©r√©e: chercher un champ qui pourrait contenir du texte
    const excludedKeys = ['model', 'id', 'object', 'created', 'usage', 'system_fingerprint'];
    
    for (const key of Object.keys(data)) {
      // Ignorer les champs de m√©tadonn√©es qui ne devraient pas contenir le contenu principal
      if (excludedKeys.includes(key)) continue;
      
      const value = data[key];
      if (typeof value === 'string' && value.length > 10) {
        if (this.debugMode) {
          console.warn(`‚ö†Ô∏è Fallback: contenu extrait depuis champ "${key}" pour ${modelName}`);
        }
        return value;
      }
      
      // V√©rifier √©galement les objets imbriqu√©s de premier niveau
      if (value && typeof value === 'object') {
        for (const subKey of Object.keys(value)) {
          const subValue = value[subKey];
          if (typeof subValue === 'string' && subValue.length > 10) {
            if (this.debugMode) {
              console.warn(`‚ö†Ô∏è Fallback: contenu extrait depuis champ "${key}.${subKey}" pour ${modelName}`);
            }
            return subValue;
          }
        }
      }
    }
    
    // 4. √âchec total: retourner une cha√Æne vide ou un message d'erreur
    console.error(`‚ùå Impossible d'extraire le contenu pour le mod√®le ${modelName}. Format de r√©ponse inconnu:`, data);
    return "";
  }
  
  /**
   * Teste la connectivit√© avec Ollama
   * @returns {Promise<Object>} R√©sultat du test avec des informations d√©taill√©es
   */
  async testOllamaConnectivity() {
    // Le mode "prompt" est une option sp√©ciale qui :
    // 1. Ne n√©cessite aucune connectivit√© r√©seau
    // 2. Est toujours consid√©r√© comme disponible
    // 3. Permet de continuer √† utiliser l'application sans IA
    // 4. Sert de solution de secours en cas de probl√®me de connexion
    if (this.stateManager?.getState()?.iaModel === 'prompt') {
      return {
        status: 'success',
        success: true,
        message: 'Mode Prompt toujours disponible',
        details: { mode: 'prompt' },
        suggestions: []
      };
    }

    try {
      // Utiliser la fonction am√©lior√©e de l'API
      const result = await testOllamaConnectivity();
      return result;
    } catch (error) {
      console.error("Erreur lors du test de connectivit√© Ollama:", error);
      return {
        status: 'error',
        success: false,
        message: `Erreur lors du test de connectivit√©: ${error.message}`,
        details: error,
        suggestions: ['warnings.unexpectedError', 'warnings.tryAgain']
      };
    }
  }

  /**
   * M√©thode utilitaire centralis√©e pour la gestion des erreurs
   * @param {Error} error - L'erreur attrap√©e
   * @param {string} context - Contexte dans lequel l'erreur s'est produite
   * @param {boolean} rethrow - Si true, l'erreur sera relanc√©e apr√®s traitement
   * @returns {Object|null} - Un objet d'erreur format√© ou null
   */
  handleServiceError(error, context, rethrow = false) {
    // Standardiser le format de l'erreur
    const errorInfo = {
      message: error.message || 'Erreur inconnue',
      context: context,
      timestamp: new Date().toISOString(),
      type: error.name || 'Error',
      stack: this.debugMode ? error.stack : null
    };
    
    // Log de l'erreur avec contexte
    console.error(`AIService - Erreur dans ${context}:`, errorInfo);
    
    // Mettre √† jour l'√©tat
    this.isGenerating = false;
    
    // Relancer l'erreur si demand√©
    if (rethrow) {
      throw error;
    }
    
    return errorInfo;
  }

  /**
   * Traiter les erreurs sp√©cifiques √† l'API OpenAI
   * @param {Error} error - L'erreur d'origine
   * @param {string} context - Contexte dans lequel l'erreur s'est produite
   * @returns {string} - Message d'erreur utilisateur
   */
  handleOpenAIError(error, context) {
    let userMessage = "Une erreur est survenue lors de la communication avec OpenAI.";
    
    // V√©rifier si c'est une erreur de cl√© API ou d'authentification
    if (error.message && (
        error.message.includes('API key') ||
        error.message.includes('authentication') ||
        error.message.includes('401')
    )) {
      userMessage = "Erreur d'authentification OpenAI. V√©rifiez votre cl√© API.";
      
      // R√©initialiser la cl√© API dans le localStorage
      localStorage.removeItem('openai_api_key');
    }
    
    this.handleServiceError(error, context);
    return userMessage;
  }

  /**
   * Traiter les erreurs sp√©cifiques √† Ollama
   * @param {Error} error - L'erreur d'origine
   * @param {string} context - Contexte dans lequel l'erreur s'est produite
   * @returns {string} - Message d'erreur utilisateur
   */
  handleOllamaError(error, context) {
    let userMessage = "Une erreur est survenue lors de la communication avec Ollama.";
    
    // V√©rifier si c'est une erreur de connexion
    if (error.message && (
        error.message.includes('network') ||
        error.message.includes('fetch') ||
        error.message.includes('connect')
    )) {
      userMessage = "Impossible de se connecter √† Ollama. V√©rifiez que le service est bien lanc√©.";
    }
    
    this.handleServiceError(error, context);
    return userMessage;
  }
}

export default AIService; 