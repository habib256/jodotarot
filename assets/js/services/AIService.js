/**
 * Service g√©rant les interactions avec diff√©rents mod√®les d'IA
 * Centralise les appels API et la gestion des r√©ponses
 */
import PERSONAS, { getPersonaPrompt } from '../models/personas/index.js';
import { createSpread } from '../models/spreads/index.js';
import { API_KEY, API_URL_OPENAI, API_URL_OLLAMA, API_URL_OLLAMA_TAGS, getOllamaModelFormat, DEBUG_LEVEL } from '../config.js';
import { getMetaPrompt, getEmphasisText, enrichirPromptContextuel } from '../prompt.js';
import { testOllamaConnectivity } from '../api.js';

class AIService {
  constructor() {
    // D'abord, essayer de charger la cl√© API depuis le localStorage
    const savedApiKey = this.loadApiKey();
    
    // Si pas de cl√© sauvegard√©e, utiliser celle du fichier de configuration
    this.apiKey = savedApiKey || (API_KEY !== "YOUR API KEY" ? API_KEY : null);
    
    this.defaultModel = 'openai/gpt-3.5-turbo';
    this.baseUrl = {
      ollama: API_URL_OLLAMA.replace('/api/chat', ''),
      openai: API_URL_OPENAI.replace('/chat/completions', '')
    };
    
    // Activation du mode de d√©bogage selon le niveau d√©fini dans la configuration
    this.debugMode = typeof DEBUG_LEVEL !== 'undefined' && DEBUG_LEVEL > 1;
    
    // Cache pour les r√©sultats d'interpr√©tation
    this.interpreterCache = {};
  }
  
  /**
   * R√©cup√®re la liste des mod√®les Ollama disponibles
   * @returns {Promise<Array>} Liste des mod√®les disponibles
   */
  async getOllamaModels() {
    try {
      console.log('Tentative de r√©cup√©ration des mod√®les Ollama...');
      const response = await fetch(API_URL_OLLAMA_TAGS);
      
      if (!response.ok) {
        console.error(`Erreur HTTP lors de la r√©cup√©ration des mod√®les Ollama: ${response.status}`);
        return [];
      }
      
      const data = await response.json();
      
      if (!data.models || !Array.isArray(data.models)) {
        console.error('Format de r√©ponse Ollama invalide:', data);
        return [];
      }
      
      // Filtrer les mod√®les pour ne garder que ceux qui sont complets
      const validModels = data.models.filter(model => {
        // Exclure les mod√®les partiels ou en cours de t√©l√©chargement
        return !model.name.includes('partial') && !model.name.includes('downloading');
      });
      
      console.log('Mod√®les Ollama r√©cup√©r√©s:', validModels);
      return validModels;
    } catch (error) {
      console.error('Erreur lors de la r√©cup√©ration des mod√®les Ollama:', error);
      return [];
    }
  }
  
  /**
   * Teste la disponibilit√© d'un mod√®le sp√©cifique
   * @param {string} modelName - Nom du mod√®le √† tester
   * @return {Promise<Object>} R√©sultat du test avec statut, disponibilit√© et d√©tails
   */
  async testModelAvailability(modelName) {
    try {
      console.log(`Test de disponibilit√© du mod√®le: ${modelName}`);
      
      // R√©sultat par d√©faut
      const result = {
        available: false,
        status: 'error',
        modelName,
        message: '',
        details: {},
        suggestions: []
      };
      
      // V√©rifier le format du mod√®le
      if (!modelName) {
        result.message = 'Aucun mod√®le sp√©cifi√©';
        return result;
      }
      
      // D√©finir un timeout pour le test de connectivit√©
      // Timeout plus long pour les mod√®les plus complexes comme llama3.1
      const timeout = modelName.includes('llama3.1') ? 20000 : 10000; // 20 secondes pour llama3.1, 10 secondes pour les autres
      
      // Gestion des mod√®les Ollama (y compris avec format llama3.1:latest)
      if (modelName.startsWith('ollama:') || modelName.includes(':')) {
        const ollamaModelName = modelName.startsWith('ollama:') 
          ? modelName.replace('ollama:', '') 
          : modelName; // Pour g√©rer les cas comme llama3.1:latest directement
        
        try {
          // Utiliser Promise.race pour ajouter un timeout
          const availabilityPromise = testOllamaConnectivity(ollamaModelName);
          const timeoutPromise = new Promise((_, reject) => 
            setTimeout(() => reject(new Error('Timeout lors du test de connectivit√©')), timeout)
          );
          
          const availability = await Promise.race([availabilityPromise, timeoutPromise]);
          
          result.available = availability.success;
          result.status = availability.success ? 'success' : 'error';
          result.message = availability.message;
          result.details = availability.details || {};
          
          // Sugg√©rer des alternatives en cas d'√©chec
          if (!availability.success) {
            result.suggestions.push('V√©rifier que le serveur Ollama est bien d√©marr√©');
            result.suggestions.push('V√©rifier que le mod√®le est correctement install√© dans Ollama');
          }
        } catch (error) {
          console.warn(`Erreur lors du test de connectivit√© pour ${modelName}:`, error);
          result.message = `Erreur lors du test du mod√®le: ${error.message}`;
          result.suggestions.push('V√©rifier que le serveur Ollama est bien d√©marr√©');
          
          // Si c'est un timeout, ajouter des suggestions sp√©cifiques
          if (error.message.includes('Timeout')) {
            result.message = `Le serveur Ollama ne r√©pond pas dans le d√©lai imparti (${timeout/1000}s)`;
            result.suggestions.push('V√©rifier la charge du serveur Ollama');
            result.suggestions.push('Augmenter le timeout dans les param√®tres');
          }
        }
        
        // Retourner explicitement le r√©sultat pour les mod√®les Ollama
        return result;
      } else if (modelName.startsWith('openai/')) {
        // Test de connectivit√© pour OpenAI
        const modelId = modelName.replace('openai/', '');
        result.details.type = 'openai';
        result.details.modelId = modelId;
        
        if (!this.apiKey) {
          result.message = 'Cl√© API OpenAI manquante';
          result.suggestions.push('Configurer une cl√© API');
          return result;
        }
        
        try {
          const response = await this.fetchWithRetry(
            `${this.baseUrl.openai}/models`, 
            {
              headers: {
                'Authorization': `Bearer ${this.apiKey}`,
                'Content-Type': 'application/json'
              }
            },
            1,  // maxRetries
            5000 // timeoutMs: 5 secondes
          );
          
          if (!response.ok) {
            result.message = `Erreur OpenAI: ${response.status} ${response.statusText}`;
            result.details.statusCode = response.status;
            
            // Suggestions sp√©cifiques selon le code d'erreur
            if (response.status === 401) {
              result.suggestions.push('V√©rifier la validit√© de la cl√© API');
            } else if (response.status === 429) {
              result.suggestions.push('Limite de requ√™tes atteinte, r√©essayer plus tard');
            } else {
              result.suggestions.push('V√©rifier la connexion au service OpenAI');
            }
            
            return result;
          }
          
          // API accessible, v√©rifier si le mod√®le sp√©cifique est disponible
          const data = await response.json();
          const availableModels = data.data || [];
          const modelExists = availableModels.some(m => m.id === modelId);
          
          if (modelExists) {
            result.available = true;
            result.status = 'success';
            result.message = `Mod√®le ${modelId} disponible`;
          } else {
            result.message = `Mod√®le ${modelId} non trouv√© dans la liste des mod√®les disponibles`;
            result.suggestions.push('V√©rifier le nom du mod√®le sp√©cifi√©');
            result.suggestions.push('Utiliser un mod√®le standard comme gpt-3.5-turbo');
          }
          
          return result;
        } catch (error) {
          result.message = `Erreur de connexion √† OpenAI: ${error.message}`;
          result.suggestions.push('V√©rifier la connexion internet');
          result.suggestions.push('V√©rifier la disponibilit√© des serveurs OpenAI');
          return result;
        }
        
      } else {
        // Type de mod√®le non reconnu
        result.message = `Type de mod√®le non reconnu: ${modelName}`;
        result.details.invalidPrefix = true;
        result.suggestions.push('Utiliser un pr√©fixe valide: openai/ ou ollama:');
        return result;
      }
    } catch (error) {
      console.error(`Erreur lors du test de disponibilit√© pour ${modelName}:`, error);
      return {
        available: false,
        status: 'error',
        modelName,
        message: `Erreur inattendue: ${error.message}`,
        details: { unexpectedError: true },
        suggestions: ['V√©rifier les logs pour plus de d√©tails']
      };
    }
  }
  
  /**
   * D√©finit la cl√© API OpenAI et la sauvegarde dans le localStorage
   * @param {string} apiKey - La cl√© API OpenAI
   */
  setApiKey(apiKey) {
    if (!apiKey || typeof apiKey !== 'string') {
      console.error('La cl√© API doit √™tre une cha√Æne de caract√®res valide');
      return;
    }
    
    try {
      // Enregistrer la cl√© en m√©moire
      this.apiKey = apiKey;
      
      // Sauvegarder dans le localStorage pour la persistance entre les sessions
      if (window.localStorage) {
        // Masquer l√©g√®rement la cl√© avant le stockage (n'est pas un cryptage s√©curis√©)
        const encodedKey = btoa(apiKey);
        localStorage.setItem('jodotarot_api_key', encodedKey);
        console.log('Cl√© API OpenAI sauvegard√©e dans le localStorage');
      }
    } catch (error) {
      console.error('Erreur lors de la sauvegarde de la cl√© API:', error);
    }
  }
  
  /**
   * Charge la cl√© API depuis le localStorage
   * @returns {string|null} La cl√© API ou null si non trouv√©e
   */
  loadApiKey() {
    try {
      if (window.localStorage) {
        const encodedKey = localStorage.getItem('jodotarot_api_key');
        if (encodedKey) {
          // D√©coder la cl√©
          const apiKey = atob(encodedKey);
          this.apiKey = apiKey;
          console.log('Cl√© API OpenAI charg√©e depuis le localStorage');
          return apiKey;
        }
      }
    } catch (error) {
      console.error('Erreur lors du chargement de la cl√© API:', error);
    }
    
    return null;
  }
  
  /**
   * Obtient l'interpr√©tation d'un tirage de tarot
   * @param {Array} reading - Les cartes tir√©es
   * @param {string} question - La question pos√©e
   * @param {string} persona - Le persona s√©lectionn√©
   * @param {string} model - Le mod√®le d'IA √† utiliser
   * @param {string} language - La langue (par d√©faut: 'fr')
   * @param {string} spreadType - Le type de tirage
   * @param {Function} onChunk - Callback pour le streaming (optionnel)
   * @return {Promise<string>} L'interpr√©tation du tirage
   */
  async getInterpretation(reading, question, persona, model, language = 'fr', spreadType = 'cross', onChunk = null) {
    // V√©rifier les param√®tres essentiels
    if (!reading || !reading.length || !question.trim()) {
      throw new Error('Les cartes et la question sont requises pour l\'interpr√©tation');
    }
    
    const systemPrompts = this.buildSystemPrompts(persona, language, spreadType);
    const prompt = this.buildPrompt(reading, question, language, spreadType);
    
    // Afficher uniquement le prompt final
    if (this.debugMode) {
      // Construire le prompt complet comme il sera envoy√© √† l'IA
      const fullPrompt = `${systemPrompts.join('\n\n')}\n\n${prompt}`;
      
      console.log('üì® PROMPT FINAL ENVOY√â √Ä L\'IA:');
      console.log(fullPrompt);
      
      // Afficher des informations suppl√©mentaires sur le persona si possible
      if (PERSONAS[persona]) {
        const personaInstance = new PERSONAS[persona](language);
        console.log(`üßô‚Äç‚ôÇÔ∏è Persona: ${personaInstance.getName()}`);
        console.log(`üìù Description: ${personaInstance.getDescription()}`);
        console.log(`üîÆ Sp√©cialisations: ${personaInstance.getSpecializations().join(', ')}`);
      }
    }
    
    // Si un callback de streaming est fourni, utiliser le mode streaming
    if (onChunk && typeof onChunk === 'function') {
      if (model.startsWith('openai/')) {
        // OpenAI ne supporte pas facilement le streaming dans cette impl√©mentation
        return this.getOpenAIResponse(prompt, systemPrompts, model.replace('openai/', ''));
      } else {
        return this.getOllamaStreamingResponse(prompt, systemPrompts, model, onChunk);
      }
    } else {
      // Mode standard sans streaming
      if (model.startsWith('openai/')) {
        return this.getOpenAIResponse(prompt, systemPrompts, model.replace('openai/', ''));
      } else {
        return this.getOllamaResponse(prompt, systemPrompts, model);
      }
    }
  }
  
  /**
   * Construit les prompts syst√®me pour l'IA
   * @param {string} persona - Persona choisi
   * @param {string} language - Langue
   * @param {string} spreadType - Type de tirage
   * @return {Array} Liste des prompts syst√®me
   */
  buildSystemPrompts(persona, language, spreadType) {
    // R√©cup√©rer le prompt sp√©cifique au persona via getPersonaPrompt
    // (maintenant enrichi avec les sp√©cialisations gr√¢ce √† notre am√©lioration de BasePersona)
    const personaPrompt = getPersonaPrompt(persona, language, spreadType);
    
    // R√©cup√©rer le m√©taprompt adapt√© √† la langue
    const metaPrompt = getMetaPrompt(language);
    
    // Prompts de base - le prompt du persona et le m√©taprompt
    const basePrompts = [
      // Le m√©taprompt pour d√©finir les r√®gles g√©n√©rales
      metaPrompt,
      // Le prompt principal du persona, d√©j√† enrichi
      personaPrompt
    ];
    
    return basePrompts;
  }
  
  /**
   * Construit le prompt principal pour l'interpr√©tation
   * @param {Array} reading - Tableau des cartes tir√©es
   * @param {string} question - Question pos√©e
   * @param {string} language - Code de langue 
   * @param {string} spreadType - Type de tirage
   * @return {string} Prompt format√©
   */
  buildPrompt(reading, question, language, spreadType = 'cross') {
    // S'assurer que reading est un tableau
    if (!Array.isArray(reading)) {
      console.error('Le param√®tre reading doit √™tre un tableau de cartes');
      return `Question: ${question}\n\nErreur: Format de tirage invalide`;
    }
    
    // Cr√©er une instance temporaire du tirage pour g√©n√©rer une description riche
    const spreadInstance = createSpread(spreadType, null, language);
    
    // Copier les cartes dans l'instance de tirage
    spreadInstance.cards = [...reading];
    
    // G√©n√©rer une description d√©taill√©e du tirage avec les cartes
    const spreadDescription = spreadInstance.generateReadingDescription(true);
    
    // Construction du prompt de base
    let promptBase = `${spreadDescription}`;
    
    // Enrichir le prompt avec la question et le texte d'emphase
    return enrichirPromptContextuel(question, promptBase, language);
  }
  
  /**
   * Obtient une r√©ponse d'OpenAI
   * @param {string} prompt - Le prompt principal
   * @param {Array} systemPrompts - Les prompts syst√®me
   * @param {string} model - Le mod√®le OpenAI √† utiliser
   * @return {Promise<string>} La r√©ponse g√©n√©r√©e
   */
  async getOpenAIResponse(prompt, systemPrompts, model) {
    if (!this.apiKey) {
      throw new Error('Cl√© API OpenAI non configur√©e');
    }
    
    try {
      const systemContent = systemPrompts.join('\n');
      
      // Format des messages pour OpenAI
      const messages = [
        { role: 'system', content: systemContent },
        { role: 'user', content: prompt }
      ];
      
      // Supprimer les logs d√©taill√©s
      
      const response = await fetch(API_URL_OPENAI, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: model,
          messages: messages,
          temperature: 0.7
        })
      });
      
      if (!response.ok) {
        throw new Error(`Erreur API OpenAI: ${response.status}`);
      }
      
      const data = await response.json();
      
      // Supprimer les logs d√©taill√©s
      
      return data.choices[0]?.message?.content || '';
    } catch (error) {
      console.error('Erreur lors de l\'appel √† OpenAI:', error);
      throw error;
    }
  }
  
  /**
   * Utilitaire pour les requ√™tes fetch avec timeout et r√©essai
   * @param {string} url - URL de la requ√™te
   * @param {Object} options - Options de fetch
   * @param {number} maxRetries - Nombre maximum de tentatives
   * @param {number} timeoutMs - D√©lai d'expiration en millisecondes
   * @return {Promise<Response>} - Promesse de r√©ponse
   */
  async fetchWithRetry(url, options, maxRetries = 2, timeoutMs = 15000) {
    let retries = 0;
    let lastError = null;
    
    while (retries <= maxRetries) {
      try {
        // Cr√©er un contr√¥leur d'abandon pour le timeout
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), timeoutMs);
        
        // Ajouter le signal au options
        const optionsWithSignal = {
          ...options,
          signal: controller.signal
        };
        
        try {
          // Tenter la requ√™te
          const response = await fetch(url, optionsWithSignal);
          clearTimeout(timeoutId);
          
          // Si la r√©ponse est OK, la retourner
          if (response.ok) {
            return response;
          }
          
          // Sinon, lire le texte d'erreur et le lancer
          const errorText = await response.text();
          throw new Error(`Erreur API (${response.status}): ${errorText.slice(0, 100)}`);
        } catch (fetchError) {
          clearTimeout(timeoutId);
          throw fetchError;
        }
      } catch (error) {
        lastError = error;
        retries++;
        
        const isTimeout = error.name === 'AbortError';
        const retriesLeft = maxRetries - retries + 1;
        
        if (retriesLeft > 0) {
          // D√©lai exponentiel avec un peu d'al√©atoire pour √©viter les collisions
          const delay = Math.pow(2, retries) * 1000 + Math.random() * 500;
          console.warn(`Tentative ${retries}/${maxRetries} √©chou√©e${isTimeout ? ' (timeout)' : ''}: ${error.message}. Nouvelle tentative dans ${delay/1000} secondes...`);
          
          // Attendre avant de r√©essayer
          await new Promise(resolve => setTimeout(resolve, delay));
        } else {
          console.error(`Toutes les tentatives ont √©chou√© (${retries}/${maxRetries}):`, error);
          throw error;
        }
      }
    }
    
    // Ce code ne devrait jamais √™tre atteint, mais par pr√©caution
    throw lastError || new Error("Erreur inconnue pendant les tentatives de connexion");
  }
  
  /**
   * Obtient une r√©ponse d'Ollama
   * @param {string} prompt - Le prompt principal
   * @param {Array} systemPrompts - Les prompts syst√®me
   * @param {string} model - Le mod√®le Ollama √† utiliser
   * @return {Promise<string>} La r√©ponse g√©n√©r√©e
   */
  async getOllamaResponse(prompt, systemPrompts, model) {
    // Construction du payload pour l'API chat
    const systemContent = systemPrompts.join('\n');
    const payload = {
      model: model.replace('ollama:', ''), // Supprimer le pr√©fixe "ollama:" si pr√©sent
      messages: [
        { role: 'system', content: systemContent },
        { role: 'user', content: prompt }
      ],
      stream: false
    };
    
    console.log("üîç DEBUG getOllamaResponse - Payload:", JSON.stringify(payload, null, 2));
    
    try {
      // Obtenir le format de r√©ponse pour ce mod√®le
      const modelNameWithoutPrefix = model.replace('ollama:', '');
      const modelFormat = getOllamaModelFormat(modelNameWithoutPrefix);
      const responseKey = modelFormat.responseKey || "message.content";
      
      if (this.debugMode) {
        console.log(`üîç DEBUG getOllamaResponse - Format d√©tect√© pour ${modelNameWithoutPrefix}: ${modelFormat.description || responseKey}`);
      }
      
      // Utiliser fetchWithRetry pour une meilleure r√©silience
      const response = await this.fetchWithRetry(
        API_URL_OLLAMA, 
        {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(payload)
        },
        2,  // maxRetries
        20000 // timeoutMs: 20 secondes
      );
      
      console.log("üîç DEBUG getOllamaResponse - Statut r√©ponse:", response.status);
      
      // Traiter la r√©ponse
      const data = await response.json();
      if (this.debugMode) {
        console.log("üîç DEBUG getOllamaResponse - R√©ponse re√ßue:", data);
      }
      
      // Utiliser la m√©thode centralis√©e pour extraire le contenu
      const responseContent = this.extractResponseContent(data, responseKey, modelNameWithoutPrefix);
      
      return responseContent;
    } catch (error) {
      console.error('Erreur lors de l\'appel √† Ollama apr√®s plusieurs tentatives:', error);
      throw error;
    }
  }
  
  /**
   * Obtient une r√©ponse en streaming d'Ollama
   * @param {string} prompt - Le prompt principal
   * @param {Array} systemPrompts - Les prompts syst√®me
   * @param {string} model - Le mod√®le Ollama √† utiliser
   * @param {Function} onChunk - Callback appel√© pour chaque fragment de r√©ponse
   * @return {Promise<string>} La r√©ponse compl√®te
   */
  async getOllamaStreamingResponse(prompt, systemPrompts, model, onChunk) {
    // Construction du payload pour l'API chat
    const systemContent = systemPrompts.join('\n');
    const payload = {
      model: model.replace('ollama:', ''), // Supprimer le pr√©fixe "ollama:" si pr√©sent
      messages: [
        { role: 'system', content: systemContent },
        { role: 'user', content: prompt }
      ],
      stream: true
    };
    
    console.log("üîç DEBUG getOllamaStreamingResponse - Payload:", JSON.stringify(payload, null, 2));
    
    try {
      // Obtenir le format de r√©ponse pour ce mod√®le
      const modelNameWithoutPrefix = model.replace('ollama:', '');
      const modelFormat = getOllamaModelFormat(modelNameWithoutPrefix);
      const responseKey = modelFormat.responseKey || "message.content";
      
      if (this.debugMode) {
        console.log(`üîç DEBUG getOllamaStreamingResponse - Format d√©tect√© pour ${modelNameWithoutPrefix}: ${modelFormat.description || responseKey}`);
      }
      
      // Utiliser fetchWithRetry pour une meilleure r√©silience
      const response = await this.fetchWithRetry(
        API_URL_OLLAMA, 
        {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(payload)
        },
        2,  // maxRetries
        20000  // timeoutMs: 20 secondes
      );
      
      console.log("üîç DEBUG getOllamaStreamingResponse - Statut r√©ponse:", response.status);
      
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let fullResponse = '';
      
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim() !== '');
        
        for (const line of lines) {
          try {
            const data = JSON.parse(line);
            
            // Utiliser la m√©thode centralis√©e pour extraire le contenu
            const responseContent = this.extractResponseContent(data, responseKey, modelNameWithoutPrefix);
            
            if (responseContent) {
              onChunk(responseContent);
              fullResponse += responseContent;
            }
          } catch (e) {
            console.warn('Erreur de parsing JSON:', e);
          }
        }
      }
      
      return fullResponse;
    } catch (error) {
      console.error('Erreur lors du streaming Ollama apr√®s plusieurs tentatives:', error);
      throw error;
    }
  }
  
  /**
   * Extrait le contenu de la r√©ponse selon le format du mod√®le
   * @param {Object} data - Donn√©es de r√©ponse JSON
   * @param {string} responseKey - Chemin d'acc√®s √† la r√©ponse (ex: "message.content")
   * @param {string} modelName - Nom du mod√®le pour le d√©bogage
   * @return {string} - Le contenu extrait ou cha√Æne vide si non trouv√©
   */
  extractResponseContent(data, responseKey, modelName = "inconnu") {
    if (!data) return "";
    
    // Mode debug pour diagnostiquer les r√©ponses
    if (this.debugMode) {
      console.log(`üîç DEBUG extractResponseContent - Structure de donn√©es pour ${modelName}:`, data);
    }
    
    // 1. Essayer d'extraire selon le chemin d√©fini dans le format du mod√®le
    try {
      const parts = responseKey.split('.');
      let content = data;
      
      for (const part of parts) {
        if (!content || typeof content !== 'object') {
          if (this.debugMode) {
            console.warn(`‚ö†Ô∏è Chemin ${responseKey} interrompu √† "${part}" pour le mod√®le ${modelName}`);
          }
          break;
        }
        content = content[part];
      }
      
      if (content && typeof content === 'string') {
        if (this.debugMode) {
          console.log(`‚úÖ Contenu extrait avec chemin "${responseKey}" pour le mod√®le ${modelName}`);
        }
        return content;
      }
    } catch (e) {
      console.warn(`Erreur lors de l'extraction via ${responseKey}:`, e.message);
    }
    
    // 2. Tentatives de r√©cup√©ration par ordre de priorit√©
    // Format message.content (API chat standard)
    if (data.message && data.message.content) {
      if (this.debugMode) {
        console.log(`‚ÑπÔ∏è Contenu extrait depuis message.content pour ${modelName}`);
      }
      return data.message.content;
    }
    
    // Format response (certains mod√®les Ollama)
    if (data.response) {
      if (this.debugMode) {
        console.log(`‚ÑπÔ∏è Contenu extrait depuis response pour ${modelName}`);
      }
      return data.response;
    }
    
    // Format content (certains mod√®les Ollama exp√©rimentaux)
    if (data.content) {
      if (this.debugMode) {
        console.log(`‚ÑπÔ∏è Contenu extrait depuis content pour ${modelName}`);
      }
      return data.content;
    }
    
    // Format sp√©cifique llama3.1
    if (modelName.includes('llama3.1') && data.choices && data.choices.length > 0) {
      const choice = data.choices[0];
      
      if (choice.message && choice.message.content) {
        if (this.debugMode) {
          console.log(`‚ÑπÔ∏è Contenu extrait depuis choices[0].message.content pour ${modelName}`);
        }
        return choice.message.content;
      }
      
      if (choice.content) {
        if (this.debugMode) {
          console.log(`‚ÑπÔ∏è Contenu extrait depuis choices[0].content pour ${modelName}`);
        }
        return choice.content;
      }
      
      if (choice.text) {
        if (this.debugMode) {
          console.log(`‚ÑπÔ∏è Contenu extrait depuis choices[0].text pour ${modelName}`);
        }
        return choice.text;
      }
    }
    
    // 3. Tentative d√©sesp√©r√©e: chercher un champ qui pourrait contenir du texte
    const excludedKeys = ['model', 'id', 'object', 'created', 'usage', 'system_fingerprint'];
    
    for (const key of Object.keys(data)) {
      // Ignorer les champs de m√©tadonn√©es qui ne devraient pas contenir le contenu principal
      if (excludedKeys.includes(key)) continue;
      
      const value = data[key];
      if (typeof value === 'string' && value.length > 10) {
        if (this.debugMode) {
          console.warn(`‚ö†Ô∏è Fallback: contenu extrait depuis champ "${key}" pour ${modelName}`);
        }
        return value;
      }
      
      // V√©rifier √©galement les objets imbriqu√©s de premier niveau
      if (value && typeof value === 'object') {
        for (const subKey of Object.keys(value)) {
          const subValue = value[subKey];
          if (typeof subValue === 'string' && subValue.length > 10) {
            if (this.debugMode) {
              console.warn(`‚ö†Ô∏è Fallback: contenu extrait depuis champ "${key}.${subKey}" pour ${modelName}`);
            }
            return subValue;
          }
        }
      }
    }
    
    // 4. √âchec total: retourner une cha√Æne vide ou un message d'erreur
    console.error(`‚ùå Impossible d'extraire le contenu pour le mod√®le ${modelName}. Format de r√©ponse inconnu:`, data);
    return "";
  }
  
  /**
   * Teste la connectivit√© avec Ollama
   * @returns {Promise<Object>} R√©sultat du test avec des informations d√©taill√©es
   */
  async testOllamaConnectivity() {
    try {
      // Utiliser la fonction am√©lior√©e de l'API
      const result = await testOllamaConnectivity();
      return result;
    } catch (error) {
      console.error("Erreur lors du test de connectivit√© Ollama:", error);
      return {
        status: 'error',
        success: false,
        message: `Erreur lors du test de connectivit√©: ${error.message}`,
        details: error,
        suggestions: ['warnings.unexpectedError', 'warnings.tryAgain']
      };
    }
  }
}

export default AIService; 