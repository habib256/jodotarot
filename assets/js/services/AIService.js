/**
 * Service g√©rant les interactions avec diff√©rents mod√®les d'IA
 * Centralise les appels API et la gestion des r√©ponses
 * 
 * @class AIService
 * @description Service principal pour la gestion des interactions avec les mod√®les d'IA
 * @property {StateManager} stateManager - Instance du gestionnaire d'√©tat
 * @property {string} apiKey - Cl√© API pour les services d'IA
 * @property {string} defaultModel - Mod√®le d'IA par d√©faut
 * @property {Object} baseUrl - URLs de base pour les diff√©rents services
 * @property {boolean} debugMode - Mode de d√©bogage
 * @property {AbortController} currentController - Contr√¥leur pour annuler les requ√™tes
 * @property {boolean} isGenerating - √âtat de g√©n√©ration en cours
 */
import PERSONAS, { getPersonaPrompt } from '../models/personas/index.js';
import { createSpread } from '../models/spreads/index.js';
import { API_KEY, API_URL_OPENAI, API_URL_OLLAMA, API_URL_OLLAMA_TAGS, getOllamaModelFormat, DEBUG_LEVEL, TIMEOUTS } from '../config.js';
import { getMetaPrompt, getEmphasisText, enrichirPromptContextuel } from '../prompt.js';

class AIService {
  /**
   * Cr√©e une instance du service d'IA
   * @param {StateManager} stateManager - Instance du gestionnaire d'√©tat
   * @throws {Error} Si le stateManager n'est pas fourni
   */
  constructor(stateManager) {
    if (!stateManager) {
      throw new Error('Le StateManager est requis pour initialiser AIService');
    }
    
    this.stateManager = stateManager;
    
    // D'abord, essayer de charger la cl√© API depuis le localStorage
    const savedApiKey = this.loadApiKey();
    
    // Si pas de cl√© sauvegard√©e, utiliser celle du fichier de configuration
    this.apiKey = savedApiKey || (API_KEY !== "YOUR API KEY" ? API_KEY : null);
    
    this.defaultModel = 'openai/gpt-3.5-turbo';
    this.baseUrl = {
      ollama: API_URL_OLLAMA.replace('/api/chat', ''),
      openai: API_URL_OPENAI.replace('/chat/completions', '')
    };
    
    // Activation du mode de d√©bogage selon le niveau d√©fini dans la configuration
    this.debugMode = typeof DEBUG_LEVEL !== 'undefined' && DEBUG_LEVEL > 1;
    
    // AbortController pour pouvoir annuler les requ√™tes en cours
    this.currentController = null;
    
    // Flag indiquant si une g√©n√©ration est en cours
    this.isGenerating = false;
  }
  
  /**
   * G√®re une erreur d'annulation
   * @param {Error} error - L'erreur √† g√©rer
   * @returns {boolean} True si c'√©tait une erreur d'annulation
   */
  handleAbortError(error) {
    if (error.name === 'AbortError') {
      console.log('Interpr√©tation annul√©e par l\'utilisateur');
      this.isGenerating = false;
      return true;
    }
    return false;
  }
  
  /**
   * Annule l'interpr√©tation en cours si elle existe
   * @returns {boolean} Indique si une interpr√©tation a √©t√© annul√©e
   */
  cancelCurrentInterpretation() {
    if (this.currentController && this.isGenerating) {
      console.log('Annulation de l\'interpr√©tation en cours...');
      
      // Cr√©er une erreur d'annulation
      const abortError = new Error('Generation aborted by user');
      abortError.name = 'AbortError';
      
      // Annuler la requ√™te en cours
      this.currentController.abort();
      this.currentController = null;
      this.isGenerating = false;
      
      return true;
    }
    return false;
  }
  
  /**
   * R√©cup√®re la liste des mod√®les Ollama disponibles
   * @returns {Promise<Array>} Liste des mod√®les disponibles
   */
  async getOllamaModels() {
    try {
      console.log('Tentative de r√©cup√©ration des mod√®les Ollama...');
      const response = await fetch(API_URL_OLLAMA_TAGS);
      
      if (!response.ok) {
        console.error(`Erreur HTTP lors de la r√©cup√©ration des mod√®les Ollama: ${response.status}`);
        return [];
      }
      
      const data = await response.json();
      
      if (!data.models || !Array.isArray(data.models)) {
        console.error('Format de r√©ponse Ollama invalide:', data);
        return [];
      }
      
      // Filtrer les mod√®les pour ne garder que ceux qui sont complets
      const validModels = data.models.filter(model => {
        // Exclure les mod√®les partiels ou en cours de t√©l√©chargement
        return !model.name.includes('partial') && !model.name.includes('downloading');
      });
      
      console.log('Mod√®les Ollama r√©cup√©r√©s:', validModels);
      return validModels;
    } catch (error) {
      console.error('Erreur lors de la r√©cup√©ration des mod√®les Ollama:', error);
      return [];
    }
  }
  
  /**
   * Teste la disponibilit√© d'un mod√®le sp√©cifique
   * @param {string} modelName - Nom du mod√®le √† tester
   * @return {Promise<Object>} R√©sultat du test avec statut, disponibilit√© et d√©tails
   */
  async testModelAvailability(modelName) {
    try {
      // Le mode "prompt" est un cas sp√©cial qui :
      // 1. Est toujours disponible car il n'utilise aucun mod√®le d'IA
      // 2. Sert de fallback s√©curis√© quand aucun mod√®le n'est disponible
      // 3. Permet de voir le prompt qui serait envoy√© √† l'IA sans faire d'appel
      // 4. Utile pour le d√©bogage et la personnalisation des prompts
      if (modelName === 'prompt') {
        return {
          available: true,
          status: 'success',
          modelName: 'prompt',
          message: 'Mode Prompt toujours disponible',
          details: { mode: 'prompt' },
          suggestions: []
        };
      }

      const result = {
        available: true,
        status: 'success',
        modelName,
        message: 'Mod√®le disponible',
        details: {},
        suggestions: []
      };

      // D√©tection des mod√®les Ollama
      const isOllamaModel = 
        modelName.startsWith('ollama:') || 
        modelName.includes(':') ||
        !modelName.startsWith('openai/');
      
      if (isOllamaModel) {
        // Nettoyer le nom du mod√®le - supprimer les pr√©fixes
        let ollamaModelName = modelName;
        if (ollamaModelName.startsWith('ollama:')) {
          ollamaModelName = ollamaModelName.replace('ollama:', '');
        }
        
        if (this.debugMode) {
          console.log(`Mod√®le Ollama d√©tect√©: ${ollamaModelName}`);
        }
      }
      
      return result;
    } catch (error) {
      console.error('Erreur lors du test de disponibilit√© du mod√®le:', error);
      return {
        available: false,
        status: 'error',
        modelName,
        message: error.message,
        details: {},
        suggestions: ['R√©essayer plus tard']
      };
    }
  }
  
  /**
   * D√©finit la cl√© API OpenAI et la sauvegarde dans le localStorage
   * @param {string} apiKey - La cl√© API OpenAI
   */
  setApiKey(apiKey) {
    if (!apiKey || typeof apiKey !== 'string') {
      console.error('La cl√© API doit √™tre une cha√Æne de caract√®res valide');
      return;
    }
    
    try {
      // Enregistrer la cl√© en m√©moire
      this.apiKey = apiKey;
      
      // Sauvegarder dans le localStorage pour la persistance entre les sessions
      if (window.localStorage) {
        // Masquer l√©g√®rement la cl√© avant le stockage (n'est pas un cryptage s√©curis√©)
        const encodedKey = btoa(apiKey);
        localStorage.setItem('jodotarot_api_key', encodedKey);
        console.log('Cl√© API OpenAI sauvegard√©e dans le localStorage');
      }
    } catch (error) {
      console.error('Erreur lors de la sauvegarde de la cl√© API:', error);
    }
  }
  
  /**
   * Charge la cl√© API depuis le localStorage
   * @returns {string|null} La cl√© API ou null si non trouv√©e
   */
  loadApiKey() {
    try {
      if (window.localStorage) {
        const encodedKey = localStorage.getItem('jodotarot_api_key');
        if (encodedKey) {
          // D√©coder la cl√©
          const apiKey = atob(encodedKey);
          this.apiKey = apiKey;
          console.log('Cl√© API OpenAI charg√©e depuis le localStorage');
          return apiKey;
        }
      }
    } catch (error) {
      console.error('Erreur lors du chargement de la cl√© API:', error);
    }
    
    return null;
  }
  
  /**
   * G√®re les erreurs d'API de mani√®re uniforme
   * @param {Error} error - L'erreur √† g√©rer
   * @param {string} service - Le nom du service ('openai' ou 'ollama')
   * @returns {Error} L'erreur format√©e
   */
  handleApiError(error, service) {
    console.error(`Erreur lors de l'appel √† ${service}:`, error);
    
    let errorMessage = `Erreur lors de la communication avec ${service}`;
    
    if (error.message.includes('timeout')) {
      errorMessage = `Le temps de r√©ponse de ${service} a d√©pass√© la limite`;
    } else if (error.message.includes('connect')) {
      errorMessage = `Impossible de se connecter √† ${service}`;
    } else if (error.message.includes('401') || error.message.includes('403')) {
      errorMessage = `Erreur d'authentification avec ${service}`;
    } else if (error.message.includes('429')) {
      errorMessage = `Limite de requ√™tes atteinte pour ${service}`;
    }
    
    return new Error(errorMessage);
  }
  
  /**
   * G√®re les logs de d√©bogage de mani√®re uniforme
   * @param {string} message - Le message √† logger
   * @param {Object} [data] - Les donn√©es √† logger
   * @param {string} [level='info'] - Le niveau de log ('info', 'warn', 'error')
   */
  debugLog(message, data = null, level = 'info') {
    if (!this.debugMode) return;
    
    const emoji = {
      info: 'üîç',
      warn: '‚ö†Ô∏è',
      error: '‚ùå'
    }[level] || 'üîç';
    
    console[level](`${emoji} ${message}`);
    if (data) {
      console[level](data);
    }
  }
  
  /**
   * Obtient l'interpr√©tation d'un tirage de tarot
   * @param {Array} reading - Les cartes tir√©es
   * @param {string} question - La question pos√©e
   * @param {string} persona - Le persona s√©lectionn√©
   * @param {string} model - Le mod√®le d'IA √† utiliser
   * @param {string} [language='fr'] - La langue de l'interpr√©tation
   * @param {string} [spreadType='cross'] - Le type de tirage
   * @param {Function} [onChunk] - Callback pour le streaming de la r√©ponse
   * @returns {Promise<string>} L'interpr√©tation du tirage
   * @throws {Error} Si les param√®tres sont invalides ou si une erreur survient
   */
  async getInterpretation(reading, question, persona, model, language = 'fr', spreadType = 'cross', onChunk = null) {
    try {
      // Validation des param√®tres obligatoires
      if (!reading || !Array.isArray(reading) || reading.length === 0) {
        throw new Error('Le tirage doit contenir au moins une carte');
      }
      
      if (!question || typeof question !== 'string' || question.trim().length === 0) {
        throw new Error('La question est requise pour l\'interpr√©tation');
      }
      
      if (!persona || typeof persona !== 'string') {
        throw new Error('Le persona est requis pour l\'interpr√©tation');
      }
      
      if (!model || typeof model !== 'string') {
        throw new Error('Le mod√®le d\'IA est requis pour l\'interpr√©tation');
      }
      
      // Validation de la langue
      if (!language || typeof language !== 'string') {
        console.warn('Langue invalide, utilisation du fran√ßais par d√©faut');
        language = 'fr';
      }
      
      // Annuler toute interpr√©tation en cours
      this.cancelCurrentInterpretation();
      
      // Cr√©er un nouvel AbortController
      this.currentController = new AbortController();
      this.isGenerating = true;
      
      // V√©rifier les param√®tres essentiels
      if (!reading || !reading.length || !question.trim()) {
        throw new Error('Les cartes et la question sont requises pour l\'interpr√©tation');
      }
      
      const systemPrompts = await this.buildSystemPrompts(persona, language, spreadType);
      const prompt = this.buildPrompt(reading, question, language, spreadType);
      
      // Mode sp√©cial "prompt" (Sans IA)
      // Ce mode est une fonctionnalit√© de s√©curit√© et de d√©bogage qui :
      // 1. Est toujours disponible m√™me sans connexion √† un service d'IA
      // 2. Affiche les prompts syst√®me et utilisateur qui seraient envoy√©s √† l'IA
      // 3. Permet de v√©rifier et ajuster les prompts sans faire d'appels API
      // 4. Sert de solution de repli si aucun mod√®le d'IA n'est disponible
      // 5. Aide √† la compr√©hension du syst√®me de prompts pour les d√©veloppeurs
      if (model === 'prompt') {
        console.log('üìù Mode Prompt activ√© : affichage du prompt sans appel √† l\'IA');
        
        // Concat√©ner simplement les prompts syst√®me et utilisateur
        const fullPrompt = `${systemPrompts.join('\n\n')}\n\n${prompt}`;
        
        // Affichage minimal sans formatage particulier
        const response = `<div class="prompt-display">${fullPrompt}</div>`;
        
        this.isGenerating = false;
        return response;
      }
      
      // Continuer avec le reste de la logique pour les autres mod√®les...
      
      // Afficher uniquement le prompt final
      if (this.debugMode) {
        // Construire le prompt complet comme il sera envoy√© √† l'IA
        const fullPrompt = `${systemPrompts.join('\n\n')}\n\n${prompt}`;
        
        console.log('üì® PROMPT FINAL ENVOY√â √Ä L\'IA:');
        console.log(fullPrompt);
        
        // Afficher des informations suppl√©mentaires sur le persona si possible
        if (PERSONAS[persona]) {
          const personaInstance = new PERSONAS[persona](language);
          console.log(`üßô‚Äç‚ôÇÔ∏è Persona: ${personaInstance.getName()}`);
          console.log(`üìù Description: ${personaInstance.getDescription()}`);
          console.log(`üîÆ Sp√©cialisations: ${personaInstance.getSpecializations().join(', ')}`);
        }
      }
      
      // Obtenir la r√©ponse selon le type de mod√®le (OpenAI ou Ollama)
      let response;
      
      if (model.startsWith('openai/')) {
        response = await this.getOpenAIResponse(prompt, systemPrompts, model.replace('openai/', ''));
      } else {
        // Si un callback de streaming est fourni, utiliser le streaming pour Ollama
        if (onChunk && typeof onChunk === 'function') {
          try {
            response = await this.getOllamaStreamingResponse(prompt, systemPrompts, model, onChunk, this.currentController.signal);
          } catch (error) {
            if (this.handleAbortError(error)) {
              return "";
            }
            throw error;
          }
        } else {
          response = await this.getOllamaResponse(prompt, systemPrompts, model);
        }
      }
      
      // Marquer la g√©n√©ration comme termin√©e
      this.isGenerating = false;
      return response;
    } catch (error) {
      this.isGenerating = false;
      console.error("Erreur lors de l'obtention de l'interpr√©tation:", error);
      
      if (this.handleAbortError(error)) {
        return "";
      }
      
      // Gestion plus d√©taill√©e des erreurs
      let errorMessage = "Une erreur est survenue lors de l'interpr√©tation.";
      
      if (!this.apiKey && model.startsWith('openai/')) {
        errorMessage = "La cl√© API OpenAI n'est pas configur√©e.";
      } else if (error.message.includes('timeout')) {
        errorMessage = "Le temps de r√©ponse a d√©pass√© la limite.";
      } else if (error.message.includes('connect')) {
        errorMessage = "Impossible de se connecter au service d'IA.";
      }
      
      throw new Error(errorMessage);
    }
  }
  
  /**
   * Construit les prompts syst√®me pour le mod√®le d'IA
   * @param {string} persona - Persona choisi
   * @param {string} language - Langue
   * @param {string} spreadType - Type de tirage
   * @return {Promise<Array>} Liste des prompts syst√®me
   */
  async buildSystemPrompts(persona, language, spreadType) {
    this.debugLog(`Chargement du prompt pour le persona: ${persona}, langue: ${language}, tirage: ${spreadType}`);
    
    try {
      const personaPrompt = await getPersonaPrompt(persona, language, spreadType);
      this.debugLog('Contenu du prompt persona:', personaPrompt);
      
      const metaPrompt = getMetaPrompt(language);
      
      const basePrompts = [
        metaPrompt,
        personaPrompt
      ];
      
      return basePrompts;
    } catch (error) {
      this.debugLog("Erreur lors du chargement des prompts syst√®me:", error, 'error');
      return [getMetaPrompt(language)];
    }
  }
  
  /**
   * Construit le prompt principal pour l'interpr√©tation
   * @param {Array} reading - Tableau des cartes tir√©es
   * @param {string} question - Question pos√©e
   * @param {string} language - Code de langue 
   * @param {string} spreadType - Type de tirage
   * @return {string} Prompt format√©
   */
  buildPrompt(reading, question, language, spreadType = 'cross') {
    // S'assurer que reading est un tableau
    if (!Array.isArray(reading)) {
      console.error('Le param√®tre reading doit √™tre un tableau de cartes');
      return `Question: ${question}\n\nErreur: Format de tirage invalide`;
    }
    
    // Cr√©er une instance temporaire du tirage pour g√©n√©rer une description riche
    const spreadInstance = createSpread(spreadType, null, language);
    
    // Copier les cartes dans l'instance de tirage
    spreadInstance.cards = [...reading];
    
    // G√©n√©rer une description d√©taill√©e du tirage avec les cartes
    const spreadDescription = spreadInstance.generateReadingDescription(true);
    
    // Construction du prompt de base avec toutes les informations sur les cartes
    let promptBase = `${spreadDescription}\n\n`;
    
    // Enrichir le prompt avec la question et le texte d'emphase
    return enrichirPromptContextuel(question, promptBase, language);
  }
  
  /**
   * Obtient une r√©ponse d'OpenAI
   * @param {string} prompt - Le prompt principal
   * @param {Array} systemPrompts - Les prompts syst√®me
   * @param {string} model - Le mod√®le OpenAI √† utiliser
   * @return {Promise<string>} La r√©ponse g√©n√©r√©e
   */
  async getOpenAIResponse(prompt, systemPrompts, model) {
    if (!this.apiKey) {
      throw new Error('Cl√© API OpenAI non configur√©e');
    }
    
    try {
      const systemContent = systemPrompts.join('\n');
      
      const messages = [
        { role: 'system', content: systemContent },
        { role: 'user', content: prompt }
      ];
      
      const response = await fetch(API_URL_OPENAI, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: model,
          messages: messages,
          temperature: 0.7
        })
      });
      
      if (!response.ok) {
        throw new Error(`Erreur API OpenAI: ${response.status}`);
      }
      
      const data = await response.json();
      return data.choices[0]?.message?.content || '';
    } catch (error) {
      throw this.handleApiError(error, 'OpenAI');
    }
  }
  
  /**
   * Utilitaire pour les requ√™tes fetch avec timeout et r√©essai
   * @param {string} url - URL de la requ√™te
   * @param {Object} options - Options de fetch
   * @param {number} maxRetries - Nombre maximum de tentatives
   * @param {number} timeoutMs - D√©lai d'expiration en millisecondes
   * @return {Promise<Response>} - Promesse de r√©ponse
   */
  async fetchWithRetry(url, options, maxRetries = TIMEOUTS.MAX_RETRIES, timeoutMs = TIMEOUTS.OLLAMA_CONNECT) {
    let retries = 0;
    let lastError = null;
    
    while (retries <= maxRetries) {
      try {
        // Cr√©er un contr√¥leur d'abandon pour le timeout
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), timeoutMs);
        
        // Ajouter le signal au options
        const optionsWithSignal = {
          ...options,
          signal: controller.signal
        };
        
        try {
          // Tenter la requ√™te
          const response = await fetch(url, optionsWithSignal);
          clearTimeout(timeoutId);
          
          // Si la r√©ponse est OK, la retourner
          if (response.ok) {
            return response;
          }
          
          // Sinon, lire le texte d'erreur et le lancer
          const errorText = await response.text();
          throw new Error(`Erreur API (${response.status}): ${errorText.slice(0, 100)}`);
        } catch (fetchError) {
          clearTimeout(timeoutId);
          throw fetchError;
        }
      } catch (error) {
        lastError = error;
        retries++;
        
        const isTimeout = error.name === 'AbortError';
        const retriesLeft = maxRetries - retries + 1;
        
        if (retriesLeft > 0) {
          // D√©lai exponentiel avec un peu d'al√©atoire pour √©viter les collisions
          const delay = Math.pow(2, retries) * 1000 + Math.random() * 500;
          console.warn(`Tentative ${retries}/${maxRetries} √©chou√©e${isTimeout ? ' (timeout)' : ''}: ${error.message}. Nouvelle tentative dans ${delay/1000} secondes...`);
          
          // Attendre avant de r√©essayer
          await new Promise(resolve => setTimeout(resolve, delay));
        } else {
          console.error(`Toutes les tentatives ont √©chou√© (${retries}/${maxRetries}):`, error);
          throw error;
        }
      }
    }
    
    // Ce code ne devrait jamais √™tre atteint, mais par pr√©caution
    throw lastError || new Error("Erreur inconnue pendant les tentatives de connexion");
  }
  
  /**
   * Obtient une r√©ponse d'Ollama
   * @param {string} prompt - Le prompt principal
   * @param {Array} systemPrompts - Les prompts syst√®me
   * @param {string} model - Le mod√®le Ollama √† utiliser
   * @return {Promise<string>} La r√©ponse g√©n√©r√©e
   */
  async getOllamaResponse(prompt, systemPrompts, model) {
    try {
      const systemContent = systemPrompts.join('\n');
      const payload = {
        model: model.replace('ollama:', ''),
        messages: [
          { role: 'system', content: systemContent },
          { role: 'user', content: prompt }
        ],
        stream: false
      };
      
      this.debugLog("Payload Ollama:", payload);
      
      const response = await this.fetchWithRetry(
        API_URL_OLLAMA, 
        {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(payload)
        },
        2,
        20000
      );
      
      this.debugLog("Statut r√©ponse Ollama:", response.status);
      
      const data = await response.json();
      this.debugLog("R√©ponse Ollama re√ßue:", data);
      
      const modelNameWithoutPrefix = model.replace('ollama:', '');
      const modelFormat = getOllamaModelFormat(modelNameWithoutPrefix);
      const responseKey = modelFormat.responseKey || "message.content";
      
      this.debugLog(`Format d√©tect√© pour ${modelNameWithoutPrefix}: ${modelFormat.description || responseKey}`);
      
      return this.extractResponseContent(data, responseKey, modelNameWithoutPrefix);
    } catch (error) {
      throw this.handleApiError(error, 'Ollama');
    }
  }
  
  /**
   * Obtient une r√©ponse en streaming d'Ollama
   * @param {string} prompt - Le prompt utilisateur
   * @param {Array} systemPrompts - Les prompts syst√®me
   * @param {string} modelName - Nom du mod√®le Ollama
   * @param {Function} onChunk - Callback pour chaque morceau de r√©ponse
   * @param {AbortSignal} signal - Signal pour annuler la requ√™te
   * @return {Promise<string>} La r√©ponse compl√®te
   */
  async getOllamaStreamingResponse(prompt, systemPrompts, modelName, onChunk, signal) {
    // Nettoyer le nom du mod√®le (enlever le pr√©fixe ollama: si pr√©sent)
    const cleanModelName = modelName.replace('ollama:', '');
    
    // Construire le prompt complet en combinant les prompts syst√®me et le prompt utilisateur
    const fullPrompt = [
      ...systemPrompts,
      prompt
    ].join('\n\n');
    
    // Corps de la requ√™te pour Ollama
    const body = {
      model: cleanModelName,
      prompt: fullPrompt,
      stream: true,
      options: {
        temperature: 0.7,
        num_predict: 1000
      }
    };
    
    try {
      if (this.debugMode) {
        console.log(`üîÑ Envoi de la requ√™te en streaming √† Ollama (${cleanModelName})`);
        console.log('Prompt complet:', fullPrompt);
        console.log('Corps de la requ√™te:', body);
      }
      
      // V√©rifier que le callback est bien une fonction
      if (typeof onChunk !== 'function') {
        throw new Error("Le callback onChunk doit √™tre une fonction");
      }
      
      // Options de la requ√™te
      const options = {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(body),
        signal // Utiliser le signal d'annulation
      };
      
      // Effectuer la requ√™te
      const response = await fetch(API_URL_OLLAMA, options);
      
      if (!response.ok) {
        const errorData = await response.text();
        throw new Error(`Erreur Ollama [${response.status}]: ${errorData}`);
      }
      
      // Initialiser le lecteur de flux
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let completeResponse = '';
      
      // Lire le flux de r√©ponse
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        // Convertir les donn√©es binaires en texte
        const chunk = decoder.decode(value, { stream: true });
        
        // Traiter les lignes JSON individuelles
        const lines = chunk.split('\n').filter(line => line.trim() !== '');
        
        for (const line of lines) {
          try {
            // Analyser chaque ligne comme un objet JSON
            const data = JSON.parse(line);
            
            // Extraire le contenu de la r√©ponse
            if (data.response) {
              completeResponse += data.response;
              onChunk(data.response);
            }
          } catch (error) {
            console.error("Erreur lors de l'analyse du chunk JSON:", error);
            // Ne pas interrompre le traitement en cas d'erreur sur un chunk
          }
        }
      }
      
      return completeResponse;
    } catch (error) {
      // Propager l'erreur d'annulation
      if (error.name === 'AbortError') {
        throw error;
      }
      
      console.error("Erreur lors de l'obtention de la r√©ponse streaming Ollama:", error);
      throw new Error(`Erreur lors de la communication avec Ollama: ${error.message}`);
    }
  }
  
  /**
   * Extrait le contenu de la r√©ponse selon le format du mod√®le
   * @param {Object} data - Donn√©es de r√©ponse JSON
   * @param {string} responseKey - Chemin d'acc√®s √† la r√©ponse (ex: "message.content")
   * @param {string} modelName - Nom du mod√®le pour le d√©bogage
   * @return {string} - Le contenu extrait ou cha√Æne vide si non trouv√©
   */
  extractResponseContent(data, responseKey, modelName = "inconnu") {
    if (!data) return "";
    
    // Mode debug pour diagnostiquer les r√©ponses
    if (this.debugMode) {
      console.log(`R√©ponse brute du mod√®le ${modelName}:`, data);
    }
    
    // ... existing code ...
  }
}

export default AIService;