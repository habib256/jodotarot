/**
 * Service g√©rant les interactions avec diff√©rents mod√®les d'IA
 * Centralise les appels API et la gestion des r√©ponses
 */
import PERSONAS, { getPersonaPrompt } from '../models/personas/index.js';
import { createSpread } from '../models/spreads/index.js';
import { API_KEY, API_URL_OPENAI, API_URL_OLLAMA, API_URL_OLLAMA_TAGS, getOllamaModelFormat } from '../config.js';
import { getMetaPrompt, getEmphasisText, enrichirPromptContextuel } from '../prompt.js';

class AIService {
  constructor() {
    this.apiKey = API_KEY !== "YOUR API KEY" ? API_KEY : null;
    this.defaultModel = 'openai/gpt-3.5-turbo';
    this.baseUrl = {
      ollama: API_URL_OLLAMA.replace('/api/chat', ''),
      openai: API_URL_OPENAI.replace('/v1/chat/completions', '/v1')
    };
    this.debugMode = true; // Activer le mode debug pour afficher les prompts
  }
  
  /**
   * R√©cup√®re la liste des mod√®les Ollama disponibles
   * @return {Promise<Array>} Liste des mod√®les disponibles
   */
  async getOllamaModels() {
    try {
      const response = await fetch(API_URL_OLLAMA_TAGS);
      
      if (!response.ok) {
        throw new Error(`Erreur HTTP: ${response.status}`);
      }
      
      const data = await response.json();
      return data.models || [];
    } catch (error) {
      console.error('Erreur lors de la r√©cup√©ration des mod√®les Ollama:', error);
      return [];
    }
  }
  
  /**
   * Trouve un mod√®le Llama3 parmi les mod√®les Ollama disponibles
   * @param {Array} models - Liste des mod√®les disponibles
   * @return {Object|null} Le mod√®le Llama3 trouv√© ou null
   */
  findLlama3Model(models) {
    if (!models || !Array.isArray(models) || models.length === 0) {
      return null;
    }
    
    // Patterns pour identifier un mod√®le Llama3
    const llama3Patterns = [
      /llama3/i,
      /llama-3/i,
      /llama_3/i
    ];
    
    // Chercher d'abord un mod√®le qui contient explicitement "llama3"
    for (const pattern of llama3Patterns) {
      const foundModel = models.find(model => pattern.test(model.name));
      if (foundModel) {
        console.log(`Mod√®le Llama3 trouv√©: ${foundModel.name}`);
        return foundModel;
      }
    }
    
    // Si aucun mod√®le Llama3 n'est trouv√©, retourner null
    console.log('Aucun mod√®le Llama3 trouv√© parmi les mod√®les Ollama disponibles');
    return null;
  }
  
  /**
   * Teste la connectivit√© avec un mod√®le sp√©cifique
   * @param {string} modelName - Nom du mod√®le √† tester
   * @return {Promise<boolean>} Disponibilit√© du mod√®le
   */
  async testConnectivity(modelName) {
    try {
      if (modelName.startsWith('openai/')) {
        // Test de connectivit√© pour OpenAI
        if (!this.apiKey) {
          return false;
        }
        
        const response = await fetch(`${this.baseUrl.openai}/models`, {
          headers: {
            'Authorization': `Bearer ${this.apiKey}`,
            'Content-Type': 'application/json'
          }
        });
        
        return response.ok;
      } else {
        // Test de connectivit√© pour Ollama
        const response = await fetch(API_URL_OLLAMA_TAGS);
        if (!response.ok) {
          return false;
        }
        
        // V√©rifier si le mod√®le est disponible
        const data = await response.json();
        return data.models?.some(model => model.name === modelName) || false;
      }
    } catch (error) {
      console.error(`Erreur lors du test de connectivit√© pour ${modelName}:`, error);
      return false;
    }
  }
  
  /**
   * Configure la cl√© API pour OpenAI
   * @param {string} apiKey - Cl√© API OpenAI
   */
  setApiKey(apiKey) {
    this.apiKey = apiKey;
  }
  
  /**
   * Obtient l'interpr√©tation d'un tirage de tarot
   * @param {Array} reading - Les cartes tir√©es
   * @param {string} question - La question pos√©e
   * @param {string} persona - Le persona s√©lectionn√©
   * @param {string} model - Le mod√®le d'IA √† utiliser
   * @param {string} language - La langue (par d√©faut: 'fr')
   * @param {string} spreadType - Le type de tirage
   * @param {Function} onChunk - Callback pour le streaming (optionnel)
   * @return {Promise<string>} L'interpr√©tation du tirage
   */
  async getInterpretation(reading, question, persona, model, language = 'fr', spreadType = 'cross', onChunk = null) {
    // V√©rifier les param√®tres essentiels
    if (!reading || !reading.length || !question.trim()) {
      throw new Error('Les cartes et la question sont requises pour l\'interpr√©tation');
    }
    
    const systemPrompts = this.buildSystemPrompts(persona, language, spreadType);
    const prompt = this.buildPrompt(reading, question, language, spreadType);
    
    // Afficher uniquement le prompt final
    if (this.debugMode) {
      // Construire le prompt complet comme il sera envoy√© √† l'IA
      const fullPrompt = `${systemPrompts.join('\n\n')}\n\n${prompt}`;
      
      console.log('üì® PROMPT FINAL ENVOY√â √Ä L\'IA:');
      console.log(fullPrompt);
      
      // Afficher des informations suppl√©mentaires sur le persona si possible
      if (PERSONAS[persona]) {
        const personaInstance = new PERSONAS[persona](language);
        console.log(`üßô‚Äç‚ôÇÔ∏è Persona: ${personaInstance.getName()}`);
        console.log(`üìù Description: ${personaInstance.getDescription()}`);
        console.log(`üîÆ Sp√©cialisations: ${personaInstance.getSpecializations().join(', ')}`);
      }
    }
    
    // Si un callback de streaming est fourni, utiliser le mode streaming
    if (onChunk && typeof onChunk === 'function') {
      if (model.startsWith('openai/')) {
        // OpenAI ne supporte pas facilement le streaming dans cette impl√©mentation
        return this.getOpenAIResponse(prompt, systemPrompts, model.replace('openai/', ''));
      } else {
        return this.getOllamaStreamingResponse(prompt, systemPrompts, model, onChunk);
      }
    } else {
      // Mode standard sans streaming
      if (model.startsWith('openai/')) {
        return this.getOpenAIResponse(prompt, systemPrompts, model.replace('openai/', ''));
      } else {
        return this.getOllamaResponse(prompt, systemPrompts, model);
      }
    }
  }
  
  /**
   * Construit les prompts syst√®me pour l'IA
   * @param {string} persona - Persona choisi
   * @param {string} language - Langue
   * @param {string} spreadType - Type de tirage
   * @return {Array} Liste des prompts syst√®me
   */
  buildSystemPrompts(persona, language, spreadType) {
    // R√©cup√©rer le prompt sp√©cifique au persona via getPersonaPrompt
    // (maintenant enrichi avec les sp√©cialisations gr√¢ce √† notre am√©lioration de BasePersona)
    const personaPrompt = getPersonaPrompt(persona, language, spreadType);
    
    // R√©cup√©rer le m√©taprompt adapt√© √† la langue
    const metaPrompt = getMetaPrompt(language);
    
    // Prompts de base - le prompt du persona et le m√©taprompt
    const basePrompts = [
      // Le m√©taprompt pour d√©finir les r√®gles g√©n√©rales
      metaPrompt,
      // Le prompt principal du persona, d√©j√† enrichi
      personaPrompt
    ];
    
    return basePrompts;
  }
  
  /**
   * Construit le prompt principal pour l'interpr√©tation
   * @param {Array} reading - Les cartes tir√©es
   * @param {string} question - La question pos√©e
   * @param {string} language - La langue
   * @param {string} spreadType - Le type de tirage (cross, horseshoe, love, celticCross)
   * @return {string} Le prompt format√©
   */
  buildPrompt(reading, question, language, spreadType = 'cross') {
    // S'assurer que reading est un tableau
    if (!Array.isArray(reading)) {
      console.error('Le param√®tre reading doit √™tre un tableau de cartes');
      return `Question: ${question}\n\nErreur: Format de tirage invalide`;
    }
    
    // Cr√©er une instance temporaire du tirage pour g√©n√©rer une description riche
    const spreadInstance = createSpread(spreadType, null, language);
    
    // Copier les cartes dans l'instance de tirage
    spreadInstance.cards = [...reading];
    
    // G√©n√©rer une description d√©taill√©e du tirage avec les cartes
    const spreadDescription = spreadInstance.generateReadingDescription(true);
    
    // Construction du prompt de base
    let promptBase = `${spreadDescription}`;
    
    // Enrichir le prompt avec la question et le texte d'emphase
    return enrichirPromptContextuel(question, promptBase, language);
  }
  
  /**
   * Obtient une r√©ponse d'OpenAI
   * @param {string} prompt - Le prompt principal
   * @param {Array} systemPrompts - Les prompts syst√®me
   * @param {string} model - Le mod√®le OpenAI √† utiliser
   * @return {Promise<string>} La r√©ponse g√©n√©r√©e
   */
  async getOpenAIResponse(prompt, systemPrompts, model) {
    if (!this.apiKey) {
      throw new Error('Cl√© API OpenAI non configur√©e');
    }
    
    try {
      const systemContent = systemPrompts.join('\n');
      
      // Format des messages pour OpenAI
      const messages = [
        { role: 'system', content: systemContent },
        { role: 'user', content: prompt }
      ];
      
      // Supprimer les logs d√©taill√©s
      
      const response = await fetch(API_URL_OPENAI, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: model,
          messages: messages,
          temperature: 0.7
        })
      });
      
      if (!response.ok) {
        throw new Error(`Erreur API OpenAI: ${response.status}`);
      }
      
      const data = await response.json();
      
      // Supprimer les logs d√©taill√©s
      
      return data.choices[0]?.message?.content || '';
    } catch (error) {
      console.error('Erreur lors de l\'appel √† OpenAI:', error);
      throw error;
    }
  }
  
  /**
   * Obtient une r√©ponse d'Ollama
   * @param {string} prompt - Le prompt principal
   * @param {Array} systemPrompts - Les prompts syst√®me
   * @param {string} model - Le mod√®le Ollama √† utiliser
   * @return {Promise<string>} La r√©ponse g√©n√©r√©e
   */
  async getOllamaResponse(prompt, systemPrompts, model) {
    try {
      const systemContent = systemPrompts.join('\n');
      
      // Journalisation de d√©bogage pour l'URL et le mod√®le
      const generateUrl = API_URL_OLLAMA.replace('/api/chat', '/api/generate');
      console.log("üîç DEBUG getOllamaResponse - URL Ollama:", generateUrl);
      console.log("üîç DEBUG getOllamaResponse - Mod√®le utilis√©:", model);
      
      // Construction d'un prompt combin√© pour /api/generate
      const combinedPrompt = `${systemContent}\n\n${prompt}`;
      
      // Construction du payload pour l'API generate
      const payload = {
        model: model.replace('ollama/', ''), // Supprimer le pr√©fixe "ollama/" si pr√©sent
        prompt: combinedPrompt,
        stream: false
      };
      
      console.log("üîç DEBUG getOllamaResponse - Payload:", JSON.stringify(payload, null, 2));
      
      const response = await fetch(generateUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(payload)
      });
      
      console.log("üîç DEBUG getOllamaResponse - Statut r√©ponse:", response.status);
      
      if (!response.ok) {
        const errorText = await response.text();
        console.error("üîç DEBUG getOllamaResponse - Erreur d√©taill√©e:", errorText);
        throw new Error(`Erreur API Ollama: ${response.status}`);
      }
      
      const data = await response.json();
      
      console.log("üîç DEBUG getOllamaResponse - R√©ponse re√ßue:", data);
      
      // La r√©ponse de /api/generate est dans data.response et non data.message.content
      return data.response || '';
    } catch (error) {
      console.error('Erreur lors de l\'appel √† Ollama:', error);
      throw error;
    }
  }
  
  /**
   * Obtient une r√©ponse en streaming d'Ollama
   * @param {string} prompt - Le prompt principal
   * @param {Array} systemPrompts - Les prompts syst√®me
   * @param {string} model - Le mod√®le Ollama √† utiliser
   * @param {Function} onChunk - Callback appel√© pour chaque fragment de r√©ponse
   * @return {Promise<string>} La r√©ponse compl√®te
   */
  async getOllamaStreamingResponse(prompt, systemPrompts, model, onChunk) {
    try {
      const systemContent = systemPrompts.join('\n');
      
      // Journalisation de d√©bogage pour l'URL et le mod√®le
      const generateUrl = API_URL_OLLAMA.replace('/api/chat', '/api/generate');
      console.log("üîç DEBUG getOllamaStreamingResponse - URL Ollama:", generateUrl);
      console.log("üîç DEBUG getOllamaStreamingResponse - Mod√®le utilis√©:", model);
      
      // Construction d'un prompt combin√© pour /api/generate
      const combinedPrompt = `${systemContent}\n\n${prompt}`;
      
      // Construction du payload pour l'API generate
      const payload = {
        model: model.replace('ollama/', ''), // Supprimer le pr√©fixe "ollama/" si pr√©sent
        prompt: combinedPrompt,
        stream: true
      };
      
      console.log("üîç DEBUG getOllamaStreamingResponse - Payload:", JSON.stringify(payload, null, 2));
      
      const response = await fetch(generateUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(payload)
      });
      
      console.log("üîç DEBUG getOllamaStreamingResponse - Statut r√©ponse:", response.status);
      
      if (!response.ok) {
        const errorText = await response.text();
        console.error("üîç DEBUG getOllamaStreamingResponse - Erreur d√©taill√©e:", errorText);
        throw new Error(`Erreur API Ollama: ${response.status}`);
      }
      
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let fullResponse = '';
      
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim() !== '');
        
        for (const line of lines) {
          try {
            const data = JSON.parse(line);
            
            // Pour l'API generate, la r√©ponse est dans data.response
            let responseContent = data.response || '';
            
            if (responseContent) {
              onChunk(responseContent);
              fullResponse += responseContent;
            }
          } catch (e) {
            console.warn('Erreur de parsing JSON:', e);
          }
        }
      }
      
      return fullResponse;
    } catch (error) {
      console.error('Erreur lors du streaming Ollama:', error);
      throw error;
    }
  }
}

export default AIService; 