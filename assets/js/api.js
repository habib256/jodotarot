/**
 * Module de gestion des appels API aux mod√®les d'IA
 */

import { API_KEY, API_URL_OPENAI, API_URL_OLLAMA, API_URL_OLLAMA_TAGS, getOllamaModelFormat } from './config.js';
import { getMetaPrompt, enrichirPromptContextuel } from './metaprompt.js';
import { genererPromptTirage } from './tarot.js';
import PERSONAS, { getPersonaPrompt } from './personas/index.js';
import { getTranslation } from './translations.js';

// Configuration du niveau de d√©boggage
// 0 = Erreurs seulement, 1 = Infos importantes, 2 = D√©tails, 3 = Verbeux
const DEBUG_LEVEL = 2; 

// Syst√®me simple de cache pour les r√©ponses
const responseCache = new Map();

// Cr√©er un fichier de configuration centrale
const SETTINGS = {
  DEFAULT_PERSONA: "tarologue",
  DEFAULT_LANGUAGE: "fr",
  MAX_TOKENS: 500,
  // etc.
};

/**
 * Fonction pour obtenir une r√©ponse de l'API OpenAI avec GPT-4o
 * @param {string} message - La question pos√©e par l'utilisateur
 * @param {Array} systemPrompts - Tableau de prompts syst√®me additionnels
 * @param {string} modele - Mod√®le d'IA √† utiliser
 * @param {string} persona - Persona s√©lectionn√© pour l'interpr√©tation
 * @param {Array} tirage - Tableau des cartes tir√©es
 * @param {string} langue - Langue s√©lectionn√©e (fr, en, es, de, it)
 * @param {string} spreadType - Type de tirage (cross ou horseshoe)
 * @returns {Promise<string>} - R√©ponse de l'API
 */
async function obtenirReponseGPT4O(message, systemPrompts = [], modele = 'openai/gpt-3.5-turbo', persona = 'tarologue', tirage = [], langue = 'fr', spreadType = 'cross') {
  // V√©rifier quel type d'API nous utilisons (OpenAI ou Ollama)
  const isOllama = !modele.startsWith('openai/');
  
  console.log("üîç DEBUG - D√©marrage appel API:", { 
    modele, 
    isOllama, 
    persona,
    nombreCartes: tirage.length,
    langue,
    spreadType
  });
  
  // R√©cup√©rer l'√©l√©ment pour afficher les erreurs et l'interpr√©tation
  const interpretationsDiv = document.getElementById('interpretations');
  
  try {
    // Pr√©parer les donn√©es pour l'API
    const modelName = isOllama ? modele.split('/')[1] : modele.split('/')[1];
    const API_URL = isOllama ? API_URL_OLLAMA : API_URL_OPENAI;
    
    console.log("üîç DEBUG - Configuration API:", { 
      modelName, 
      API_URL,
      headers: isOllama ? "Standard" : "Avec API_KEY" 
    });
    
    // Tester la connectivit√© d'Ollama si applicable
    if (isOllama) {
      interpretationsDiv.innerHTML = `<p class="loading">${getTranslation('interpretation.testingConnection', langue) || 'Test de connexion √† Ollama...'}</p>`;
      const connectivityTest = await testOllamaConnectivity(modelName);
      
      if (!connectivityTest.success) {
        console.error("üîç DEBUG - √âchec du test de connectivit√© Ollama:", connectivityTest.message);
        interpretationsDiv.innerHTML = `<p class="error">${getTranslation('interpretation.connectionError', langue) || 'Erreur de connexion √† Ollama:'} ${connectivityTest.message}</p>`;
        return `Erreur de connexion: ${connectivityTest.message}`;
      }
    }
    
    // Obtenir le prompt sp√©cifique au persona
    const personaPrompt = getPersonaPrompt(persona, langue);
    
    // Obtenir le prompt sp√©cifique au tirage des cartes
    const tiragePrompt = tirage.length > 0 ? genererPromptTirage(tirage, message, spreadType, langue) : "";
    
    // Fusionner les prompts syst√®me avec ceux du persona et du tirage
    const mergedSystemPrompt = 
      `${getMetaPrompt(langue)}\n\n${personaPrompt}\n\n${tiragePrompt}`;
    
    // Ajouter l'emphase sur la question au d√©but du syst√®me prompt
    const finalSystemPrompt = enrichirPromptContextuel(message, mergedSystemPrompt, langue);
    
    console.log("üîç DEBUG - Prompts pr√©par√©s:", { 
      personaPromptLength: personaPrompt.length,
      tiragePromptLength: tiragePrompt.length,
      finalSystemPromptLength: finalSystemPrompt.length
    });
    
    // Pour le d√©bogage
    logPrompt(persona, message, finalSystemPrompt);
    
    // Cr√©er les messages pour l'API
    const messages = [
      {
        role: "system",
        content: finalSystemPrompt
      },
      {
        role: "user",
        content: message
      }
    ];
    
    // Ajouter les messages syst√®me suppl√©mentaires s'il y en a
    if (systemPrompts && systemPrompts.length > 0) {
      for (const sysPrompt of systemPrompts) {
        messages.push({
          role: "system",
          content: sysPrompt
        });
      }
    }
    
    // Construire la cl√© de cache
    const cacheKey = JSON.stringify({
      persona,
      message,
      tirage: tirage.map(card => card.id),
      spreadType,
      langue
    });
    
    // V√©rifier si nous avons d√©j√† cette r√©ponse en cache
    if (responseCache.has(cacheKey)) {
      console.log("üîç DEBUG - R√©ponse trouv√©e dans le cache");
      return responseCache.get(cacheKey);
    }
    
    // Pr√©parer les donn√©es de la requ√™te pour OpenAI ou Ollama
    const requestData = isOllama ? {
      model: modelName,
      messages: messages,
      stream: true
    } : {
      model: modelName,
      messages: messages,
      max_tokens: SETTINGS.MAX_TOKENS,
      stream: true
    };
    
    console.log("üîç DEBUG - Donn√©es de la requ√™te:", {
      model: modelName,
      messagesCount: messages.length,
      stream: true,
      max_tokens: isOllama ? "Non sp√©cifi√©" : SETTINGS.MAX_TOKENS
    });
    
    // En-t√™tes de la requ√™te (API key pour OpenAI uniquement)
    const headers = {
      'Content-Type': 'application/json',
      ...(isOllama ? {} : {'Authorization': `Bearer ${API_KEY}`})
    };
    
    console.log("üîç DEBUG - Envoi de la requ√™te API √†:", API_URL);
    
    // Faire la requ√™te √† l'API avec streaming
    const progressContainer = document.createElement('div');
    progressContainer.className = 'ollama-progress';
    progressContainer.innerHTML = `
      <p>${getTranslation('interpretation.streamingResponse', langue)}</p>
      <div class="progress-container">
        <div class="progress-bar"></div>
      </div>
    `;
    interpretationsDiv.innerHTML = '';
    interpretationsDiv.appendChild(progressContainer);
    
    const partialResponse = document.createElement('div');
    partialResponse.className = 'partial-response';
    interpretationsDiv.appendChild(partialResponse);
    
    try {
      console.log("üîç DEBUG - D√©but fetch API");
      
      // Fonction pour cr√©er un timeout pour le fetch
      const fetchWithTimeout = async (url, options, timeoutMs = 30000) => {
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), timeoutMs);
        
        try {
          const response = await fetch(url, {
            ...options,
            signal: controller.signal
          });
          clearTimeout(timeoutId);
          return response;
        } catch (error) {
          clearTimeout(timeoutId);
          if (error.name === 'AbortError') {
            throw new Error(`La requ√™te a expir√© apr√®s ${timeoutMs / 1000} secondes`);
          }
          throw error;
        }
      };
      
      const response = await fetchWithTimeout(API_URL, {
        method: 'POST',
        headers: headers,
        body: JSON.stringify(requestData)
      }, 60000); // 60 secondes de timeout
      
      console.log("üîç DEBUG - R√©ponse fetch re√ßue:", { 
        status: response.status, 
        ok: response.ok,
        statusText: response.statusText
      });
      
      // V√©rifier si la r√©ponse est OK
      if (!response.ok) {
        const errorText = await response.text();
        console.error("üîç DEBUG - Erreur API - R√©ponse texte:", errorText);
        
        try {
          const errorData = JSON.parse(errorText);
          console.error("üîç DEBUG - Erreur API - D√©tails:", errorData);
          throw new Error(`Erreur API: ${errorData.error?.message || response.statusText}`);
        } catch (parseError) {
          throw new Error(`Erreur API (${response.status}): ${response.statusText} - ${errorText.substring(0, 100)}...`);
        }
      }
      
      // Traiter le stream de la r√©ponse
      console.log("üîç DEBUG - D√©but traitement du stream");
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let fullResponse = '';
      
      // Timeout pour chaque chunk de r√©ponse
      let chunkPromise;
      let lastChunkTime = Date.now();
      const chunkTimeout = 30000; // 30 secondes entre chaque chunk
      
      const checkTimeout = () => {
        const now = Date.now();
        if (now - lastChunkTime > chunkTimeout) {
          throw new Error(`D√©lai d'attente d√©pass√©: aucune donn√©e re√ßue depuis ${chunkTimeout / 1000} secondes`);
        }
      };
      
      const timeoutInterval = setInterval(checkTimeout, 5000);
      
      try {
        // Utiliser un tableau pour stocker les morceaux de r√©ponse (optimisation)
        const responseChunks = [];
        let detectedFormat = null;
        
        while (true) {
          // Logging simplifi√©
          if (DEBUG_LEVEL > 1) console.log("üîç DEBUG - Attente de chunk de donn√©es");
          
          // Check for timeout while waiting for chunks
          chunkPromise = reader.read();
          const { done, value } = await chunkPromise;
          
          lastChunkTime = Date.now(); // Reset timeout counter
          
          if (done) {
            if (DEBUG_LEVEL > 0) console.log("üîç DEBUG - Stream termin√©");
            break;
          }
          
          const chunk = decoder.decode(value, { stream: true });
          if (DEBUG_LEVEL > 1) console.log("üîç DEBUG - Chunk re√ßu:", chunk.substring(0, 80) + (chunk.length > 80 ? "..." : ""));
          
          // Traiter le chunk en fonction du type d'API
          if (isOllama) {
            // Format Ollama: objets JSON s√©par√©s par des sauts de ligne
            const lines = chunk.split('\n');
            if (DEBUG_LEVEL > 1) console.log("üîç DEBUG - Traitement chunk Ollama:", { nombreLignes: lines.length });
            
            // Obtenir le format de r√©ponse pour ce mod√®le (utilis√© comme r√©f√©rence initiale)
            const modelFormat = getOllamaModelFormat(modelName);
            
            for (const line of lines) {
              if (!line.trim()) continue;
              
              try {
                const parsedChunk = JSON.parse(line);
                
                // Afficher le premier chunk complet pour le d√©bogage des formats
                if (!detectedFormat && DEBUG_LEVEL > 0) {
                  console.log("üîç DEBUG - Premier chunk Ollama pour analyse:", parsedChunk);
                }
                
                // D√©tection intelligente du format (si pas encore d√©tect√©)
                if (!detectedFormat) {
                  if (parsedChunk.message?.content !== undefined) {
                    detectedFormat = 'message.content';
                  } else if (parsedChunk.response !== undefined) {
                    detectedFormat = 'response';
                  } else if (modelFormat && modelFormat.responseKey) {
                    detectedFormat = modelFormat.responseKey;
                  } else {
                    detectedFormat = 'unknown';
                  }
                  
                  if (DEBUG_LEVEL > 0) console.log(`üîç DEBUG - Format d√©tect√© pour ${modelName}: ${detectedFormat}`);
                }
                
                // Extraction du texte selon le format d√©tect√© (approche am√©lior√©e)
                let responseText = '';
                
                // V√©rifier d'abord explicitement le format pour llama3.1
                if (modelName.includes('llama3.1')) {
                  // Pour llama3.1, essayer sp√©cifiquement ces chemins
                  if (parsedChunk.message?.content !== undefined) {
                    responseText = parsedChunk.message.content;
                  } else if (parsedChunk.content !== undefined) {
                    responseText = parsedChunk.content;
                  } else if (parsedChunk.response !== undefined) {
                    responseText = parsedChunk.response;
                  }
                  
                  if (DEBUG_LEVEL > 1 && responseText) {
                    console.log(`üîç DEBUG - Texte extrait pour llama3.1:`, responseText.substring(0, 20) + "...");
                  }
                } else {
                  // Pour les autres mod√®les, suivre la logique normale
                  if (detectedFormat === 'message.content' && parsedChunk.message?.content !== undefined) {
                    responseText = parsedChunk.message.content;
                  } else if (detectedFormat === 'response' && parsedChunk.response !== undefined) {
                    responseText = parsedChunk.response;
                  } else {
                    // Fallback aux autres m√©thodes si le format d√©tect√© n'est pas disponible
                    responseText = getValueByPath(parsedChunk, detectedFormat) || 
                                  parsedChunk.response || 
                                  parsedChunk.message?.content || 
                                  '';
                  }
                }
                
                // Logging minimal des informations importantes
                if (DEBUG_LEVEL > 1) {
                  console.log("üîç DEBUG - Ligne Ollama:", { 
                    format: detectedFormat,
                    textFound: responseText !== undefined && responseText !== '',
                    snippet: responseText ? (responseText.substring(0, 20) + "...") : null,
                    done: parsedChunk.done
                  });
                }
                
                // Si on a trouv√© du texte de r√©ponse, l'ajouter √† la r√©ponse
                if (responseText) {
                  responseChunks.push(responseText);
                  // Mise √† jour de l'affichage avec la r√©ponse cumulative
                  fullResponse = responseChunks.join('');
                  partialResponse.innerHTML = formatStreamingResponse(fullResponse);
                }
                
                if (parsedChunk.done) {
                  if (DEBUG_LEVEL > 0) console.log("üîç DEBUG - Ollama a signal√© la fin (done=true)");
                }
              } catch (e) {
                console.error("üîç DEBUG - Erreur parsing chunk Ollama:", { 
                  error: e.message,
                  snippet: line.substring(0, 50) + (line.length > 50 ? "..." : "") 
                });
              }
            }
          } else {
            // Format OpenAI: "data: {JSON}" s√©par√©s par des sauts de ligne
            const lines = chunk.split('\n');
            for (const line of lines) {
              if (!line.trim() || !line.startsWith('data: ')) continue;
              
              const jsonStr = line.replace(/^data: /, '');
              if (jsonStr === '[DONE]') continue;
              
              try {
                const parsedChunk = JSON.parse(jsonStr);
                const content = parsedChunk.choices[0]?.delta?.content || '';
                if (content) {
                  fullResponse += content;
                  partialResponse.innerHTML = formatStreamingResponse(fullResponse);
                }
              } catch (e) {
                console.error("Erreur lors du parsing du chunk OpenAI:", e);
              }
            }
          }
        }
        
        // √Ä la fin du traitement du stream, juste avant de retourner la r√©ponse
        if (fullResponse.trim() === '') {
          console.error("üîç DEBUG - R√©ponse vide apr√®s traitement complet.");
          throw new Error("R√©ponse vide re√ßue du mod√®le");
        }
        
        // V√©rifier que la r√©ponse n'est pas trop courte ou incompl√®te
        if (fullResponse.length < 20) {
          console.warn("üîç DEBUG - R√©ponse tr√®s courte:", fullResponse);
          // Ne pas lever d'erreur mais enregistrer l'avertissement
        }
        
        // Ajouter un marqueur pour indiquer que le streaming s'est termin√© correctement
        fullResponse += "\n\n<!-- streaming-completed -->";
        
        // Mettre en cache la r√©ponse
        responseCache.set(cacheKey, fullResponse);
        
        // Retourner la r√©ponse compl√®te
        return fullResponse;
      } finally {
        clearInterval(timeoutInterval);
      }
    } catch (fetchError) {
      console.error("üîç DEBUG - Erreur critique fetch:", fetchError);
      interpretationsDiv.innerHTML = `<p class="error">${getTranslation('interpretation.apiError', langue) || 'Erreur API:'} ${fetchError.message}</p>`;
      throw fetchError;
    }
  } catch (error) {
    console.error("üîç DEBUG - Erreur globale:", error);
    interpretationsDiv.innerHTML = `<p class="error">${getTranslation('interpretation.error', langue) || 'Erreur:'} ${error.message}</p>`;
    throw error;
  }
}

/**
 * Fonction pour r√©cup√©rer les mod√®les disponibles sur Ollama
 * @returns {Promise<Array>} - Liste des mod√®les disponibles
 */
async function obtenirModelesOllama() {
  try {
    // V√©rifier d'abord si Ollama est accessible
    const ollamaConnected = await verifierConnexionOllama();
    
    if (!ollamaConnected) {
      console.warn("Ollama n'est pas accessible");
      return [];
    }
    
    // R√©cup√©rer la liste des mod√®les
    const response = await fetch(API_URL_OLLAMA_TAGS, {
      method: 'GET',
      headers: {
        'Content-Type': 'application/json'
      }
    });
    
    if (!response.ok) {
      throw new Error(`Erreur lors de la r√©cup√©ration des mod√®les: ${response.statusText}`);
    }
    
    const data = await response.json();
    
    // Retourner la liste des mod√®les
    return data.models || [];
  } catch (error) {
    console.error("Erreur lors de la r√©cup√©ration des mod√®les Ollama:", error);
    return [];
  }
}

/**
 * Fonction pour v√©rifier si Ollama est accessible
 * @returns {Promise<boolean>} - true si Ollama est accessible, false sinon
 */
async function verifierConnexionOllama() {
  try {
    const response = await fetch(API_URL_OLLAMA_TAGS, { 
      method: 'GET',
      headers: {
        'Content-Type': 'application/json'
      },
      signal: AbortSignal.timeout(2000) // Timeout apr√®s 2 secondes
    });
    
    return response.ok;
  } catch (error) {
    console.warn("Ollama n'est pas accessible:", error);
    return false;
  }
}

/**
 * Fonction pour tester la connectivit√© avec Ollama
 * @param {string} modelName - Nom du mod√®le √† tester
 * @returns {Promise<Object>} - R√©sultat du test avec statut et message
 */
async function testOllamaConnectivity(modelName) {
  console.log("üîç DEBUG - Test de connectivit√© Ollama pour le mod√®le:", modelName);
  
  try {
    // 1. Test simple de ping sur le serveur Ollama
    console.log("üîç DEBUG - Test ping serveur Ollama");
    const pingResponse = await fetch(`${API_URL_OLLAMA.replace('/api/chat', '')}/api/tags`, {
      method: 'GET',
      headers: { 'Content-Type': 'application/json' }
    });
    
    if (!pingResponse.ok) {
      console.error("üîç DEBUG - Serveur Ollama inaccessible:", pingResponse.status, pingResponse.statusText);
      return { 
        success: false, 
        message: `Serveur Ollama inaccessible (${pingResponse.status}): ${pingResponse.statusText}` 
      };
    }
    
    // 2. V√©rifier si le mod√®le est disponible
    console.log("üîç DEBUG - V√©rification disponibilit√© du mod√®le:", modelName);
    const modelsData = await pingResponse.json();
    
    if (!modelsData.models) {
      console.error("üîç DEBUG - Format de r√©ponse Ollama inattendu:", modelsData);
      return { 
        success: false, 
        message: "Format de r√©ponse Ollama inattendu" 
      };
    }
    
    const modelExists = modelsData.models.some(m => m.name === modelName);
    if (!modelExists) {
      console.error("üîç DEBUG - Mod√®le non trouv√©:", modelName);
      return { 
        success: false, 
        message: `Le mod√®le ${modelName} n'est pas disponible sur ce serveur Ollama` 
      };
    }
    
    // 3. Test rapide du mod√®le
    console.log("üîç DEBUG - Test rapide du mod√®le:", modelName);
    const testResponse = await fetch(API_URL_OLLAMA, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: modelName,
        messages: [{ role: "user", content: "R√©ponds simplement par 'OK' pour tester la connectivit√©." }],
        stream: false
      })
    });
    
    if (!testResponse.ok) {
      console.error("üîç DEBUG - Test du mod√®le √©chou√©:", testResponse.status, testResponse.statusText);
      return { 
        success: false, 
        message: `Test du mod√®le √©chou√© (${testResponse.status}): ${testResponse.statusText}` 
      };
    }
    
    console.log("üîç DEBUG - Connectivit√© Ollama OK pour:", modelName);
    return { success: true, message: "Connectivit√© Ollama OK" };
    
  } catch (error) {
    console.error("üîç DEBUG - Erreur lors du test de connectivit√© Ollama:", error);
    return { 
      success: false, 
      message: `Erreur de connectivit√©: ${error.message}` 
    };
  }
}

// Am√©liorer les logs
function logPrompt(persona, question, systemPrompt) {
  console.group("G√©n√©ration de r√©ponse");
  console.log("Persona:", persona);
  console.log("Question:", question);
  console.log("Prompt syst√®me:", systemPrompt);
  console.groupEnd();
}

/**
 * Formate le texte de r√©ponse pour l'affichage en HTML
 * @param {string} text - Texte √† formater
 * @returns {string} - HTML format√©
 */
function formatStreamingResponse(text) {
  if (!text || typeof text !== 'string') {
    console.error("Erreur: Texte invalide pour formatage", text);
    return '<p>En attente de r√©ponse...</p>';
  }
  
  // R√©duire les logs de d√©bogage
  if (DEBUG_LEVEL > 2) console.log("üîç DEBUG - formatStreamingResponse:", text.substring(0, 30) + "...");
  
  // Cache statique pour optimiser les appels r√©p√©t√©s
  if (!formatStreamingResponse.containsHtmlCache) {
    formatStreamingResponse.containsHtmlCache = new Map();
  }
  
  // V√©rifier si on a d√©j√† analys√© ce texte (optimisation)
  const cacheKey = text.substring(0, 100); // Utiliser d√©but du texte comme cl√©
  
  let containsHtml;
  if (formatStreamingResponse.containsHtmlCache.has(cacheKey)) {
    containsHtml = formatStreamingResponse.containsHtmlCache.get(cacheKey);
  } else {
    // V√©rifie si le texte contient d√©j√† des balises HTML
    containsHtml = /<\/?[a-z][\s\S]*>/i.test(text);
    formatStreamingResponse.containsHtmlCache.set(cacheKey, containsHtml);
  }
  
  if (containsHtml) {
    // Si le texte contient d√©j√† du HTML, v√©rifier seulement qu'il est envelopp√© dans un conteneur
    return text.trim().startsWith('<') ? text : `<div>${text}</div>`;
  } else {
    // Pour le texte brut, diviser en paragraphes et formater
    return text.split('\n').map(paragraph => 
      paragraph.trim() ? `<p>${paragraph}</p>` : ''
    ).join('');
  }
}

/**
 * Extrait une valeur depuis un objet en utilisant une notation par points
 * @param {Object} obj - L'objet source
 * @param {string} path - Chemin de la propri√©t√© (ex: "message.content")
 * @returns {*} - La valeur ou undefined si non trouv√©e
 */
function getValueByPath(obj, path) {
  if (!obj || !path) return undefined;
  
  const parts = path.split('.');
  let value = obj;
  
  for (const part of parts) {
    if (value === undefined || value === null) return undefined;
    value = value[part];
  }
  
  return value;
}

// Exporter les fonctions
export {
  obtenirReponseGPT4O,
  obtenirModelesOllama,
  verifierConnexionOllama,
  enrichirPromptContextuel,
  logPrompt,
  testOllamaConnectivity
};