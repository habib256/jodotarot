/**
 * Module de gestion des appels API aux mod√®les d'IA
 */

import { API_KEY, API_URL_OPENAI, API_URL_OLLAMA, API_URL_OLLAMA_TAGS, getMetaPrompt, getOllamaModelFormat } from './config.js';
import { genererPromptTirage } from './tarot.js';
import PERSONAS, { getPersonaPrompt } from './personas/index.js';
import { getTranslation } from './translations.js';

// Configuration du niveau de d√©boggage
// 0 = Erreurs seulement, 1 = Infos importantes, 2 = D√©tails, 3 = Verbeux
const DEBUG_LEVEL = 1; 

// Syst√®me simple de cache pour les r√©ponses
const responseCache = new Map();

// Cr√©er un fichier de configuration centrale
const SETTINGS = {
  DEFAULT_PERSONA: "tarologue",
  DEFAULT_LANGUAGE: "fr",
  MAX_TOKENS: 500,
  // etc.
};

/**
 * Fonction pour enrichir le prompt syst√®me avec le contexte sp√©cifique √† la question
 * @param {string} question - La question pos√©e par l'utilisateur
 * @param {string} systemPrompt - Le prompt syst√®me de base
 * @param {string} langue - La langue √† utiliser (fr par d√©faut)
 * @returns {string} - Le prompt syst√®me enrichi avec la question
 */
function enrichirPromptContextuel(question, systemPrompt, langue = 'fr') {
  // Ajouter la question de l'utilisateur au prompt syst√®me
  if (question && question.trim()) {
    // Obtenir la traduction de l'introduction √† la question
    const questionIntro = getTranslation('interpretation.userQuestion', langue);
    
    // Textes d'emphase traduits pour chaque langue
    const emphaseTextes = {
      fr: `IMPORTANT: Ta r√©ponse doit √™tre DIRECTEMENT et SP√âCIFIQUEMENT li√©e √† cette question.
Concentre-toi sur ce que la question demande pr√©cis√©ment et adapte ton interpr√©tation 
en fonction des √©l√©ments mentionn√©s dans la question. Ne donne pas une r√©ponse g√©n√©rique.
Chaque aspect de ton interpr√©tation doit r√©pondre √† un aspect de cette question.`,
      
      en: `IMPORTANT: Your answer must be DIRECTLY and SPECIFICALLY related to this question.
Focus on what the question precisely asks and adapt your interpretation
based on the elements mentioned in the question. Do not give a generic answer.
Each aspect of your interpretation must address an aspect of this question.`,
      
      es: `IMPORTANTE: Tu respuesta debe estar DIRECTA y ESPEC√çFICAMENTE relacionada con esta pregunta.
Conc√©ntrate en lo que la pregunta pide con precisi√≥n y adapta tu interpretaci√≥n
seg√∫n los elementos mencionados en la pregunta. No des una respuesta gen√©rica.
Cada aspecto de tu interpretaci√≥n debe responder a un aspecto de esta pregunta.`,
      
      de: `WICHTIG: Deine Antwort muss DIREKT und SPEZIFISCH mit dieser Frage zusammenh√§ngen.
Konzentriere dich darauf, was die Frage genau fragt, und passe deine Interpretation
an die in der Frage genannten Elemente an. Gib keine allgemeine Antwort.
Jeder Aspekt deiner Interpretation muss auf einen Aspekt dieser Frage eingehen.`,
      
      it: `IMPORTANTE: La tua risposta deve essere DIRETTAMENTE e SPECIFICAMENTE legata a questa domanda.
Concentrati su ci√≤ che la domanda chiede precisamente e adatta la tua interpretazione
in base agli elementi menzionati nella domanda. Non dare una risposta generica.
Ogni aspetto della tua interpretazione deve rispondere a un aspetto di questa domanda.`
    };
    
    // S√©lectionner le texte d'emphase dans la langue appropri√©e
    const emphaseTexte = emphaseTextes[langue] || emphaseTextes.fr;
    
    // Former le bloc d'emphase avec les d√©limiteurs et la question
    const questionBlock = `====================
${questionIntro}:
"${question.trim()}"
====================`;
    
    // V√©rifier si le prompt contient d√©j√† ce bloc ou une partie significative
    if (systemPrompt.includes(questionBlock) || 
        (systemPrompt.includes(`"${question.trim()}"`) && 
         systemPrompt.includes("====================") && 
         systemPrompt.includes(emphaseTexte.substring(0, 50)))) {
      return systemPrompt;
    }
    
    return `
${questionBlock}

${emphaseTexte}

${systemPrompt}`;
  }
  
  return systemPrompt;
}

/**
 * Fonction pour obtenir une r√©ponse de l'API OpenAI avec GPT-4o
 * @param {string} message - La question pos√©e par l'utilisateur
 * @param {Array} systemPrompts - Tableau de prompts syst√®me additionnels
 * @param {string} modele - Mod√®le d'IA √† utiliser
 * @param {string} persona - Persona s√©lectionn√© pour l'interpr√©tation
 * @param {Array} tirage - Tableau des cartes tir√©es
 * @param {string} langue - Langue s√©lectionn√©e (fr, en, es, de, it)
 * @param {string} spreadType - Type de tirage (cross ou horseshoe)
 * @returns {Promise<string>} - R√©ponse de l'API
 */
async function obtenirReponseGPT4O(message, systemPrompts = [], modele = 'openai/gpt-3.5-turbo', persona = 'tarologue', tirage = [], langue = 'fr', spreadType = 'cross') {
  // V√©rifier quel type d'API nous utilisons (OpenAI ou Ollama)
  const isOllama = !modele.startsWith('openai/');
  
  console.log("üîç DEBUG - D√©marrage appel API:", { 
    modele, 
    isOllama, 
    persona,
    nombreCartes: tirage.length,
    langue,
    spreadType
  });
  
  // R√©cup√©rer l'√©l√©ment pour afficher les erreurs et l'interpr√©tation
  const interpretationsDiv = document.getElementById('interpretations');
  
  try {
    // Pr√©parer les donn√©es pour l'API
    const modelName = isOllama ? modele.split('/')[1] : modele.split('/')[1];
    const API_URL = isOllama ? API_URL_OLLAMA : API_URL_OPENAI;
    
    console.log("üîç DEBUG - Configuration API:", { 
      modelName, 
      API_URL,
      headers: isOllama ? "Standard" : "Avec API_KEY" 
    });
    
    // Tester la connectivit√© d'Ollama si applicable
    if (isOllama) {
      interpretationsDiv.innerHTML = `<p class="loading">${getTranslation('interpretation.testingConnection', langue) || 'Test de connexion √† Ollama...'}</p>`;
      const connectivityTest = await testOllamaConnectivity(modelName);
      
      if (!connectivityTest.success) {
        console.error("üîç DEBUG - √âchec du test de connectivit√© Ollama:", connectivityTest.message);
        interpretationsDiv.innerHTML = `<p class="error">${getTranslation('interpretation.connectionError', langue) || 'Erreur de connexion √† Ollama:'} ${connectivityTest.message}</p>`;
        return `Erreur de connexion: ${connectivityTest.message}`;
      }
    }
    
    // Obtenir le prompt sp√©cifique au persona
    const personaPrompt = getPersonaPrompt(persona, langue);
    
    // Obtenir le prompt sp√©cifique au tirage des cartes
    const tiragePrompt = tirage.length > 0 ? genererPromptTirage(tirage, message, spreadType, langue) : "";
    
    // Fusionner les prompts syst√®me avec ceux du persona et du tirage
    const mergedSystemPrompt = 
      `${getMetaPrompt(langue)}\n\n${personaPrompt}\n\n${tiragePrompt}`;
    
    // Ajouter l'emphase sur la question au d√©but du syst√®me prompt
    const finalSystemPrompt = enrichirPromptContextuel(message, mergedSystemPrompt, langue);
    
    console.log("üîç DEBUG - Prompts pr√©par√©s:", { 
      personaPromptLength: personaPrompt.length,
      tiragePromptLength: tiragePrompt.length,
      finalSystemPromptLength: finalSystemPrompt.length
    });
    
    // Pour le d√©bogage
    logPrompt(persona, message, finalSystemPrompt);
    
    // Cr√©er les messages pour l'API
    const messages = [
      {
        role: "system",
        content: finalSystemPrompt
      },
      {
        role: "user",
        content: message
      }
    ];
    
    // Ajouter les messages syst√®me suppl√©mentaires s'il y en a
    if (systemPrompts && systemPrompts.length > 0) {
      for (const sysPrompt of systemPrompts) {
        messages.push({
          role: "system",
          content: sysPrompt
        });
      }
    }
    
    // Construire la cl√© de cache
    const cacheKey = JSON.stringify({
      persona,
      message,
      tirage: tirage.map(card => card.id),
      spreadType,
      langue
    });
    
    // V√©rifier si nous avons d√©j√† cette r√©ponse en cache
    if (responseCache.has(cacheKey)) {
      console.log("üîç DEBUG - R√©ponse trouv√©e dans le cache");
      return responseCache.get(cacheKey);
    }
    
    // Pr√©parer les donn√©es de la requ√™te pour OpenAI ou Ollama
    const requestData = isOllama ? {
      model: modelName,
      messages: messages,
      stream: true
    } : {
      model: modelName,
      messages: messages,
      max_tokens: SETTINGS.MAX_TOKENS,
      stream: true
    };
    
    console.log("üîç DEBUG - Donn√©es de la requ√™te:", {
      model: modelName,
      messagesCount: messages.length,
      stream: true,
      max_tokens: isOllama ? "Non sp√©cifi√©" : SETTINGS.MAX_TOKENS
    });
    
    // En-t√™tes de la requ√™te (API key pour OpenAI uniquement)
    const headers = {
      'Content-Type': 'application/json',
      ...(isOllama ? {} : {'Authorization': `Bearer ${API_KEY}`})
    };
    
    console.log("üîç DEBUG - Envoi de la requ√™te API √†:", API_URL);
    
    // Faire la requ√™te √† l'API avec streaming
    const progressContainer = document.createElement('div');
    progressContainer.className = 'ollama-progress';
    progressContainer.innerHTML = `
      <p>${getTranslation('interpretation.streamingResponse', langue)}</p>
      <div class="progress-container">
        <div class="progress-bar"></div>
      </div>
    `;
    interpretationsDiv.innerHTML = '';
    interpretationsDiv.appendChild(progressContainer);
    
    const partialResponse = document.createElement('div');
    partialResponse.className = 'partial-response';
    interpretationsDiv.appendChild(partialResponse);
    
    try {
      console.log("üîç DEBUG - D√©but fetch API");
      
      // Fonction pour cr√©er un timeout pour le fetch
      const fetchWithTimeout = async (url, options, timeoutMs = 30000) => {
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), timeoutMs);
        
        try {
          const response = await fetch(url, {
            ...options,
            signal: controller.signal
          });
          clearTimeout(timeoutId);
          return response;
        } catch (error) {
          clearTimeout(timeoutId);
          if (error.name === 'AbortError') {
            throw new Error(`La requ√™te a expir√© apr√®s ${timeoutMs / 1000} secondes`);
          }
          throw error;
        }
      };
      
      const response = await fetchWithTimeout(API_URL, {
        method: 'POST',
        headers: headers,
        body: JSON.stringify(requestData)
      }, 60000); // 60 secondes de timeout
      
      console.log("üîç DEBUG - R√©ponse fetch re√ßue:", { 
        status: response.status, 
        ok: response.ok,
        statusText: response.statusText
      });
      
      // V√©rifier si la r√©ponse est OK
      if (!response.ok) {
        const errorText = await response.text();
        console.error("üîç DEBUG - Erreur API - R√©ponse texte:", errorText);
        
        try {
          const errorData = JSON.parse(errorText);
          console.error("üîç DEBUG - Erreur API - D√©tails:", errorData);
          throw new Error(`Erreur API: ${errorData.error?.message || response.statusText}`);
        } catch (parseError) {
          throw new Error(`Erreur API (${response.status}): ${response.statusText} - ${errorText.substring(0, 100)}...`);
        }
      }
      
      // Traiter le stream de la r√©ponse
      console.log("üîç DEBUG - D√©but traitement du stream");
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let fullResponse = '';
      
      // Timeout pour chaque chunk de r√©ponse
      let chunkPromise;
      let lastChunkTime = Date.now();
      const chunkTimeout = 30000; // 30 secondes entre chaque chunk
      
      const checkTimeout = () => {
        const now = Date.now();
        if (now - lastChunkTime > chunkTimeout) {
          throw new Error(`D√©lai d'attente d√©pass√©: aucune donn√©e re√ßue depuis ${chunkTimeout / 1000} secondes`);
        }
      };
      
      const timeoutInterval = setInterval(checkTimeout, 5000);
      
      try {
        // Utiliser un tableau pour stocker les morceaux de r√©ponse (optimisation)
        const responseChunks = [];
        let detectedFormat = null;
        
        while (true) {
          // Logging simplifi√©
          if (DEBUG_LEVEL > 1) console.log("üîç DEBUG - Attente de chunk de donn√©es");
          
          // Check for timeout while waiting for chunks
          chunkPromise = reader.read();
          const { done, value } = await chunkPromise;
          
          lastChunkTime = Date.now(); // Reset timeout counter
          
          if (done) {
            if (DEBUG_LEVEL > 0) console.log("üîç DEBUG - Stream termin√©");
            break;
          }
          
          const chunk = decoder.decode(value, { stream: true });
          if (DEBUG_LEVEL > 1) console.log("üîç DEBUG - Chunk re√ßu:", chunk.substring(0, 80) + (chunk.length > 80 ? "..." : ""));
          
          // Traiter le chunk en fonction du type d'API
          if (isOllama) {
            // Format Ollama: objets JSON s√©par√©s par des sauts de ligne
            const lines = chunk.split('\n');
            if (DEBUG_LEVEL > 1) console.log("üîç DEBUG - Traitement chunk Ollama:", { nombreLignes: lines.length });
            
            // Obtenir le format de r√©ponse pour ce mod√®le (utilis√© comme r√©f√©rence initiale)
            const modelFormat = getOllamaModelFormat(modelName);
            
            for (const line of lines) {
              if (!line.trim()) continue;
              
              try {
                const parsedChunk = JSON.parse(line);
                
                // D√©tection intelligente du format (si pas encore d√©tect√©)
                if (!detectedFormat) {
                  if (parsedChunk.message?.content !== undefined) {
                    detectedFormat = 'message.content';
                  } else if (parsedChunk.response !== undefined) {
                    detectedFormat = 'response';
                  } else if (modelFormat && modelFormat.responseKey) {
                    detectedFormat = modelFormat.responseKey;
                  } else {
                    detectedFormat = 'unknown';
                  }
                  
                  if (DEBUG_LEVEL > 0) console.log(`üîç DEBUG - Format d√©tect√© pour ${modelName}: ${detectedFormat}`);
                }
                
                // Extraction du texte selon le format d√©tect√© (approche simplifi√©e)
                let responseText;
                if (detectedFormat === 'message.content' && parsedChunk.message?.content !== undefined) {
                  responseText = parsedChunk.message.content;
                } else if (detectedFormat === 'response' && parsedChunk.response !== undefined) {
                  responseText = parsedChunk.response;
                } else {
                  // Fallback aux autres m√©thodes si le format d√©tect√© n'est pas disponible
                  responseText = getValueByPath(parsedChunk, detectedFormat) || 
                                 parsedChunk.response || 
                                 parsedChunk.message?.content || 
                                 '';
                }
                
                // Logging minimal des informations importantes
                if (DEBUG_LEVEL > 1) {
                  console.log("üîç DEBUG - Ligne Ollama:", { 
                    format: detectedFormat,
                    textFound: responseText !== undefined && responseText !== '',
                    snippet: responseText ? (responseText.substring(0, 20) + "...") : null,
                    done: parsedChunk.done
                  });
                }
                
                // Si on a trouv√© du texte de r√©ponse, l'ajouter √† la r√©ponse
                if (responseText) {
                  responseChunks.push(responseText);
                  // Mise √† jour de l'affichage avec la r√©ponse cumulative
                  fullResponse = responseChunks.join('');
                  partialResponse.innerHTML = formatStreamingResponse(fullResponse);
                }
                
                if (parsedChunk.done) {
                  if (DEBUG_LEVEL > 0) console.log("üîç DEBUG - Ollama a signal√© la fin (done=true)");
                }
              } catch (e) {
                console.error("üîç DEBUG - Erreur parsing chunk Ollama:", { 
                  error: e.message,
                  snippet: line.substring(0, 50) + (line.length > 50 ? "..." : "") 
                });
              }
            }
          } else {
            // Format OpenAI: "data: {JSON}" s√©par√©s par des sauts de ligne
            const lines = chunk.split('\n');
            for (const line of lines) {
              if (!line.trim() || !line.startsWith('data: ')) continue;
              
              const jsonStr = line.replace(/^data: /, '');
              if (jsonStr === '[DONE]') continue;
              
              try {
                const parsedChunk = JSON.parse(jsonStr);
                const content = parsedChunk.choices[0]?.delta?.content || '';
                if (content) {
                  fullResponse += content;
                  partialResponse.innerHTML = formatStreamingResponse(fullResponse);
                }
              } catch (e) {
                console.error("Erreur lors du parsing du chunk OpenAI:", e);
              }
            }
          }
        }
        
        // Mettre en cache la r√©ponse
        responseCache.set(cacheKey, fullResponse);
        
        // Retourner la r√©ponse compl√®te
        return fullResponse;
      } finally {
        clearInterval(timeoutInterval);
      }
    } catch (fetchError) {
      console.error("üîç DEBUG - Erreur critique fetch:", fetchError);
      interpretationsDiv.innerHTML = `<p class="error">${getTranslation('interpretation.apiError', langue) || 'Erreur API:'} ${fetchError.message}</p>`;
      throw fetchError;
    }
  } catch (error) {
    console.error("üîç DEBUG - Erreur globale:", error);
    interpretationsDiv.innerHTML = `<p class="error">${getTranslation('interpretation.error', langue) || 'Erreur:'} ${error.message}</p>`;
    throw error;
  }
}

/**
 * Fonction pour r√©cup√©rer les mod√®les disponibles sur Ollama
 * @returns {Promise<Array>} - Liste des mod√®les disponibles
 */
async function obtenirModelesOllama() {
  try {
    // V√©rifier d'abord si Ollama est accessible
    const ollamaConnected = await verifierConnexionOllama();
    
    if (!ollamaConnected) {
      console.warn("Ollama n'est pas accessible");
      return [];
    }
    
    // R√©cup√©rer la liste des mod√®les
    const response = await fetch(API_URL_OLLAMA_TAGS, {
      method: 'GET',
      headers: {
        'Content-Type': 'application/json'
      }
    });
    
    if (!response.ok) {
      throw new Error(`Erreur lors de la r√©cup√©ration des mod√®les: ${response.statusText}`);
    }
    
    const data = await response.json();
    
    // Retourner la liste des mod√®les
    return data.models || [];
  } catch (error) {
    console.error("Erreur lors de la r√©cup√©ration des mod√®les Ollama:", error);
    return [];
  }
}

/**
 * Fonction pour v√©rifier si Ollama est accessible
 * @returns {Promise<boolean>} - true si Ollama est accessible, false sinon
 */
async function verifierConnexionOllama() {
  try {
    const response = await fetch(API_URL_OLLAMA_TAGS, { 
      method: 'GET',
      headers: {
        'Content-Type': 'application/json'
      },
      signal: AbortSignal.timeout(2000) // Timeout apr√®s 2 secondes
    });
    
    return response.ok;
  } catch (error) {
    console.warn("Ollama n'est pas accessible:", error);
    return false;
  }
}

/**
 * Fonction pour tester la connectivit√© avec Ollama
 * @param {string} modelName - Nom du mod√®le √† tester
 * @returns {Promise<Object>} - R√©sultat du test avec statut et message
 */
async function testOllamaConnectivity(modelName) {
  console.log("üîç DEBUG - Test de connectivit√© Ollama pour le mod√®le:", modelName);
  
  try {
    // 1. Test simple de ping sur le serveur Ollama
    console.log("üîç DEBUG - Test ping serveur Ollama");
    const pingResponse = await fetch(`${API_URL_OLLAMA.replace('/api/chat', '')}/api/tags`, {
      method: 'GET',
      headers: { 'Content-Type': 'application/json' }
    });
    
    if (!pingResponse.ok) {
      console.error("üîç DEBUG - Serveur Ollama inaccessible:", pingResponse.status, pingResponse.statusText);
      return { 
        success: false, 
        message: `Serveur Ollama inaccessible (${pingResponse.status}): ${pingResponse.statusText}` 
      };
    }
    
    // 2. V√©rifier si le mod√®le est disponible
    console.log("üîç DEBUG - V√©rification disponibilit√© du mod√®le:", modelName);
    const modelsData = await pingResponse.json();
    
    if (!modelsData.models) {
      console.error("üîç DEBUG - Format de r√©ponse Ollama inattendu:", modelsData);
      return { 
        success: false, 
        message: "Format de r√©ponse Ollama inattendu" 
      };
    }
    
    const modelExists = modelsData.models.some(m => m.name === modelName);
    if (!modelExists) {
      console.error("üîç DEBUG - Mod√®le non trouv√©:", modelName);
      return { 
        success: false, 
        message: `Le mod√®le ${modelName} n'est pas disponible sur ce serveur Ollama` 
      };
    }
    
    // 3. Test rapide du mod√®le
    console.log("üîç DEBUG - Test rapide du mod√®le:", modelName);
    const testResponse = await fetch(API_URL_OLLAMA, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: modelName,
        messages: [{ role: "user", content: "R√©ponds simplement par 'OK' pour tester la connectivit√©." }],
        stream: false
      })
    });
    
    if (!testResponse.ok) {
      console.error("üîç DEBUG - Test du mod√®le √©chou√©:", testResponse.status, testResponse.statusText);
      return { 
        success: false, 
        message: `Test du mod√®le √©chou√© (${testResponse.status}): ${testResponse.statusText}` 
      };
    }
    
    console.log("üîç DEBUG - Connectivit√© Ollama OK pour:", modelName);
    return { success: true, message: "Connectivit√© Ollama OK" };
    
  } catch (error) {
    console.error("üîç DEBUG - Erreur lors du test de connectivit√© Ollama:", error);
    return { 
      success: false, 
      message: `Erreur de connectivit√©: ${error.message}` 
    };
  }
}

// Am√©liorer les logs
function logPrompt(persona, question, systemPrompt) {
  console.group("G√©n√©ration de r√©ponse");
  console.log("Persona:", persona);
  console.log("Question:", question);
  console.log("Prompt syst√®me:", systemPrompt);
  console.groupEnd();
}

/**
 * Formate le texte de r√©ponse pour l'affichage en HTML
 * @param {string} text - Texte √† formater
 * @returns {string} - HTML format√©
 */
function formatStreamingResponse(text) {
  if (!text || typeof text !== 'string') {
    console.error("Erreur: Texte invalide pour formatage", text);
    return '<p>En attente de r√©ponse...</p>';
  }
  
  // R√©duire les logs de d√©bogage
  if (DEBUG_LEVEL > 2) console.log("üîç DEBUG - formatStreamingResponse:", text.substring(0, 30) + "...");
  
  // Cache statique pour optimiser les appels r√©p√©t√©s
  if (!formatStreamingResponse.containsHtmlCache) {
    formatStreamingResponse.containsHtmlCache = new Map();
  }
  
  // V√©rifier si on a d√©j√† analys√© ce texte (optimisation)
  const cacheKey = text.substring(0, 100); // Utiliser d√©but du texte comme cl√©
  
  let containsHtml;
  if (formatStreamingResponse.containsHtmlCache.has(cacheKey)) {
    containsHtml = formatStreamingResponse.containsHtmlCache.get(cacheKey);
  } else {
    // V√©rifie si le texte contient d√©j√† des balises HTML
    containsHtml = /<\/?[a-z][\s\S]*>/i.test(text);
    formatStreamingResponse.containsHtmlCache.set(cacheKey, containsHtml);
  }
  
  if (containsHtml) {
    // Si le texte contient d√©j√† du HTML, v√©rifier seulement qu'il est envelopp√© dans un conteneur
    return text.trim().startsWith('<') ? text : `<div>${text}</div>`;
  } else {
    // Pour le texte brut, diviser en paragraphes et formater
    return text.split('\n').map(paragraph => 
      paragraph.trim() ? `<p>${paragraph}</p>` : ''
    ).join('');
  }
}

/**
 * Extrait une valeur depuis un objet en utilisant une notation par points
 * @param {Object} obj - L'objet source
 * @param {string} path - Chemin de la propri√©t√© (ex: "message.content")
 * @returns {*} - La valeur ou undefined si non trouv√©e
 */
function getValueByPath(obj, path) {
  if (!obj || !path) return undefined;
  
  const parts = path.split('.');
  let value = obj;
  
  for (const part of parts) {
    if (value === undefined || value === null) return undefined;
    value = value[part];
  }
  
  return value;
}

// Exporter les fonctions
export {
  obtenirReponseGPT4O,
  obtenirModelesOllama,
  verifierConnexionOllama,
  enrichirPromptContextuel,
  logPrompt,
  testOllamaConnectivity
};