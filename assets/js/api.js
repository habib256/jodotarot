/**
 * Module de gestion des appels API aux mod√®les d'IA
 * 
 * IMPORTANT: Toutes les configurations doivent √™tre import√©es depuis /assets/js/config.js
 * Ne pas red√©finir de constantes de configuration ici - importer uniquement depuis config.js
 */

import { API_KEY, API_URL_OPENAI, API_URL_OLLAMA, API_URL_OLLAMA_TAGS, getOllamaModelFormat, DEBUG_LEVEL, SETTINGS, TIMEOUTS } from './config.js';
import { getMetaPrompt, enrichirPromptContextuel } from './prompt.js';
import PERSONAS, { getPersonaPrompt } from './models/personas/index.js';
import { TRANSLATIONS, getTranslation } from './translations/index.js';

// Syst√®me simple de cache pour les r√©ponses
const responseCache = new Map();

/**
 * Fonction pour obtenir une r√©ponse de l'API OpenAI avec GPT-4o
 * @param {string} message - La question pos√©e par l'utilisateur
 * @param {Array} systemPrompts - Tableau de prompts syst√®me additionnels
 * @param {string} modele - Mod√®le d'IA √† utiliser
 * @param {string} persona - Persona s√©lectionn√© pour l'interpr√©tation
 * @param {Array} tirage - Tableau des cartes tir√©es
 * @param {string} langue - Langue s√©lectionn√©e (fr, en, es, de, it)
 * @param {string} spreadType - Type de tirage (cross ou horseshoe)
 * @returns {Promise<string>} - R√©ponse de l'API
 */
async function obtenirReponseGPT4O(message, systemPrompts = [], modele = 'openai/gpt-3.5-turbo', persona = 'tarologue', tirage = [], langue = 'fr', spreadType = 'cross') {
  try {
    console.log(`üîç DEBUG - Requ√™te API avec mod√®le: ${modele}`);
    
    // √âl√©ments DOM
    const responseContent = document.querySelector('.response-content');
    const loadingAnimations = document.getElementById('loading-animations');
    const ollamaPromo = document.getElementById('ollama-promo');
    
    // Si on utilise Ollama, v√©rifier d'abord la connexion
    const isOllama = modele.startsWith('ollama:');
    const ollamaModelName = isOllama ? modele.replace('ollama:', '') : '';
    
    if (isOllama) {
      // Afficher la promotion Ollama
      if (ollamaPromo) {
        ollamaPromo.innerText = getTranslation('ollama.promo', langue) || "Powered by Ollama - A local AI model running on your computer";
      }
      
      try {
        const connectivityTest = await testOllamaConnectivity(ollamaModelName);
        
        if (DEBUG_LEVEL > 0) {
          console.log("‚öôÔ∏è Test Ollama connectivit√©:", connectivityTest);
        }
        
        if (connectivityTest.status === 'testing') {
          const testingText = getTranslation('ollama.testing', langue) || `V√©rification de la connexion √† Ollama...`;
          
          // Afficher le message de test dans responseContent
          if (responseContent) {
            responseContent.innerHTML = `<p>${testingText}</p>`;
          }
          
          return testingText;
        } else if (connectivityTest.status === 'error') {
          const errorMessage = `${getTranslation('interpretation.connectionError', langue) || 'Erreur de connexion √† Ollama:'} ${connectivityTest.message}`;
          
          // Afficher le message d'erreur dans responseContent
          if (responseContent) {
            responseContent.innerHTML = `<p class="error">${errorMessage}</p>`;
          }
          
          throw new Error(`Erreur Ollama: ${connectivityTest.message}`);
        }
      } catch (err) {
        console.error("‚ùå Erreur lors du test de connectivit√© Ollama:", err);
        throw err;
      }
    }
    
    // Construction des messages √† envoyer √† l'API
    const messages = [];
    
    // Ajouter les system prompts (instructions pour l'IA)
    systemPrompts.forEach(prompt => {
      messages.push({
        role: "system",
        content: prompt
      });
    });
    
    // Ajouter le message de l'utilisateur
    messages.push({
      role: "user",
      content: message
    });
    
    // Construire l'URL en fonction du type de mod√®le (Ollama ou OpenAI)
    const API_URL = isOllama ? API_URL_OLLAMA : API_URL_OPENAI;
    
    // Construire les donn√©es de la requ√™te
    const requestData = {
      model: isOllama ? ollamaModelName : modele.replace('openai/', ''),
      messages: messages,
      ...(isOllama ? 
        { stream: SETTINGS.ENABLE_STREAMING } : 
        { 
          max_tokens: parseInt(SETTINGS.MAX_TOKENS || 1000),
          temperature: parseFloat(SETTINGS.TEMPERATURE || 0.7),
          stream: SETTINGS.ENABLE_STREAMING
        }
      )
    };
    
    // Logs de d√©bogage pour diagnostiquer les probl√®mes
    if (DEBUG_LEVEL > 0) {
      logPrompt(persona, message, systemPrompts);
    }
    
    // Format adapt√© selon que c'est Ollama ou OpenAI
    if (!isOllama) {
      requestData.stream = SETTINGS.ENABLE_STREAMING;
    }
    
    console.log("üîç DEBUG - Donn√©es de la requ√™te:", {
      model: modele,
      messagesCount: messages.length,
      stream: SETTINGS.ENABLE_STREAMING,
      max_tokens: isOllama ? "Non sp√©cifi√©" : SETTINGS.MAX_TOKENS
    });
    
    // En-t√™tes de la requ√™te (API key pour OpenAI uniquement)
    const headers = {
      'Content-Type': 'application/json',
      ...(isOllama ? {} : {'Authorization': `Bearer ${API_KEY}`})
    };
    
    console.log("üîç DEBUG - Envoi de la requ√™te API √†:", API_URL);
    
    // Afficher un indicateur de chargement si le streaming est activ√©
    if (SETTINGS.ENABLE_STREAMING && responseContent) {
      const progressContainer = document.createElement('div');
      progressContainer.className = 'ollama-progress';
      progressContainer.innerHTML = `
        <p>${getTranslation('interpretation.streamingResponse', langue)}</p>
        <div class="progress-container">
          <div class="progress-bar"></div>
        </div>
      `;
      responseContent.innerHTML = '';
      responseContent.appendChild(progressContainer);
    }
    
    try {
      console.log("üîç DEBUG - D√©but fetch API");
      
      // Fonction pour cr√©er un timeout pour le fetch
      const fetchWithTimeout = async (url, options, timeoutMs = 30000) => {
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), timeoutMs);
        
        try {
          const response = await fetch(url, {
            ...options,
            signal: controller.signal
          });
          clearTimeout(timeoutId);
          return response;
        } catch (error) {
          clearTimeout(timeoutId);
          if (error.name === 'AbortError') {
            throw new Error(`La requ√™te a expir√© apr√®s ${timeoutMs / 1000} secondes`);
          }
          throw error;
        }
      };
      
      const response = await fetchWithTimeout(API_URL, {
        method: 'POST',
        headers: headers,
        body: JSON.stringify(requestData)
      }, 60000); // 60 secondes de timeout
      
      console.log("üîç DEBUG - R√©ponse fetch re√ßue:", { 
        status: response.status, 
        ok: response.ok,
        statusText: response.statusText
      });
      
      // V√©rifier si la r√©ponse est OK
      if (!response.ok) {
        const errorText = await response.text();
        console.error("üîç DEBUG - Erreur API - R√©ponse texte:", errorText);
        
        try {
          const errorData = JSON.parse(errorText);
          console.error("üîç DEBUG - Erreur API - D√©tails:", errorData);
          throw new Error(`Erreur API: ${errorData.error?.message || response.statusText}`);
        } catch (parseError) {
          throw new Error(`Erreur API (${response.status}): ${response.statusText} - ${errorText.substring(0, 100)}...`);
        }
      }
      
      // Traiter le stream de la r√©ponse
      console.log("üîç DEBUG - D√©but traitement du stream");
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let fullResponse = '';
      
      // Timeout pour chaque chunk de r√©ponse
      let chunkPromise;
      let lastChunkTime = Date.now();
      const chunkTimeout = TIMEOUTS.OLLAMA_RESPONSE; // Utiliser le timeout configur√© pour la r√©ponse
      
      const checkTimeout = () => {
        const now = Date.now();
        if (now - lastChunkTime > chunkTimeout) {
          throw new Error(`D√©lai d'attente d√©pass√©: aucune donn√©e re√ßue depuis ${chunkTimeout / 1000} secondes`);
        }
      };
      
      const timeoutInterval = setInterval(checkTimeout, 5000);
      
      try {
        // Utiliser un tableau pour stocker les morceaux de r√©ponse (optimisation)
        const responseChunks = [];
        let detectedFormat = null;
        
        while (true) {
          // Logging simplifi√©
          if (DEBUG_LEVEL > 1) console.log("üîç DEBUG - Attente de chunk de donn√©es");
          
          // Check for timeout while waiting for chunks
          chunkPromise = reader.read();
          const { done, value } = await chunkPromise;
          
          lastChunkTime = Date.now(); // Reset timeout counter
          
          if (done) {
            if (DEBUG_LEVEL > 0) console.log("üîç DEBUG - Stream termin√©");
            break;
          }
          
          const chunk = decoder.decode(value, { stream: true });
          if (DEBUG_LEVEL > 1) console.log("üîç DEBUG - Chunk re√ßu:", chunk.substring(0, 80) + (chunk.length > 80 ? "..." : ""));
          
          // Traiter le chunk en fonction du type d'API
          if (isOllama) {
            // Format Ollama: objets JSON s√©par√©s par des sauts de ligne
            const lines = chunk.split('\n');
            if (DEBUG_LEVEL > 1) console.log("üîç DEBUG - Traitement chunk Ollama:", { nombreLignes: lines.length });
            
            // Obtenir le format de r√©ponse pour ce mod√®le (utilis√© comme r√©f√©rence initiale)
            const modelFormat = getOllamaModelFormat(ollamaModelName);
            
            for (const line of lines) {
              if (!line.trim()) continue;
              
              try {
                const parsedChunk = JSON.parse(line);
                
                // Afficher le premier chunk complet pour le d√©bogage des formats
                if (!detectedFormat && DEBUG_LEVEL > 0) {
                  console.log("üîç DEBUG - Premier chunk Ollama pour analyse:", parsedChunk);
                }
                
                // D√©tection intelligente du format (si pas encore d√©tect√©)
                if (!detectedFormat) {
                  // Utiliser prioritairement le format d√©termin√© par getOllamaModelFormat
                  if (modelFormat && modelFormat.responseKey) {
                    detectedFormat = modelFormat.responseKey;
                    if (DEBUG_LEVEL > 0) {
                      console.log(`üîç DEBUG - Utilisation du format d√©termin√© par configuration: ${detectedFormat}`);
                    }
                  } 
                  // Sinon, d√©tecter automatiquement en inspectant la r√©ponse
                  else if (parsedChunk.message?.content !== undefined) {
                    detectedFormat = 'message.content';
                    if (DEBUG_LEVEL > 0) {
                      console.log(`üîç DEBUG - Format d√©tect√© automatiquement: ${detectedFormat}`);
                    }
                  } else if (parsedChunk.response !== undefined) {
                    detectedFormat = 'response';
                    if (DEBUG_LEVEL > 0) {
                      console.log(`üîç DEBUG - Format d√©tect√© automatiquement: ${detectedFormat}`);
                    }
                  } else {
                    detectedFormat = 'unknown';
                    console.warn(`‚ö†Ô∏è ATTENTION - Format de r√©ponse inconnu pour ${ollamaModelName}`);
                  }
                }
                
                // Extraction du texte selon le format d√©tect√©
                let responseText = '';
                
                // Utiliser la fonction getValueByPath pour extraire la valeur selon le chemin d√©tect√©
                if (detectedFormat && detectedFormat !== 'unknown') {
                  responseText = getValueByPath(parsedChunk, detectedFormat) || '';
                }
                
                // Logging minimal des informations importantes
                if (DEBUG_LEVEL > 1) {
                  console.log("üîç DEBUG - Ligne Ollama:", { 
                    format: detectedFormat,
                    textFound: responseText !== undefined && responseText !== '',
                    snippet: responseText ? (responseText.substring(0, 20) + "...") : null,
                    done: parsedChunk.done
                  });
                }
                
                // Si on a trouv√© du texte de r√©ponse, l'ajouter √† la r√©ponse
                if (responseText) {
                  responseChunks.push(responseText);
                  // Mise √† jour de l'affichage avec la r√©ponse cumulative
                  fullResponse = responseChunks.join('');
                  if (responseContent) {
                    responseContent.innerHTML = formatStreamingResponse(fullResponse);
                  }
                }
                
                if (parsedChunk.done) {
                  if (DEBUG_LEVEL > 0) console.log("üîç DEBUG - Ollama a signal√© la fin (done=true)");
                }
              } catch (e) {
                console.error("üîç DEBUG - Erreur parsing chunk Ollama:", { 
                  error: e.message,
                  snippet: line.substring(0, 50) + (line.length > 50 ? "..." : "") 
                });
              }
            }
          } else {
            // Format OpenAI: "data: {JSON}" s√©par√©s par des sauts de ligne
            const lines = chunk.split('\n');
            for (const line of lines) {
              if (!line.trim() || !line.startsWith('data: ')) continue;
              
              const jsonStr = line.replace(/^data: /, '');
              if (jsonStr === '[DONE]') continue;
              
              try {
                const parsedChunk = JSON.parse(jsonStr);
                const content = parsedChunk.choices[0]?.delta?.content || '';
                if (content) {
                  fullResponse += content;
                  if (responseContent) {
                    responseContent.innerHTML = formatStreamingResponse(fullResponse);
                  }
                }
              } catch (e) {
                console.error("Erreur lors du parsing du chunk OpenAI:", e);
              }
            }
          }
        }
        
        // √Ä la fin du traitement du stream, juste avant de retourner la r√©ponse
        if (fullResponse.trim() === '') {
          console.error("üîç DEBUG - R√©ponse vide apr√®s traitement complet.");
          throw new Error("R√©ponse vide re√ßue du mod√®le");
        }
        
        // V√©rifier que la r√©ponse n'est pas trop courte ou incompl√®te
        if (fullResponse.length < 20) {
          console.warn("üîç DEBUG - R√©ponse tr√®s courte:", fullResponse);
          // Ne pas lever d'erreur mais enregistrer l'avertissement
        }
        
        // Ajouter un marqueur pour indiquer que le streaming s'est termin√© correctement
        fullResponse += "\n\n<!-- streaming-completed -->";
        
        // Mettre en cache la r√©ponse
        responseCache.set(JSON.stringify({
          persona,
          message,
          tirage: tirage.map(card => card.id),
          spreadType,
          langue
        }), fullResponse);
        
        // Retourner la r√©ponse compl√®te
        return fullResponse;
      } finally {
        clearInterval(timeoutInterval);
      }
    } catch (fetchError) {
      console.error("üîç DEBUG - Erreur critique fetch:", fetchError);
      if (responseContent) {
        responseContent.innerHTML = `<p class="error">${getTranslation('interpretation.apiError', langue) || 'Erreur API:'} ${fetchError.message}</p>`;
      }
      throw fetchError;
    }
  } catch (error) {
    console.error("üîç DEBUG - Erreur globale:", error);
    if (responseContent) {
      responseContent.innerHTML = `<p class="error">${getTranslation('interpretation.error', langue) || 'Erreur:'} ${error.message}</p>`;
    }
    throw error;
  }
}

/**
 * Fonction pour r√©cup√©rer les mod√®les disponibles sur Ollama
 * @returns {Promise<Array>} - Liste des mod√®les disponibles
 */
async function obtenirModelesOllama() {
  try {
    // V√©rifier d'abord si Ollama est accessible
    const ollamaConnected = await verifierConnexionOllama();
    
    if (!ollamaConnected) {
      console.warn("Ollama n'est pas accessible");
      return [];
    }
    
    // R√©cup√©rer la liste des mod√®les
    const response = await fetch(API_URL_OLLAMA_TAGS, {
      method: 'GET',
      headers: {
        'Content-Type': 'application/json'
      }
    });
    
    if (!response.ok) {
      throw new Error(`Erreur lors de la r√©cup√©ration des mod√®les: ${response.statusText}`);
    }
    
    const data = await response.json();
    
    // Retourner la liste des mod√®les
    return data.models || [];
  } catch (error) {
    console.error("Erreur lors de la r√©cup√©ration des mod√®les Ollama:", error);
    return [];
  }
}

/**
 * V√©rifie la connectivit√© avec le serveur Ollama
 * @return {Promise<Object>} Objet contenant des d√©tails sur l'√©tat de la connexion
 */
async function verifierConnexionOllama() {
  try {
    const response = await fetchWithRetry(
      API_URL_OLLAMA_TAGS,
      {
        method: 'GET',
        timeout: 3000 // Augmenter le timeout √† 3 secondes
      },
      2 // Effectuer 2 tentatives
    );

    if (response.ok) {
      try {
        const data = await response.json();
        // V√©rifier que la r√©ponse contient des mod√®les
        if (data && Array.isArray(data.models) && data.models.length > 0) {
          return {
            connected: true,
            status: 'ok',
            message: 'Ollama est accessible et contient des mod√®les',
            models: data.models.length,
            details: data
          };
        } else {
          return {
            connected: true,
            status: 'warning',
            message: 'Ollama est accessible mais aucun mod√®le n\'est disponible',
            models: 0,
            details: data
          };
        }
      } catch (jsonError) {
        return {
          connected: true,
          status: 'warning',
          message: 'Ollama est accessible mais la r√©ponse n\'est pas au format JSON attendu',
          error: jsonError.message,
          details: await response.text()
        };
      }
    } else {
      return {
        connected: false,
        status: 'error',
        message: `Ollama n'est pas accessible (${response.status}: ${response.statusText})`,
        statusCode: response.status,
        details: await response.text().catch(e => 'Impossible de lire la r√©ponse')
      };
    }
  } catch (error) {
    console.warn('Erreur lors de la v√©rification de la connexion Ollama:', error);
    
    // Fournir des informations plus d√©taill√©es sur l'erreur
    return {
      connected: false,
      status: 'error',
      message: error.timeout 
        ? 'La connexion √† Ollama a expir√©' 
        : (error.name === 'AbortError' 
            ? 'La requ√™te vers Ollama a √©t√© interrompue'
            : `Impossible de se connecter √† Ollama: ${error.message}`),
      error: error.message,
      type: error.timeout ? 'timeout' : (error.name === 'AbortError' ? 'abort' : 'connection'),
      details: error
    };
  }
}

/**
 * Utilitaire pour les requ√™tes fetch avec timeout et r√©essai
 * @param {string} url - URL de la requ√™te
 * @param {Object} options - Options de fetch
 * @param {number} maxRetries - Nombre maximum de tentatives
 * @param {number} timeoutMs - D√©lai d'expiration en millisecondes
 * @return {Promise<Response>} - Promesse de r√©ponse
 */
async function fetchWithRetry(url, options, maxRetries = 2, timeoutMs = 5000) {
  let retries = 0;
  let lastError = null;
  
  while (retries <= maxRetries) {
    try {
      // Cr√©er un contr√¥leur d'abandon pour le timeout
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), timeoutMs);
      
      // Ajouter le signal au options
      const optionsWithSignal = {
        ...options,
        signal: controller.signal
      };
      
      try {
        // Tenter la requ√™te
        const response = await fetch(url, optionsWithSignal);
        clearTimeout(timeoutId);
        
        // Si la r√©ponse est OK, la retourner
        return response;
      } catch (fetchError) {
        clearTimeout(timeoutId);
        throw fetchError;
      }
    } catch (error) {
      lastError = error;
      retries++;
      
      const isTimeout = error.name === 'AbortError';
      const retriesLeft = maxRetries - retries + 1;
      
      if (retriesLeft > 0) {
        // D√©lai exponentiel avec un peu d'al√©atoire pour √©viter les collisions
        const delay = Math.pow(1.5, retries) * 500 + Math.random() * 300;
        console.warn(`Tentative ${retries}/${maxRetries} √©chou√©e${isTimeout ? ' (timeout)' : ''}: ${error.message}. Nouvelle tentative dans ${delay/1000} secondes...`);
        
        // Attendre avant de r√©essayer
        await new Promise(resolve => setTimeout(resolve, delay));
      } else {
        console.error(`Toutes les tentatives ont √©chou√© (${retries}/${maxRetries}):`, error);
        throw error;
      }
    }
  }
  
  // Ce code ne devrait jamais √™tre atteint, mais par pr√©caution
  throw lastError || new Error("Erreur inconnue pendant les tentatives de connexion");
}

/**
 * Teste la connectivit√© avec le serveur Ollama et la disponibilit√© d'un mod√®le
 * @param {string} modelName - Nom du mod√®le √† tester (optionnel)
 * @returns {Promise<Object>} - R√©sultat du test
 */
async function testOllamaConnectivity(modelName = null) {
  try {
    // 1. V√©rifier d'abord les mod√®les d√©j√† charg√©s
    const loadedModels = await checkLoadedModels();
    if (modelName && loadedModels.models) {
      const isLoaded = loadedModels.models.some(m => m.name === modelName || m.model === modelName);
      if (isLoaded) {
        if (DEBUG_LEVEL > 0) {
          console.log(`üîç DEBUG - Mod√®le ${modelName} d√©j√† charg√© en m√©moire`);
        }
        return {
          success: true,
          status: 'success',
          modelName: modelName,
          message: 'Mod√®le d√©j√† charg√© en m√©moire',
          details: { isLoaded: true },
          suggestions: []
        };
      }
    }

    // 2. Test de ping du serveur
    if (DEBUG_LEVEL > 0) {
      console.log('üîç DEBUG - Test ping serveur Ollama');
    }

    const pingResponse = await fetch(`${SETTINGS.OLLAMA_URL}/api/tags`, {
      signal: AbortSignal.timeout(TIMEOUTS.OLLAMA_CONNECT)
    });

    if (!pingResponse.ok) {
      throw new Error(`Serveur Ollama non accessible: ${pingResponse.status}`);
    }

    // Si aucun mod√®le sp√©cifi√©, retourner le succ√®s du ping
    if (!modelName) {
      return {
        success: true,
        status: 'success',
        message: 'Connectivit√© Ollama OK',
        details: {},
        suggestions: []
      };
    }

    // 3. Test rapide du mod√®le avec pr√©chargement
    if (DEBUG_LEVEL > 0) {
      console.log(`üîç DEBUG - Test rapide du mod√®le: ${modelName}`);
    }

    try {
      const preloadResult = await preloadModel(modelName);
      if (preloadResult.done_reason === 'load') {
        if (DEBUG_LEVEL > 0) {
          console.log(`üîç DEBUG - Mod√®le ${modelName} charg√© avec succ√®s`);
        }
        return {
          success: true,
          status: 'success',
          modelName: modelName,
          message: 'Mod√®le charg√© avec succ√®s',
          details: { loadDuration: preloadResult.load_duration },
          suggestions: []
        };
      }
    } catch (modelError) {
      throw new Error(`Erreur lors du chargement du mod√®le: ${modelError.message}`);
    }

    return {
      success: true,
      status: 'success',
      modelName: modelName,
      message: 'Connectivit√© Ollama OK',
      details: {},
      suggestions: []
    };

  } catch (error) {
    console.error('Erreur lors du test de connectivit√© Ollama:', error);
    
    return {
      success: false,
      status: 'error',
      modelName: modelName,
      message: error.name === 'TimeoutError' ? 
        'Timeout lors du test de connectivit√©' : 
        `Erreur de connectivit√©: ${error.message}`,
      details: {},
      suggestions: [
        'V√©rifier que le serveur Ollama est en cours d\'ex√©cution',
        'V√©rifier la configuration r√©seau'
      ]
    };
  }
}

// Am√©liorer les logs
function logPrompt(persona, question, systemPrompt) {
  console.group("G√©n√©ration de r√©ponse");
  console.log("Persona:", persona);
  console.log("Question:", question);
  console.log("Prompt syst√®me:", systemPrompt);
  console.groupEnd();
}

/**
 * Formate le texte de r√©ponse pour l'affichage en HTML
 * @param {string} text - Texte √† formater
 * @returns {string} - HTML format√©
 */
function formatStreamingResponse(text) {
  if (!text || typeof text !== 'string') {
    console.error("Erreur: Texte invalide pour formatage", text);
    return '<p>En attente de r√©ponse...</p>';
  }
  
  // R√©duire les logs de d√©bogage
  if (DEBUG_LEVEL > 2) console.log("üîç DEBUG - formatStreamingResponse:", text.substring(0, 30) + "...");
  
  // Cache statique pour optimiser les appels r√©p√©t√©s
  if (!formatStreamingResponse.containsHtmlCache) {
    formatStreamingResponse.containsHtmlCache = new Map();
  }
  
  // V√©rifier si on a d√©j√† analys√© ce texte (optimisation)
  const cacheKey = text.substring(0, 100); // Utiliser d√©but du texte comme cl√©
  
  let containsHtml;
  if (formatStreamingResponse.containsHtmlCache.has(cacheKey)) {
    containsHtml = formatStreamingResponse.containsHtmlCache.get(cacheKey);
  } else {
    // V√©rifie si le texte contient d√©j√† des balises HTML
    containsHtml = /<\/?[a-z][\s\S]*>/i.test(text);
    formatStreamingResponse.containsHtmlCache.set(cacheKey, containsHtml);
  }
  
  if (containsHtml) {
    // Si le texte contient d√©j√† du HTML, v√©rifier seulement qu'il est envelopp√© dans un conteneur
    return text.trim().startsWith('<') ? text : `<div>${text}</div>`;
  } else {
    // Pour le texte brut, diviser en paragraphes et formater
    return text.split('\n').map(paragraph => 
      paragraph.trim() ? `<p>${paragraph}</p>` : ''
    ).join('');
  }
}

/**
 * Extrait la valeur d'un objet selon un chemin d'acc√®s
 * Fonction am√©lior√©e avec fallbacks pour diff√©rents formats de r√©ponse Ollama
 * @param {Object} obj - Objet √† interroger
 * @param {string} path - Chemin de la propri√©t√© (ex: "message.content")
 * @returns {*} - La valeur ou undefined si non trouv√©e
 */
function getValueByPath(obj, path) {
  if (!obj || !path) return undefined;
  
  // 1. Essayer d'extraire selon le chemin sp√©cifi√©
  try {
    const parts = path.split('.');
    let value = obj;
    
    for (const part of parts) {
      if (value === undefined || value === null) return undefined;
      value = value[part];
    }
    
    return value;
  } catch (e) {
    console.warn('Erreur lors de l\'extraction via chemin:', e.message);
  }
  
  // 2. Fallbacks pour les formats les plus communs
  if (path === 'message.content' && !obj.message) {
    // Essayer d'autres formats connus
    return obj.response || obj.content || undefined;
  }
  
  if (path === 'response' && !obj.response) {
    // Essayer d'autres formats connus
    return obj.message?.content || obj.content || undefined;
  }
  
  // En dernier recours, retourner undefined
  return undefined;
}

/**
 * V√©rifie l'√©tat des mod√®les Ollama charg√©s en m√©moire
 * @returns {Promise<Object>} - Informations sur les mod√®les charg√©s
 */
async function checkLoadedModels() {
  try {
    const response = await fetch(`${SETTINGS.OLLAMA_URL}/api/ps`);
    if (!response.ok) {
      throw new Error(`Erreur HTTP: ${response.status}`);
    }
    const data = await response.json();
    if (DEBUG_LEVEL > 0) {
      console.log('üîç DEBUG - Mod√®les charg√©s:', data.models);
    }
    return data;
  } catch (error) {
    console.error('Erreur lors de la v√©rification des mod√®les charg√©s:', error);
    throw error;
  }
}

/**
 * Force le chargement d'un mod√®le en m√©moire
 * @param {string} modelName - Nom du mod√®le √† charger
 * @returns {Promise<Object>} - R√©sultat du chargement
 */
async function preloadModel(modelName) {
  try {
    const response = await fetch(`${SETTINGS.OLLAMA_URL}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: modelName,
        messages: []
      })
    });
    
    if (!response.ok) {
      throw new Error(`Erreur HTTP: ${response.status}`);
    }
    
    const data = await response.json();
    if (DEBUG_LEVEL > 0) {
      console.log(`üîç DEBUG - Pr√©chargement du mod√®le ${modelName}:`, data);
    }
    return data;
  } catch (error) {
    console.error(`Erreur lors du pr√©chargement du mod√®le ${modelName}:`, error);
    throw error;
  }
}

// Exporter les fonctions
export {
  obtenirReponseGPT4O,
  obtenirModelesOllama,
  verifierConnexionOllama,
  enrichirPromptContextuel,
  logPrompt,
  testOllamaConnectivity,
  checkLoadedModels,
  preloadModel
};

// Remplacer par un service ou une fonction locale si n√©cessaire
/**
 * Fonction temporaire pour remplacer le genererPromptTirage de l'ancien tarot.js
 * @param {Array} tirage - Le tirage de cartes
 * @param {string} question - La question pos√©e
 * @param {string} spreadType - Le type de tirage
 * @param {string} langue - La langue utilis√©e
 * @returns {string} - Le prompt format√© pour l'IA
 */
function genererPromptTirage(tirage, question, spreadType, langue) {
  // Utiliser les traductions et le format appropri√©
  // Fonction simplifi√©e pour maintenir la compatibilit√©
  return `Tirage ${spreadType} pour la question: "${question}"`;
}