/**
 * Module de gestion des appels API aux mod√®les d'IA
 */

import { API_KEY, API_URL_OPENAI, API_URL_OLLAMA, API_URL_OLLAMA_TAGS, getOllamaModelFormat, DEBUG_LEVEL, SETTINGS } from './config.js';
import { getMetaPrompt, enrichirPromptContextuel } from './prompt.js';
import PERSONAS, { getPersonaPrompt } from './models/personas/index.js';
import { TRANSLATIONS, getTranslation } from './translations/index.js';

// Syst√®me simple de cache pour les r√©ponses
const responseCache = new Map();

/**
 * Fonction pour obtenir une r√©ponse de l'API OpenAI avec GPT-4o
 * @param {string} message - La question pos√©e par l'utilisateur
 * @param {Array} systemPrompts - Tableau de prompts syst√®me additionnels
 * @param {string} modele - Mod√®le d'IA √† utiliser
 * @param {string} persona - Persona s√©lectionn√© pour l'interpr√©tation
 * @param {Array} tirage - Tableau des cartes tir√©es
 * @param {string} langue - Langue s√©lectionn√©e (fr, en, es, de, it)
 * @param {string} spreadType - Type de tirage (cross ou horseshoe)
 * @returns {Promise<string>} - R√©ponse de l'API
 */
async function obtenirReponseGPT4O(message, systemPrompts = [], modele = 'openai/gpt-3.5-turbo', persona = 'tarologue', tirage = [], langue = 'fr', spreadType = 'cross') {
  try {
    console.log(`üîç DEBUG - Requ√™te API avec mod√®le: ${modele}`);
    
    // Pr√©parer les √©l√©ments UI
    const interpretationsInfo = document.getElementById('interpretations-info');
    const interpretationsPrompt = document.getElementById('interpretations-prompt');
    const interpretationsResponse = document.getElementById('interpretations-response');
    const promptContent = document.querySelector('#interpretations-prompt .prompt-content');
    const responseContent = document.querySelector('#interpretations-response .response-content');
    const loadingAnimations = document.getElementById('loading-animations');
    
    // Si on utilise Ollama, v√©rifier d'abord la connexion
    const isOllama = modele.startsWith('ollama/');
    const ollamaModelName = isOllama ? modele.replace('ollama/', '') : '';
    
    if (isOllama) {
      // Afficher la promotion Ollama
      const ollamaPromo = document.getElementById('ollama-promo');
      if (ollamaPromo) {
        ollamaPromo.innerText = getTranslation('ollama.promo', langue) || "Powered by Ollama - A local AI model running on your computer";
      }
      
      try {
        const connectivityTest = await testOllamaConnectivity(ollamaModelName);
        
        if (DEBUG_LEVEL > 0) {
          console.log("‚öôÔ∏è Test Ollama connectivit√©:", connectivityTest);
        }
        
        if (connectivityTest.status === 'testing') {
          const testingText = getTranslation('ollama.testing', langue) || `V√©rification de la connexion √† Ollama...`;
          
          // Mise √† jour pour respecter la nouvelle structure HTML
          const infoContent = interpretationsInfo.querySelector('.information-zone__content');
          if (infoContent) {
            infoContent.innerHTML = `<p>${testingText}</p>`;
          } else {
            interpretationsInfo.innerHTML = `
              <div class="information-zone__header">
                <h3 class="information-zone__title">${getTranslation('information.title', langue, 'Information')}</h3>
              </div>
              <div class="information-zone__content">
                <p>${testingText}</p>
              </div>
            `;
          }
          
          interpretationsPrompt.style.display = 'none';
          interpretationsResponse.style.display = 'none';
          return testingText;
        } else if (connectivityTest.status === 'error') {
          const errorMessage = `${getTranslation('interpretation.connectionError', langue) || 'Erreur de connexion √† Ollama:'} ${connectivityTest.message}`;
          
          // Mise √† jour pour respecter la nouvelle structure HTML
          const infoContent = interpretationsInfo.querySelector('.information-zone__content');
          if (infoContent) {
            infoContent.innerHTML = `<p class="error">${errorMessage}</p>`;
          } else {
            interpretationsInfo.innerHTML = `
              <div class="information-zone__header">
                <h3 class="information-zone__title">${getTranslation('information.title', langue, 'Information')}</h3>
              </div>
              <div class="information-zone__content">
                <p class="error">${errorMessage}</p>
              </div>
            `;
          }
          
          interpretationsPrompt.style.display = 'none';
          interpretationsResponse.style.display = 'none';
          throw new Error(`Erreur Ollama: ${connectivityTest.message}`);
        }
      } catch (err) {
        console.error("‚ùå Erreur lors du test de connectivit√© Ollama:", err);
        throw err;
      }
    }
    
    // Construction des messages √† envoyer √† l'API
    const messages = [];
    
    // Ajouter les system prompts (instructions pour l'IA)
    systemPrompts.forEach(prompt => {
      messages.push({
        role: "system",
        content: prompt
      });
    });
    
    // Ajouter le message de l'utilisateur
    messages.push({
      role: "user",
      content: message
    });
    
    // Construire l'URL en fonction du type de mod√®le (Ollama ou OpenAI)
    const API_URL = isOllama ? API_URL_OLLAMA : API_URL_OPENAI;
    
    // Construire les donn√©es de la requ√™te
    const requestData = {
      model: isOllama ? ollamaModelName : modele.replace('openai/', ''),
      messages: messages,
      ...(isOllama ? 
        { stream: SETTINGS.ENABLE_STREAMING } : 
        { 
          max_tokens: parseInt(SETTINGS.MAX_TOKENS || 1000),
          temperature: parseFloat(SETTINGS.TEMPERATURE || 0.7),
          stream: SETTINGS.ENABLE_STREAMING
        }
      )
    };
    
    // Logs de d√©bogage pour diagnostiquer les probl√®mes
    if (DEBUG_LEVEL > 0) {
      logPrompt(persona, message, systemPrompts);
    }
    
    // Format adapt√© selon que c'est Ollama ou OpenAI
    if (!isOllama) {
      requestData.stream = SETTINGS.ENABLE_STREAMING;
    }
    
    console.log("üîç DEBUG - Donn√©es de la requ√™te:", {
      model: modele,
      messagesCount: messages.length,
      stream: SETTINGS.ENABLE_STREAMING,
      max_tokens: isOllama ? "Non sp√©cifi√©" : SETTINGS.MAX_TOKENS
    });
    
    // En-t√™tes de la requ√™te (API key pour OpenAI uniquement)
    const headers = {
      'Content-Type': 'application/json',
      ...(isOllama ? {} : {'Authorization': `Bearer ${API_KEY}`})
    };
    
    console.log("üîç DEBUG - Envoi de la requ√™te API √†:", API_URL);
    
    // Faire la requ√™te √† l'API avec streaming
    const progressContainer = document.createElement('div');
    progressContainer.className = 'ollama-progress';
    progressContainer.innerHTML = `
      <p>${getTranslation('interpretation.streamingResponse', langue)}</p>
      <div class="progress-container">
        <div class="progress-bar"></div>
      </div>
    `;
    interpretationsInfo.innerHTML = '';
    interpretationsInfo.appendChild(progressContainer);
    
    const partialResponse = document.createElement('div');
    partialResponse.className = 'partial-response';
    responseContent.appendChild(partialResponse);
    
    try {
      console.log("üîç DEBUG - D√©but fetch API");
      
      // Fonction pour cr√©er un timeout pour le fetch
      const fetchWithTimeout = async (url, options, timeoutMs = 30000) => {
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), timeoutMs);
        
        try {
          const response = await fetch(url, {
            ...options,
            signal: controller.signal
          });
          clearTimeout(timeoutId);
          return response;
        } catch (error) {
          clearTimeout(timeoutId);
          if (error.name === 'AbortError') {
            throw new Error(`La requ√™te a expir√© apr√®s ${timeoutMs / 1000} secondes`);
          }
          throw error;
        }
      };
      
      const response = await fetchWithTimeout(API_URL, {
        method: 'POST',
        headers: headers,
        body: JSON.stringify(requestData)
      }, 60000); // 60 secondes de timeout
      
      console.log("üîç DEBUG - R√©ponse fetch re√ßue:", { 
        status: response.status, 
        ok: response.ok,
        statusText: response.statusText
      });
      
      // V√©rifier si la r√©ponse est OK
      if (!response.ok) {
        const errorText = await response.text();
        console.error("üîç DEBUG - Erreur API - R√©ponse texte:", errorText);
        
        try {
          const errorData = JSON.parse(errorText);
          console.error("üîç DEBUG - Erreur API - D√©tails:", errorData);
          throw new Error(`Erreur API: ${errorData.error?.message || response.statusText}`);
        } catch (parseError) {
          throw new Error(`Erreur API (${response.status}): ${response.statusText} - ${errorText.substring(0, 100)}...`);
        }
      }
      
      // Traiter le stream de la r√©ponse
      console.log("üîç DEBUG - D√©but traitement du stream");
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let fullResponse = '';
      
      // Timeout pour chaque chunk de r√©ponse
      let chunkPromise;
      let lastChunkTime = Date.now();
      const chunkTimeout = 30000; // 30 secondes entre chaque chunk
      
      const checkTimeout = () => {
        const now = Date.now();
        if (now - lastChunkTime > chunkTimeout) {
          throw new Error(`D√©lai d'attente d√©pass√©: aucune donn√©e re√ßue depuis ${chunkTimeout / 1000} secondes`);
        }
      };
      
      const timeoutInterval = setInterval(checkTimeout, 5000);
      
      try {
        // Utiliser un tableau pour stocker les morceaux de r√©ponse (optimisation)
        const responseChunks = [];
        let detectedFormat = null;
        
        while (true) {
          // Logging simplifi√©
          if (DEBUG_LEVEL > 1) console.log("üîç DEBUG - Attente de chunk de donn√©es");
          
          // Check for timeout while waiting for chunks
          chunkPromise = reader.read();
          const { done, value } = await chunkPromise;
          
          lastChunkTime = Date.now(); // Reset timeout counter
          
          if (done) {
            if (DEBUG_LEVEL > 0) console.log("üîç DEBUG - Stream termin√©");
            break;
          }
          
          const chunk = decoder.decode(value, { stream: true });
          if (DEBUG_LEVEL > 1) console.log("üîç DEBUG - Chunk re√ßu:", chunk.substring(0, 80) + (chunk.length > 80 ? "..." : ""));
          
          // Traiter le chunk en fonction du type d'API
          if (isOllama) {
            // Format Ollama: objets JSON s√©par√©s par des sauts de ligne
            const lines = chunk.split('\n');
            if (DEBUG_LEVEL > 1) console.log("üîç DEBUG - Traitement chunk Ollama:", { nombreLignes: lines.length });
            
            // Obtenir le format de r√©ponse pour ce mod√®le (utilis√© comme r√©f√©rence initiale)
            const modelFormat = getOllamaModelFormat(ollamaModelName);
            
            for (const line of lines) {
              if (!line.trim()) continue;
              
              try {
                const parsedChunk = JSON.parse(line);
                
                // Afficher le premier chunk complet pour le d√©bogage des formats
                if (!detectedFormat && DEBUG_LEVEL > 0) {
                  console.log("üîç DEBUG - Premier chunk Ollama pour analyse:", parsedChunk);
                }
                
                // D√©tection intelligente du format (si pas encore d√©tect√©)
                if (!detectedFormat) {
                  if (parsedChunk.message?.content !== undefined) {
                    detectedFormat = 'message.content';
                  } else if (parsedChunk.response !== undefined) {
                    detectedFormat = 'response';
                  } else if (modelFormat && modelFormat.responseKey) {
                    detectedFormat = modelFormat.responseKey;
                  } else {
                    detectedFormat = 'unknown';
                  }
                  
                  if (DEBUG_LEVEL > 0) console.log(`üîç DEBUG - Format d√©tect√© pour ${ollamaModelName}: ${detectedFormat}`);
                }
                
                // Extraction du texte selon le format d√©tect√© (approche am√©lior√©e)
                let responseText = '';
                
                // V√©rifier d'abord explicitement le format pour llama3.1
                if (ollamaModelName.includes('llama3.1')) {
                  // Pour llama3.1, essayer sp√©cifiquement ces chemins
                  if (parsedChunk.message?.content !== undefined) {
                    responseText = parsedChunk.message.content;
                  } else if (parsedChunk.content !== undefined) {
                    responseText = parsedChunk.content;
                  } else if (parsedChunk.response !== undefined) {
                    responseText = parsedChunk.response;
                  }
                  
                  if (DEBUG_LEVEL > 1 && responseText) {
                    console.log(`üîç DEBUG - Texte extrait pour llama3.1:`, responseText.substring(0, 20) + "...");
                  }
                } else {
                  // Pour les autres mod√®les, suivre la logique normale
                  if (detectedFormat === 'message.content' && parsedChunk.message?.content !== undefined) {
                    responseText = parsedChunk.message.content;
                  } else if (detectedFormat === 'response' && parsedChunk.response !== undefined) {
                    responseText = parsedChunk.response;
                  } else {
                    // Fallback aux autres m√©thodes si le format d√©tect√© n'est pas disponible
                    responseText = getValueByPath(parsedChunk, detectedFormat) || 
                                  parsedChunk.response || 
                                  parsedChunk.message?.content || 
                                  '';
                  }
                }
                
                // Logging minimal des informations importantes
                if (DEBUG_LEVEL > 1) {
                  console.log("üîç DEBUG - Ligne Ollama:", { 
                    format: detectedFormat,
                    textFound: responseText !== undefined && responseText !== '',
                    snippet: responseText ? (responseText.substring(0, 20) + "...") : null,
                    done: parsedChunk.done
                  });
                }
                
                // Si on a trouv√© du texte de r√©ponse, l'ajouter √† la r√©ponse
                if (responseText) {
                  responseChunks.push(responseText);
                  // Mise √† jour de l'affichage avec la r√©ponse cumulative
                  fullResponse = responseChunks.join('');
                  responseContent.innerHTML = formatStreamingResponse(fullResponse);
                }
                
                if (parsedChunk.done) {
                  if (DEBUG_LEVEL > 0) console.log("üîç DEBUG - Ollama a signal√© la fin (done=true)");
                }
              } catch (e) {
                console.error("üîç DEBUG - Erreur parsing chunk Ollama:", { 
                  error: e.message,
                  snippet: line.substring(0, 50) + (line.length > 50 ? "..." : "") 
                });
              }
            }
          } else {
            // Format OpenAI: "data: {JSON}" s√©par√©s par des sauts de ligne
            const lines = chunk.split('\n');
            for (const line of lines) {
              if (!line.trim() || !line.startsWith('data: ')) continue;
              
              const jsonStr = line.replace(/^data: /, '');
              if (jsonStr === '[DONE]') continue;
              
              try {
                const parsedChunk = JSON.parse(jsonStr);
                const content = parsedChunk.choices[0]?.delta?.content || '';
                if (content) {
                  fullResponse += content;
                  responseContent.innerHTML = formatStreamingResponse(fullResponse);
                }
              } catch (e) {
                console.error("Erreur lors du parsing du chunk OpenAI:", e);
              }
            }
          }
        }
        
        // √Ä la fin du traitement du stream, juste avant de retourner la r√©ponse
        if (fullResponse.trim() === '') {
          console.error("üîç DEBUG - R√©ponse vide apr√®s traitement complet.");
          throw new Error("R√©ponse vide re√ßue du mod√®le");
        }
        
        // V√©rifier que la r√©ponse n'est pas trop courte ou incompl√®te
        if (fullResponse.length < 20) {
          console.warn("üîç DEBUG - R√©ponse tr√®s courte:", fullResponse);
          // Ne pas lever d'erreur mais enregistrer l'avertissement
        }
        
        // Ajouter un marqueur pour indiquer que le streaming s'est termin√© correctement
        fullResponse += "\n\n<!-- streaming-completed -->";
        
        // Mettre en cache la r√©ponse
        responseCache.set(JSON.stringify({
          persona,
          message,
          tirage: tirage.map(card => card.id),
          spreadType,
          langue
        }), fullResponse);
        
        // Retourner la r√©ponse compl√®te
        return fullResponse;
      } finally {
        clearInterval(timeoutInterval);
      }
    } catch (fetchError) {
      console.error("üîç DEBUG - Erreur critique fetch:", fetchError);
      responseContent.innerHTML = `<p class="error">${getTranslation('interpretation.apiError', langue) || 'Erreur API:'} ${fetchError.message}</p>`;
      throw fetchError;
    }
  } catch (error) {
    console.error("üîç DEBUG - Erreur globale:", error);
    responseContent.innerHTML = `<p class="error">${getTranslation('interpretation.error', langue) || 'Erreur:'} ${error.message}</p>`;
    throw error;
  }
}

/**
 * Fonction pour r√©cup√©rer les mod√®les disponibles sur Ollama
 * @returns {Promise<Array>} - Liste des mod√®les disponibles
 */
async function obtenirModelesOllama() {
  try {
    // V√©rifier d'abord si Ollama est accessible
    const ollamaConnected = await verifierConnexionOllama();
    
    if (!ollamaConnected) {
      console.warn("Ollama n'est pas accessible");
      return [];
    }
    
    // R√©cup√©rer la liste des mod√®les
    const response = await fetch(API_URL_OLLAMA_TAGS, {
      method: 'GET',
      headers: {
        'Content-Type': 'application/json'
      }
    });
    
    if (!response.ok) {
      throw new Error(`Erreur lors de la r√©cup√©ration des mod√®les: ${response.statusText}`);
    }
    
    const data = await response.json();
    
    // Retourner la liste des mod√®les
    return data.models || [];
  } catch (error) {
    console.error("Erreur lors de la r√©cup√©ration des mod√®les Ollama:", error);
    return [];
  }
}

/**
 * Fonction pour v√©rifier si Ollama est accessible
 * @returns {Promise<boolean>} - true si Ollama est accessible, false sinon
 */
async function verifierConnexionOllama() {
  try {
    const response = await fetch(API_URL_OLLAMA_TAGS, { 
      method: 'GET',
      headers: {
        'Content-Type': 'application/json'
      },
      signal: AbortSignal.timeout(2000) // Timeout apr√®s 2 secondes
    });
    
    return response.ok;
  } catch (error) {
    console.warn("Ollama n'est pas accessible:", error);
    return false;
  }
}

/**
 * Fonction pour tester la connectivit√© avec Ollama
 * @param {string} modelName - Nom du mod√®le √† tester
 * @returns {Promise<Object>} - R√©sultat du test avec statut et message
 */
async function testOllamaConnectivity(modelName) {
  console.log("üîç DEBUG - Test de connectivit√© Ollama pour le mod√®le:", modelName);
  
  try {
    // 1. Test simple de ping sur le serveur Ollama
    console.log("üîç DEBUG - Test ping serveur Ollama");
    const pingResponse = await fetch(`${API_URL_OLLAMA.replace('/api/chat', '')}/api/tags`, {
      method: 'GET',
      headers: { 'Content-Type': 'application/json' }
    });
    
    if (!pingResponse.ok) {
      console.error("üîç DEBUG - Serveur Ollama inaccessible:", pingResponse.status, pingResponse.statusText);
      return { 
        success: false, 
        message: `Serveur Ollama inaccessible (${pingResponse.status}): ${pingResponse.statusText}` 
      };
    }
    
    // 2. V√©rifier si le mod√®le est disponible
    console.log("üîç DEBUG - V√©rification disponibilit√© du mod√®le:", modelName);
    const modelsData = await pingResponse.json();
    
    if (!modelsData.models) {
      console.error("üîç DEBUG - Format de r√©ponse Ollama inattendu:", modelsData);
      return { 
        success: false, 
        message: "Format de r√©ponse Ollama inattendu" 
      };
    }
    
    const modelExists = modelsData.models.some(m => m.name === modelName);
    if (!modelExists) {
      console.error("üîç DEBUG - Mod√®le non trouv√©:", modelName);
      return { 
        success: false, 
        message: `Le mod√®le ${modelName} n'est pas disponible sur ce serveur Ollama` 
      };
    }
    
    // 3. Test rapide du mod√®le
    console.log("üîç DEBUG - Test rapide du mod√®le:", modelName);
    const testResponse = await fetch(API_URL_OLLAMA, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: modelName,
        messages: [{ role: "user", content: "R√©ponds simplement par 'OK' pour tester la connectivit√©." }],
        stream: false
      })
    });
    
    if (!testResponse.ok) {
      console.error("üîç DEBUG - Test du mod√®le √©chou√©:", testResponse.status, testResponse.statusText);
      return { 
        success: false, 
        message: `Test du mod√®le √©chou√© (${testResponse.status}): ${testResponse.statusText}` 
      };
    }
    
    console.log("üîç DEBUG - Connectivit√© Ollama OK pour:", modelName);
    return { success: true, message: "Connectivit√© Ollama OK" };
    
  } catch (error) {
    console.error("üîç DEBUG - Erreur lors du test de connectivit√© Ollama:", error);
    return { 
      success: false, 
      message: `Erreur de connectivit√©: ${error.message}` 
    };
  }
}

// Am√©liorer les logs
function logPrompt(persona, question, systemPrompt) {
  console.group("G√©n√©ration de r√©ponse");
  console.log("Persona:", persona);
  console.log("Question:", question);
  console.log("Prompt syst√®me:", systemPrompt);
  console.groupEnd();
}

/**
 * Formate le texte de r√©ponse pour l'affichage en HTML
 * @param {string} text - Texte √† formater
 * @returns {string} - HTML format√©
 */
function formatStreamingResponse(text) {
  if (!text || typeof text !== 'string') {
    console.error("Erreur: Texte invalide pour formatage", text);
    return '<p>En attente de r√©ponse...</p>';
  }
  
  // R√©duire les logs de d√©bogage
  if (DEBUG_LEVEL > 2) console.log("üîç DEBUG - formatStreamingResponse:", text.substring(0, 30) + "...");
  
  // Cache statique pour optimiser les appels r√©p√©t√©s
  if (!formatStreamingResponse.containsHtmlCache) {
    formatStreamingResponse.containsHtmlCache = new Map();
  }
  
  // V√©rifier si on a d√©j√† analys√© ce texte (optimisation)
  const cacheKey = text.substring(0, 100); // Utiliser d√©but du texte comme cl√©
  
  let containsHtml;
  if (formatStreamingResponse.containsHtmlCache.has(cacheKey)) {
    containsHtml = formatStreamingResponse.containsHtmlCache.get(cacheKey);
  } else {
    // V√©rifie si le texte contient d√©j√† des balises HTML
    containsHtml = /<\/?[a-z][\s\S]*>/i.test(text);
    formatStreamingResponse.containsHtmlCache.set(cacheKey, containsHtml);
  }
  
  if (containsHtml) {
    // Si le texte contient d√©j√† du HTML, v√©rifier seulement qu'il est envelopp√© dans un conteneur
    return text.trim().startsWith('<') ? text : `<div>${text}</div>`;
  } else {
    // Pour le texte brut, diviser en paragraphes et formater
    return text.split('\n').map(paragraph => 
      paragraph.trim() ? `<p>${paragraph}</p>` : ''
    ).join('');
  }
}

/**
 * Extrait une valeur depuis un objet en utilisant une notation par points
 * @param {Object} obj - L'objet source
 * @param {string} path - Chemin de la propri√©t√© (ex: "message.content")
 * @returns {*} - La valeur ou undefined si non trouv√©e
 */
function getValueByPath(obj, path) {
  if (!obj || !path) return undefined;
  
  const parts = path.split('.');
  let value = obj;
  
  for (const part of parts) {
    if (value === undefined || value === null) return undefined;
    value = value[part];
  }
  
  return value;
}

// Exporter les fonctions
export {
  obtenirReponseGPT4O,
  obtenirModelesOllama,
  verifierConnexionOllama,
  enrichirPromptContextuel,
  logPrompt,
  testOllamaConnectivity
};

// Remplacer par un service ou une fonction locale si n√©cessaire
/**
 * Fonction temporaire pour remplacer le genererPromptTirage de l'ancien tarot.js
 * @param {Array} tirage - Le tirage de cartes
 * @param {string} question - La question pos√©e
 * @param {string} spreadType - Le type de tirage
 * @param {string} langue - La langue utilis√©e
 * @returns {string} - Le prompt format√© pour l'IA
 */
function genererPromptTirage(tirage, question, spreadType, langue) {
  // Utiliser les traductions et le format appropri√©
  // Fonction simplifi√©e pour maintenir la compatibilit√©
  return `Tirage ${spreadType} pour la question: "${question}"`;
}